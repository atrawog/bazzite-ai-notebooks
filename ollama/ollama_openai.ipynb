{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama OpenAI Compatibility API\n",
    "\n",
    "This notebook demonstrates Ollama's OpenAI-compatible API using the official `openai` Python library.\n",
    "\n",
    "## Features Covered\n",
    "\n",
    "- List models\n",
    "- Generate response (completions)\n",
    "- Chat completion\n",
    "- Streaming responses\n",
    "- Generate embeddings\n",
    "\n",
    "## Limitations\n",
    "\n",
    "The OpenAI compatibility layer does **not** support:\n",
    "- Show model details (`/api/show`)\n",
    "- List running models (`/api/ps`)\n",
    "- Copy model (`/api/copy`)\n",
    "- Delete model (`/api/delete`)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Ollama pod running: `ujust ollama start`\n",
    "- Model pulled: `ujust ollama pull llama3.2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama host: http://ollama:11434\n",
       "OpenAI base URL: http://ollama:11434/v1\n",
       "Default model: llama3.2:latest\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "\n",
    "# === Configuration ===\n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\n",
    "DEFAULT_MODEL = \"llama3.2:latest\"\n",
    "\n",
    "# Initialize OpenAI client pointing to Ollama\n",
    "client = OpenAI(\n",
    "    base_url=f\"{OLLAMA_HOST}/v1\",\n",
    "    api_key=\"ollama\"  # Required by library but ignored by Ollama\n",
    ")\n",
    "\n",
    "print(f\"Ollama host: {OLLAMA_HOST}\")\n",
    "print(f\"OpenAI base URL: {OLLAMA_HOST}/v1\")\n",
    "print(f\"Default model: {DEFAULT_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connection Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "✓ Ollama server is running!\n",
       "✓ Model 'llama3.2:latest' is available\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_ollama_health() -> tuple[bool, bool]:\n",
    "    \"\"\"Check if Ollama server is running and model is available.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (server_healthy, model_available)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"✓ Ollama server is running!\")\n",
    "            models = response.json()\n",
    "            model_names = [m.get(\"name\", \"\") for m in models.get(\"models\", [])]\n",
    "            \n",
    "            if DEFAULT_MODEL in model_names:\n",
    "                print(f\"✓ Model '{DEFAULT_MODEL}' is available\")\n",
    "                return True, True\n",
    "            else:\n",
    "                print(f\"✗ Model '{DEFAULT_MODEL}' not found!\")\n",
    "                print()\n",
    "                if model_names:\n",
    "                    print(\"Available models:\")\n",
    "                    for name in model_names:\n",
    "                        print(f\"  - {name}\")\n",
    "                else:\n",
    "                    print(\"No models installed.\")\n",
    "                print()\n",
    "                print(\"To fix this, run:\")\n",
    "                print(f\"  ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "                return True, False\n",
    "        else:\n",
    "            print(f\"Ollama returned unexpected status: {response.status_code}\")\n",
    "            return False, False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"✗ Cannot connect to Ollama server!\")\n",
    "        print(\"To fix this, run: ujust ollama start\")\n",
    "        return False, False\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"✗ Connection to Ollama timed out!\")\n",
    "        return False, False\n",
    "\n",
    "ollama_healthy, model_available = check_ollama_health()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. List Models\n",
    "\n",
    "**Endpoint:** `GET /v1/models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== List Available Models ===\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "  - hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\n",
       "  - llama3.2:latest\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== List Available Models ===\")\n",
    "\n",
    "models = client.models.list()\n",
    "for model in models.data:\n",
    "    print(f\"  - {model.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Response (Completions)\n",
    "\n",
    "**Endpoint:** `POST /v1/completions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Generate Response ===\n",
       "Prompt: Why is the sky blue? Answer in one sentence.\n",
       "\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Response: The sky appears blue because when sunlight enters Earth's atmosphere, it encounters tiny molecules of gases such as nitrogen and oxygen, which scatter the shorter, blue wavelengths of light more than the longer, red wavelengths.\n",
       "\n",
       "Latency: 0.26s\n",
       "Completion tokens: 42\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== Generate Response ===\")\n",
    "\n",
    "if not model_available:\n",
    "    print()\n",
    "    print(\"⚠ Skipping - model not available\")\n",
    "    print(f\"  Run: ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "else:\n",
    "    prompt = \"Why is the sky blue? Answer in one sentence.\"\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print()\n",
    "\n",
    "    try:\n",
    "        start_time = time.perf_counter()\n",
    "        response = client.completions.create(\n",
    "            model=DEFAULT_MODEL,\n",
    "            prompt=prompt,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        print(f\"Response: {response.choices[0].text}\")\n",
    "        print()\n",
    "        print(f\"Latency: {end_time - start_time:.2f}s\")\n",
    "        print(f\"Completion tokens: {response.usage.completion_tokens}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chat Completion\n",
    "\n",
    "**Endpoint:** `POST /v1/chat/completions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Chat Completion ===\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Assistant: Machine learning is a type of artificial intelligence that enables computers to learn, make decisions, and improve their performance on tasks without being explicitly programmed.\n",
       "\n",
       "Tokens used: 72\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== Chat Completion ===\")\n",
    "\n",
    "if not model_available:\n",
    "    print()\n",
    "    print(\"⚠ Skipping - model not available\")\n",
    "    print(f\"  Run: ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "else:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=DEFAULT_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant. Keep responses brief.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Explain machine learning in one sentence.\"}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=100\n",
    "        )\n",
    "\n",
    "        print(f\"Assistant: {response.choices[0].message.content}\")\n",
    "        print(f\"\\nTokens used: {response.usage.total_tokens}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Multi-turn Conversation ===\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "User: What is 2 + 2?\n",
       "Assistant: A simple but classic math question!\n",
       "\n",
       "The answer to 2 + 2 is... 4! Would you like help with anything else or want to move on to something more challenging?\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "User: And what is that multiplied by 3?\n",
       "Assistant: Since we know the result of 2 + 2 is 4, let's multiply 4 by 3...\n",
       "\n",
       "4 × 3 = 12!\n",
       "\n",
       "So, the answer is 12! Easy peasy! What's your next math question\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== Multi-turn Conversation ===\")\n",
    "\n",
    "if not model_available:\n",
    "    print()\n",
    "    print(\"⚠ Skipping - model not available\")\n",
    "    print(f\"  Run: ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "else:\n",
    "    try:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"}\n",
    "        ]\n",
    "\n",
    "        # Turn 1\n",
    "        messages.append({\"role\": \"user\", \"content\": \"What is 2 + 2?\"})\n",
    "        response = client.chat.completions.create(\n",
    "            model=DEFAULT_MODEL,\n",
    "            messages=messages,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        assistant_msg = response.choices[0].message.content\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        print(f\"User: What is 2 + 2?\")\n",
    "        print(f\"Assistant: {assistant_msg}\")\n",
    "\n",
    "        # Turn 2\n",
    "        messages.append({\"role\": \"user\", \"content\": \"And what is that multiplied by 3?\"})\n",
    "        response = client.chat.completions.create(\n",
    "            model=DEFAULT_MODEL,\n",
    "            messages=messages,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        print(f\"User: And what is that multiplied by 3?\")\n",
    "        print(f\"Assistant: {response.choices[0].message.content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Streaming Response\n",
    "\n",
    "**Endpoint:** `POST /v1/chat/completions` with `stream: true`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Streaming Response ===\n",
       "\n",
       "Response: Here we go:\n",
       "\n",
       "1, 2, 3, 4, 5!\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== Streaming Response ===\")\n",
    "\n",
    "if not model_available:\n",
    "    print()\n",
    "    print(\"⚠ Skipping - model not available\")\n",
    "    print(f\"  Run: ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "else:\n",
    "    try:\n",
    "        print()\n",
    "\n",
    "        stream = client.chat.completions.create(\n",
    "            model=DEFAULT_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Count from 1 to 5.\"}],\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        collected = []\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                collected.append(chunk.choices[0].delta.content)\n",
    "\n",
    "        print(f\"Response: {''.join(collected)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Embeddings\n",
    "\n",
    "**Endpoint:** `POST /v1/embeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Generate Embeddings ===\n",
       "Input: 'Ollama makes running LLMs locally easy and efficient.'\n",
       "Embedding dimensions: 3072\n",
       "First 5 values: [-0.026683127507567406, -0.0028091324493288994, -0.02738499455153942, -0.009667067788541317, -0.017405545338988304]\n",
       "Last 5 values: [-0.028065813705325127, 0.010568944737315178, -0.028453463688492775, 0.014874468557536602, -0.02971256710588932]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== Generate Embeddings ===\")\n",
    "\n",
    "if not model_available:\n",
    "    print()\n",
    "    print(\"⚠ Skipping - model not available\")\n",
    "    print(f\"  Run: ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "else:\n",
    "    try:\n",
    "        test_text = \"Ollama makes running LLMs locally easy and efficient.\"\n",
    "\n",
    "        response = client.embeddings.create(\n",
    "            model=DEFAULT_MODEL,\n",
    "            input=test_text\n",
    "        )\n",
    "\n",
    "        embedding = response.data[0].embedding\n",
    "        print(f\"Input: '{test_text}'\")\n",
    "        print(f\"Embedding dimensions: {len(embedding)}\")\n",
    "        print(f\"First 5 values: {embedding[:5]}\")\n",
    "        print(f\"Last 5 values: {embedding[-5:]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Error Handling ===\n",
       "\n",
       "1. Testing non-existent model...\n",
       "   Expected error: NotFoundError\n",
       "\n",
       "2. Testing empty messages...\n",
       "   Error: BadRequestError\n",
       "\n",
       "Error handling tests completed!\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== Error Handling ===\")\n",
    "\n",
    "# Test: Non-existent model\n",
    "print(\"\\n1. Testing non-existent model...\")\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"invalid-model\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "    )\n",
    "    print(f\"   Unexpected success\")\n",
    "except Exception as e:\n",
    "    print(f\"   Expected error: {type(e).__name__}\")\n",
    "\n",
    "# Test: Empty messages\n",
    "print(\"\\n2. Testing empty messages...\")\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=DEFAULT_MODEL,\n",
    "        messages=[]\n",
    "    )\n",
    "    print(f\"   Empty messages allowed\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {type(e).__name__}\")\n",
    "\n",
    "print(\"\\nError handling tests completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated Ollama's OpenAI-compatible API.\n",
    "\n",
    "### API Endpoints Used\n",
    "\n",
    "| Endpoint | Method | Purpose |\n",
    "|----------|--------|--------|\n",
    "| `/v1/models` | GET | List models |\n",
    "| `/v1/completions` | POST | Generate text |\n",
    "| `/v1/chat/completions` | POST | Chat completion |\n",
    "| `/v1/embeddings` | POST | Generate embeddings |\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://ollama:11434/v1\",\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "# Chat\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3.2:latest\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "```\n",
    "\n",
    "### Why Use OpenAI Compatibility?\n",
    "\n",
    "- **Migration** - Drop-in replacement for OpenAI API\n",
    "- **Tool ecosystem** - Works with LangChain, LlamaIndex, etc.\n",
    "- **Familiar interface** - Standard OpenAI patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
