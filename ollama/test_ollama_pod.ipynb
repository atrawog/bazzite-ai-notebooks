{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Ollama Pod Testing Notebook\n",
    "\n",
    "This notebook tests the local Ollama pod functionality using **three API approaches**:\n",
    "\n",
    "1. **`requests`** - Raw HTTP calls to Ollama REST API\n",
    "2. **`ollama`** - Official Ollama Python client\n",
    "3. **`openai`** - OpenAI compatibility layer\n",
    "\n",
    "Includes comprehensive tests and performance benchmarks.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Ollama pod must be running: `ujust ollama start`\n",
    "- At least one model must be pulled: `ujust ollama pull llama3.2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-header",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": "import os\n\n# === Configuration (Papermill parameters) ===\n# OLLAMA_HOST is auto-configured for cross-pod communication:\n# - Quadlet: Injected via service discovery (http://ollama:11434)\n# - Docker Compose: Set in compose environment (http://ollama:11434)\n# - Local development: Falls back to localhost\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\nDEFAULT_MODEL = \"llama3.2\"\nBENCHMARK_RUNS = 5\nTEST_PROMPT = \"Why is the sky blue? Answer in one sentence.\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "# === Imports ===\nimport requests\nimport json\nimport time\nimport statistics\nfrom typing import Callable, Any\n\nimport ollama\nfrom openai import OpenAI\nimport pandas as pd\n\nprint(\"Imports successful!\")\nprint(f\"Ollama host: {OLLAMA_HOST}\")\nprint(f\"  (source: {'OLLAMA_HOST env var' if 'OLLAMA_HOST' in os.environ else 'default fallback'})\")\nprint(f\"Default model: {DEFAULT_MODEL}\")"
  },
  {
   "cell_type": "markdown",
   "id": "section-2-header",
   "metadata": {},
   "source": [
    "## 2. Connection Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "health-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ollama_health() -> bool:\n",
    "    \"\"\"Check if Ollama server is running and accessible.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"Ollama server is running!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Ollama returned unexpected status: {response.status_code}\")\n",
    "            return False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"ERROR: Cannot connect to Ollama server!\")\n",
    "        print(\"\")\n",
    "        print(\"To fix this, run:\")\n",
    "        print(\"  ujust ollama start\")\n",
    "        print(\"\")\n",
    "        print(\"Then re-run this cell.\")\n",
    "        return False\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"ERROR: Connection to Ollama timed out!\")\n",
    "        return False\n",
    "\n",
    "ollama_healthy = check_ollama_health()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-header",
   "metadata": {},
   "source": [
    "## 3. List Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list-models-requests",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Method 1: Using requests ===\n",
    "print(\"=== Using requests library ===\")\n",
    "response = requests.get(f\"{OLLAMA_HOST}/api/tags\")\n",
    "models_data = response.json()\n",
    "\n",
    "if models_data.get(\"models\"):\n",
    "    for model in models_data[\"models\"]:\n",
    "        size_gb = model.get(\"size\", 0) / (1024**3)\n",
    "        print(f\"  - {model['name']} ({size_gb:.2f} GB)\")\n",
    "else:\n",
    "    print(\"  No models found. Run: ujust ollama pull llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list-models-ollama",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Method 2: Using ollama library ===\n",
    "print(\"=== Using ollama library ===\")\n",
    "models = ollama.list()\n",
    "\n",
    "if models.get(\"models\"):\n",
    "    for model in models[\"models\"]:\n",
    "        size_gb = model.get(\"size\", 0) / (1024**3)\n",
    "        print(f\"  - {model['name']} ({size_gb:.2f} GB)\")\n",
    "else:\n",
    "    print(\"  No models found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list-models-openai",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Method 3: Using OpenAI compatibility layer ===\n",
    "print(\"=== Using OpenAI compatibility layer ===\")\n",
    "\n",
    "# Initialize OpenAI client pointing to Ollama\n",
    "openai_client = OpenAI(\n",
    "    base_url=f\"{OLLAMA_HOST}/v1\",\n",
    "    api_key=\"ollama\"  # Required by library but ignored by Ollama\n",
    ")\n",
    "\n",
    "models = openai_client.models.list()\n",
    "for model in models.data:\n",
    "    print(f\"  - {model.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-header",
   "metadata": {},
   "source": [
    "## 4. Basic Generation (requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-requests",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Generation using requests ===\")\n",
    "print(f\"Prompt: {TEST_PROMPT}\")\n",
    "print()\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "response = requests.post(\n",
    "    f\"{OLLAMA_HOST}/api/generate\",\n",
    "    json={\n",
    "        \"model\": DEFAULT_MODEL,\n",
    "        \"prompt\": TEST_PROMPT,\n",
    "        \"stream\": False\n",
    "    }\n",
    ")\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "result = response.json()\n",
    "print(f\"Response: {result['response']}\")\n",
    "print()\n",
    "print(f\"Latency: {end_time - start_time:.2f}s\")\n",
    "print(f\"Eval tokens: {result.get('eval_count', 'N/A')}\")\n",
    "print(f\"Eval duration: {result.get('eval_duration', 0) / 1e9:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-header",
   "metadata": {},
   "source": [
    "## 5. Basic Generation (ollama library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-ollama",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Generation using ollama library ===\")\n",
    "print(f\"Prompt: {TEST_PROMPT}\")\n",
    "print()\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "result = ollama.generate(\n",
    "    model=DEFAULT_MODEL,\n",
    "    prompt=TEST_PROMPT\n",
    ")\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "print(f\"Response: {result['response']}\")\n",
    "print()\n",
    "print(f\"Latency: {end_time - start_time:.2f}s\")\n",
    "print(f\"Eval tokens: {result.get('eval_count', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6-header",
   "metadata": {},
   "source": [
    "## 6. Basic Generation (OpenAI compatibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-openai",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Generation using OpenAI compatibility layer ===\")\n",
    "print(f\"Prompt: {TEST_PROMPT}\")\n",
    "print()\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "response = openai_client.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    prompt=TEST_PROMPT,\n",
    "    max_tokens=100\n",
    ")\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "print(f\"Response: {response.choices[0].text}\")\n",
    "print()\n",
    "print(f\"Latency: {end_time - start_time:.2f}s\")\n",
    "print(f\"Completion tokens: {response.usage.completion_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-header",
   "metadata": {},
   "source": [
    "## 7. Chat Completion (requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chat-requests",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Chat using requests ===\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Keep responses brief.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is Python?\"}\n",
    "]\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{OLLAMA_HOST}/api/chat\",\n",
    "    json={\n",
    "        \"model\": DEFAULT_MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "result = response.json()\n",
    "print(f\"Assistant: {result['message']['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8-header",
   "metadata": {},
   "source": [
    "## 8. Chat Completion (ollama library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chat-ollama",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Chat using ollama library ===\")\n",
    "\n",
    "# Multi-turn conversation\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is 2 + 2?\"},\n",
    "]\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "print(f\"User: What is 2 + 2?\")\n",
    "print(f\"Assistant: {response['message']['content']}\")\n",
    "\n",
    "# Continue conversation\n",
    "messages.append(response[\"message\"])\n",
    "messages.append({\"role\": \"user\", \"content\": \"And what is that multiplied by 3?\"})\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "print(f\"User: And what is that multiplied by 3?\")\n",
    "print(f\"Assistant: {response['message']['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9-header",
   "metadata": {},
   "source": [
    "## 9. Chat Completion (OpenAI compatibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chat-openai",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Chat using OpenAI compatibility layer ===\")\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Keep responses brief.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain machine learning in one sentence.\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {response.choices[0].message.content}\")\n",
    "print(f\"\\nTokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-10-header",
   "metadata": {},
   "source": [
    "## 10. Streaming Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-requests",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Streaming with requests ===\")\n",
    "print(\"Response: \", end=\"\", flush=True)\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{OLLAMA_HOST}/api/generate\",\n",
    "    json={\n",
    "        \"model\": DEFAULT_MODEL,\n",
    "        \"prompt\": \"Count from 1 to 5.\",\n",
    "        \"stream\": True\n",
    "    },\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for line in response.iter_lines():\n",
    "    if line:\n",
    "        chunk = json.loads(line)\n",
    "        print(chunk.get(\"response\", \"\"), end=\"\", flush=True)\n",
    "        if chunk.get(\"done\"):\n",
    "            break\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-ollama",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Streaming with ollama library ===\")\n",
    "print(\"Response: \", end=\"\", flush=True)\n",
    "\n",
    "stream = ollama.generate(\n",
    "    model=DEFAULT_MODEL,\n",
    "    prompt=\"Count from 1 to 5.\",\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk[\"response\"], end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-openai",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Streaming with OpenAI compatibility layer ===\")\n",
    "print(\"Response: \", end=\"\", flush=True)\n",
    "\n",
    "stream = openai_client.chat.completions.create(\n",
    "    model=DEFAULT_MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Count from 1 to 5.\"}],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-11-header",
   "metadata": {},
   "source": [
    "## 11. Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "error-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Testing Error Handling ===\")\n",
    "\n",
    "# Test 1: Non-existent model\n",
    "print(\"\\n1. Testing non-existent model...\")\n",
    "try:\n",
    "    result = ollama.generate(\n",
    "        model=\"nonexistent-model-xyz\",\n",
    "        prompt=\"Hello\"\n",
    "    )\n",
    "    print(f\"   Unexpected success: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Expected error: {type(e).__name__}: {e}\")\n",
    "\n",
    "# Test 2: Empty prompt\n",
    "print(\"\\n2. Testing empty prompt...\")\n",
    "try:\n",
    "    result = ollama.generate(\n",
    "        model=DEFAULT_MODEL,\n",
    "        prompt=\"\"\n",
    "    )\n",
    "    print(f\"   Response received (empty prompts allowed): {result['response'][:50]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {type(e).__name__}: {e}\")\n",
    "\n",
    "# Test 3: OpenAI client with invalid model\n",
    "print(\"\\n3. Testing OpenAI client with invalid model...\")\n",
    "try:\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"invalid-model\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "    )\n",
    "    print(f\"   Unexpected success\")\n",
    "except Exception as e:\n",
    "    print(f\"   Expected error: {type(e).__name__}\")\n",
    "\n",
    "print(\"\\nError handling tests completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-12-header",
   "metadata": {},
   "source": [
    "## 12. Performance Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_api(name: str, api_func: Callable, n_runs: int = BENCHMARK_RUNS) -> dict:\n",
    "    \"\"\"Benchmark an API function.\"\"\"\n",
    "    latencies = []\n",
    "    tokens_per_second = []\n",
    "    \n",
    "    for i in range(n_runs):\n",
    "        start = time.perf_counter()\n",
    "        result = api_func()\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        latency = end - start\n",
    "        latencies.append(latency)\n",
    "        \n",
    "        # Extract token count based on result format\n",
    "        if isinstance(result, dict):\n",
    "            tokens = result.get(\"eval_count\", 0)\n",
    "        elif hasattr(result, \"usage\"):\n",
    "            tokens = result.usage.completion_tokens if result.usage else 0\n",
    "        else:\n",
    "            tokens = 0\n",
    "        \n",
    "        if tokens > 0:\n",
    "            tokens_per_second.append(tokens / latency)\n",
    "    \n",
    "    return {\n",
    "        \"API\": name,\n",
    "        \"Mean Latency (s)\": statistics.mean(latencies),\n",
    "        \"Std Dev (s)\": statistics.stdev(latencies) if len(latencies) > 1 else 0,\n",
    "        \"Min (s)\": min(latencies),\n",
    "        \"Max (s)\": max(latencies),\n",
    "        \"Tokens/sec\": statistics.mean(tokens_per_second) if tokens_per_second else \"N/A\"\n",
    "    }\n",
    "\n",
    "print(f\"Benchmark functions defined. Will run {BENCHMARK_RUNS} iterations per API.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-benchmarks",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Running benchmarks with prompt: '{TEST_PROMPT}'\")\n",
    "print(f\"Iterations per API: {BENCHMARK_RUNS}\")\n",
    "print()\n",
    "\n",
    "# Define API functions\n",
    "def requests_generate():\n",
    "    response = requests.post(\n",
    "        f\"{OLLAMA_HOST}/api/generate\",\n",
    "        json={\"model\": DEFAULT_MODEL, \"prompt\": TEST_PROMPT, \"stream\": False}\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "def ollama_generate():\n",
    "    return ollama.generate(model=DEFAULT_MODEL, prompt=TEST_PROMPT)\n",
    "\n",
    "def openai_generate():\n",
    "    return openai_client.completions.create(\n",
    "        model=DEFAULT_MODEL,\n",
    "        prompt=TEST_PROMPT,\n",
    "        max_tokens=100\n",
    "    )\n",
    "\n",
    "# Run benchmarks\n",
    "results = []\n",
    "\n",
    "print(\"Benchmarking requests library...\", flush=True)\n",
    "results.append(benchmark_api(\"requests\", requests_generate))\n",
    "\n",
    "print(\"Benchmarking ollama library...\", flush=True)\n",
    "results.append(benchmark_api(\"ollama\", ollama_generate))\n",
    "\n",
    "print(\"Benchmarking OpenAI compatibility...\", flush=True)\n",
    "results.append(benchmark_api(\"openai\", openai_generate))\n",
    "\n",
    "# Display results\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\n=== Benchmark Results ===\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark-chat",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Chat API Benchmarks ===\")\n",
    "print()\n",
    "\n",
    "chat_messages = [{\"role\": \"user\", \"content\": TEST_PROMPT}]\n",
    "\n",
    "def requests_chat():\n",
    "    response = requests.post(\n",
    "        f\"{OLLAMA_HOST}/api/chat\",\n",
    "        json={\"model\": DEFAULT_MODEL, \"messages\": chat_messages, \"stream\": False}\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "def ollama_chat():\n",
    "    return ollama.chat(model=DEFAULT_MODEL, messages=chat_messages)\n",
    "\n",
    "def openai_chat():\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=DEFAULT_MODEL,\n",
    "        messages=chat_messages,\n",
    "        max_tokens=100\n",
    "    )\n",
    "\n",
    "# Run chat benchmarks\n",
    "chat_results = []\n",
    "\n",
    "print(\"Benchmarking requests chat...\", flush=True)\n",
    "chat_results.append(benchmark_api(\"requests (chat)\", requests_chat))\n",
    "\n",
    "print(\"Benchmarking ollama chat...\", flush=True)\n",
    "chat_results.append(benchmark_api(\"ollama (chat)\", ollama_chat))\n",
    "\n",
    "print(\"Benchmarking OpenAI chat...\", flush=True)\n",
    "chat_results.append(benchmark_api(\"openai (chat)\", openai_chat))\n",
    "\n",
    "# Display results\n",
    "df_chat = pd.DataFrame(chat_results)\n",
    "print(\"\\n=== Chat Benchmark Results ===\")\n",
    "print(df_chat.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-13-header",
   "metadata": {},
   "source": [
    "## 13. GPU Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "print(\"=== GPU Status ===\")\n",
    "\n",
    "# Check if nvidia-smi is available\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"nvidia-smi\", \"--query-gpu=name,memory.used,memory.total,utilization.gpu\", \"--format=csv,noheader,nounits\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=5\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        lines = result.stdout.strip().split(\"\\n\")\n",
    "        for i, line in enumerate(lines):\n",
    "            parts = line.split(\", \")\n",
    "            if len(parts) >= 4:\n",
    "                name, mem_used, mem_total, util = parts\n",
    "                print(f\"GPU {i}: {name}\")\n",
    "                print(f\"  Memory: {mem_used} MB / {mem_total} MB\")\n",
    "                print(f\"  Utilization: {util}%\")\n",
    "    else:\n",
    "        print(\"nvidia-smi returned an error\")\n",
    "        print(result.stderr)\n",
    "except FileNotFoundError:\n",
    "    print(\"nvidia-smi not found - NVIDIA GPU may not be available\")\n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"nvidia-smi timed out\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking GPU: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu-inference-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== GPU Usage During Inference ===\")\n",
    "print(\"Running inference and checking GPU metrics...\")\n",
    "print()\n",
    "\n",
    "# Run a generation to load the model\n",
    "result = ollama.generate(\n",
    "    model=DEFAULT_MODEL,\n",
    "    prompt=\"Write a haiku about computers.\"\n",
    ")\n",
    "\n",
    "print(f\"Response: {result['response']}\")\n",
    "print()\n",
    "\n",
    "# Check Ollama's reported metrics\n",
    "print(\"Ollama Inference Metrics:\")\n",
    "print(f\"  Prompt eval count: {result.get('prompt_eval_count', 'N/A')}\")\n",
    "print(f\"  Prompt eval duration: {result.get('prompt_eval_duration', 0) / 1e9:.3f}s\")\n",
    "print(f\"  Eval count (tokens generated): {result.get('eval_count', 'N/A')}\")\n",
    "print(f\"  Eval duration: {result.get('eval_duration', 0) / 1e9:.3f}s\")\n",
    "print(f\"  Total duration: {result.get('total_duration', 0) / 1e9:.3f}s\")\n",
    "\n",
    "if result.get('eval_count') and result.get('eval_duration'):\n",
    "    tokens_per_sec = result['eval_count'] / (result['eval_duration'] / 1e9)\n",
    "    print(f\"  Tokens/second: {tokens_per_sec:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated three ways to interact with the Ollama pod:\n",
    "\n",
    "| Method | Best For | Pros | Cons |\n",
    "|--------|----------|------|------|\n",
    "| **requests** | Maximum control | No dependencies, full API access | Verbose, manual JSON handling |\n",
    "| **ollama** | Ollama-specific features | Clean API, streaming support | Ollama-only |\n",
    "| **openai** | OpenAI migration | Standard API, tool ecosystem | Slight overhead |\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```python\n",
    "# requests\n",
    "requests.post(\"http://localhost:11434/api/generate\", json={...})\n",
    "\n",
    "# ollama\n",
    "ollama.generate(model=\"llama3.2\", prompt=\"...\")\n",
    "\n",
    "# openai\n",
    "client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "client.chat.completions.create(model=\"llama3.2\", messages=[...])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
