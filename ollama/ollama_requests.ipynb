{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama REST API (requests)\n",
    "\n",
    "This notebook demonstrates the Ollama REST API using the `requests` library.\n",
    "\n",
    "## Features Covered\n",
    "\n",
    "- List models\n",
    "- Show model details\n",
    "- List running models\n",
    "- Generate response\n",
    "- Chat completion\n",
    "- Streaming responses\n",
    "- Generate embeddings\n",
    "- Copy and delete models\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Ollama pod running: `ujust ollama start`\n",
    "- Model pulled: `ujust ollama pull llama3.2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama host: http://ollama:11434\n",
       "Default model: llama3.2:latest\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# === Configuration ===\n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\n",
    "DEFAULT_MODEL = \"llama3.2:latest\"\n",
    "\n",
    "print(f\"Ollama host: {OLLAMA_HOST}\")\n",
    "print(f\"Default model: {DEFAULT_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connection Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "✓ Ollama server is running!\n",
       "✓ Model 'llama3.2:latest' is available\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_ollama_health() -> tuple[bool, bool]:\n",
    "    \"\"\"Check if Ollama server is running and model is available.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (server_healthy, model_available)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"✓ Ollama server is running!\")\n",
    "            models = response.json()\n",
    "            model_names = [m.get(\"name\", \"\") for m in models.get(\"models\", [])]\n",
    "            \n",
    "            if DEFAULT_MODEL in model_names:\n",
    "                print(f\"✓ Model '{DEFAULT_MODEL}' is available\")\n",
    "                return True, True\n",
    "            else:\n",
    "                print(f\"✗ Model '{DEFAULT_MODEL}' not found!\")\n",
    "                print()\n",
    "                if model_names:\n",
    "                    print(\"Available models:\")\n",
    "                    for name in model_names:\n",
    "                        print(f\"  - {name}\")\n",
    "                else:\n",
    "                    print(\"No models installed.\")\n",
    "                print()\n",
    "                print(\"To fix this, run:\")\n",
    "                print(f\"  ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "                return True, False\n",
    "        else:\n",
    "            print(f\"Ollama returned unexpected status: {response.status_code}\")\n",
    "            return False, False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"✗ Cannot connect to Ollama server!\")\n",
    "        print(\"To fix this, run: ujust ollama start\")\n",
    "        return False, False\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"✗ Connection to Ollama timed out!\")\n",
    "        return False, False\n",
    "\n",
    "ollama_healthy, model_available = check_ollama_health()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. List Models\n",
    "\n",
    "**Endpoint:** `GET /api/tags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== List Available Models ===\n",
       "  - hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M (4.07 GB)\n",
       "  - llama3.2:latest (1.88 GB)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== List Available Models ===\")\n",
    "\n",
    "response = requests.get(f\"{OLLAMA_HOST}/api/tags\")\n",
    "models_data = response.json()\n",
    "\n",
    "if models_data.get(\"models\"):\n",
    "    for model in models_data[\"models\"]:\n",
    "        size_gb = model.get(\"size\", 0) / (1024**3)\n",
    "        print(f\"  - {model['name']} ({size_gb:.2f} GB)\")\n",
    "else:\n",
    "    print(\"  No models found. Run: ujust ollama pull llama3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Show Model Details\n",
    "\n",
    "**Endpoint:** `POST /api/show`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Show Model Details ===\n",
       "Model: llama3.2:latest\n",
       "\n",
       "Details:\n",
       "  Family: llama\n",
       "  Parameter Size: 3.2B\n",
       "  Quantization: Q4_K_M\n",
       "\n",
       "Template preview:\n",
       "  <|start_header_id|>system<|end_header_id|>\n",
       "\n",
       "Cutting Knowledge Date: December 2023\n",
       "\n",
       "{{ if .System }}{{ .System }}\n",
       "{{- end }}\n",
       "{{- if .Tools }}When you receive a tool call response, use the output to for...\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== Show Model Details ===\")\n",
    "\n",
    "if not model_available:\n",
    "    print()\n",
    "    print(\"⚠ Skipping - model not available\")\n",
    "    print(f\"  Run: ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "else:\n",
    "    response = requests.post(\n",
    "        f\"{OLLAMA_HOST}/api/show\",\n",
    "        json={\"model\": DEFAULT_MODEL}\n",
    "    )\n",
    "    model_info = response.json()\n",
    "\n",
    "    if \"error\" in model_info:\n",
    "        print(f\"✗ Error: {model_info['error']}\")\n",
    "    else:\n",
    "        print(f\"Model: {DEFAULT_MODEL}\")\n",
    "        print(f\"\\nDetails:\")\n",
    "        if \"details\" in model_info:\n",
    "            details = model_info[\"details\"]\n",
    "            print(f\"  Family: {details.get('family', 'N/A')}\")\n",
    "            print(f\"  Parameter Size: {details.get('parameter_size', 'N/A')}\")\n",
    "            print(f\"  Quantization: {details.get('quantization_level', 'N/A')}\")\n",
    "\n",
    "        print(f\"\\nTemplate preview:\")\n",
    "        template = model_info.get(\"template\", \"N/A\")\n",
    "        print(f\"  {template[:200]}...\" if len(template) > 200 else f\"  {template}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. List Running Models\n",
    "\n",
    "**Endpoint:** `GET /api/ps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== List Running Models ===\n",
       "  - llama3.2:latest\n",
       "    Size: 2.56 GB | VRAM: 2.56 GB\n",
       "    Expires: 2025-12-28T20:29:33.116691371Z\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== List Running Models ===\")\n",
    "\n",
    "response = requests.get(f\"{OLLAMA_HOST}/api/ps\")\n",
    "running = response.json()\n",
    "\n",
    "if running.get(\"models\"):\n",
    "    for model in running[\"models\"]:\n",
    "        name = model.get(\"name\", \"Unknown\")\n",
    "        size = model.get(\"size\", 0) / (1024**3)\n",
    "        vram = model.get(\"size_vram\", 0) / (1024**3)\n",
    "        expires = model.get(\"expires_at\", \"N/A\")\n",
    "        print(f\"  - {name}\")\n",
    "        print(f\"    Size: {size:.2f} GB | VRAM: {vram:.2f} GB\")\n",
    "        print(f\"    Expires: {expires}\")\n",
    "else:\n",
    "    print(\"  No models currently loaded in memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Response\n",
    "\n",
    "**Endpoint:** `POST /api/generate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Generate Response ===\n",
       "Prompt: Why is the sky blue? Answer in one sentence.\n",
       "\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Response: The sky appears blue because of a phenomenon called Rayleigh scattering, where shorter wavelengths of light (such as blue and violet) are scattered more than longer wavelengths by the tiny molecules of gases in the Earth's atmosphere.\n",
       "\n",
       "Latency: 3.33s\n",
       "Eval tokens: 44\n",
       "Eval duration: 0.17s\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== Generate Response ===\")\n",
    "\n",
    "if not model_available:\n",
    "    print()\n",
    "    print(\"⚠ Skipping - model not available\")\n",
    "    print(f\"  Run: ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "else:\n",
    "    prompt = \"Why is the sky blue? Answer in one sentence.\"\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print()\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    response = requests.post(\n",
    "        f\"{OLLAMA_HOST}/api/generate\",\n",
    "        json={\n",
    "            \"model\": DEFAULT_MODEL,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    result = response.json()\n",
    "    if \"error\" in result:\n",
    "        print(f\"✗ Error: {result['error']}\")\n",
    "    else:\n",
    "        print(f\"Response: {result['response']}\")\n",
    "        print()\n",
    "        print(f\"Latency: {end_time - start_time:.2f}s\")\n",
    "        print(f\"Eval tokens: {result.get('eval_count', 'N/A')}\")\n",
    "        print(f\"Eval duration: {result.get('eval_duration', 0) / 1e9:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Chat Completion\n",
    "\n",
    "**Endpoint:** `POST /api/chat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Chat Completion ===\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Assistant: Python is a high-level, interpreted programming language that's easy to learn and use. It's known for its simplicity, readability, and versatility, making it a popular choice for web development, data analysis, artificial intelligence, and more.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== Chat Completion ===\")\n",
    "\n",
    "if not model_available:\n",
    "    print()\n",
    "    print(\"⚠ Skipping - model not available\")\n",
    "    print(f\"  Run: ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "else:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Keep responses brief.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is Python?\"}\n",
    "    ]\n",
    "\n",
    "    response = requests.post(\n",
    "        f\"{OLLAMA_HOST}/api/chat\",\n",
    "        json={\n",
    "            \"model\": DEFAULT_MODEL,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "\n",
    "    result = response.json()\n",
    "    if \"error\" in result:\n",
    "        print(f\"✗ Error: {result['error']}\")\n",
    "    else:\n",
    "        print(f\"Assistant: {result['message']['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Streaming Response\n",
    "\n",
    "**Endpoint:** `POST /api/generate` with `stream: true`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Streaming Response ===\n",
       "\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Response: Here we go:\n",
       "\n",
       "1\n",
       "2\n",
       "3\n",
       "4\n",
       "5\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== Streaming Response ===\")\n",
    "\n",
    "if not model_available:\n",
    "    print()\n",
    "    print(\"⚠ Skipping - model not available\")\n",
    "    print(f\"  Run: ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "else:\n",
    "    print()\n",
    "\n",
    "    response = requests.post(\n",
    "        f\"{OLLAMA_HOST}/api/generate\",\n",
    "        json={\n",
    "            \"model\": DEFAULT_MODEL,\n",
    "            \"prompt\": \"Count from 1 to 5.\",\n",
    "            \"stream\": True\n",
    "        },\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    collected = []\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            chunk = json.loads(line)\n",
    "            if \"error\" in chunk:\n",
    "                print(f\"✗ Error: {chunk['error']}\")\n",
    "                break\n",
    "            token = chunk.get(\"response\", \"\")\n",
    "            collected.append(token)\n",
    "            if chunk.get(\"done\"):\n",
    "                break\n",
    "\n",
    "    if collected:\n",
    "        print(f\"Response: {''.join(collected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Embeddings\n",
    "\n",
    "**Endpoint:** `POST /api/embed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Generate Embeddings ===\n",
       "Input: 'Ollama makes running LLMs locally easy and efficient.'\n",
       "Embedding dimensions: 3072\n",
       "First 5 values: [-0.026683128, -0.0028091324, -0.027384995, -0.009667068, -0.017405545]\n",
       "Last 5 values: [-0.028065814, 0.010568945, -0.028453464, 0.014874469, -0.029712567]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== Generate Embeddings ===\")\n",
    "\n",
    "if not model_available:\n",
    "    print()\n",
    "    print(\"⚠ Skipping - model not available\")\n",
    "    print(f\"  Run: ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "else:\n",
    "    test_text = \"Ollama makes running LLMs locally easy and efficient.\"\n",
    "\n",
    "    response = requests.post(\n",
    "        f\"{OLLAMA_HOST}/api/embed\",\n",
    "        json={\n",
    "            \"model\": DEFAULT_MODEL,\n",
    "            \"input\": test_text\n",
    "        }\n",
    "    )\n",
    "    result = response.json()\n",
    "\n",
    "    if \"error\" in result:\n",
    "        print(f\"✗ Error: {result['error']}\")\n",
    "    else:\n",
    "        embeddings = result.get(\"embeddings\", [[]])[0]\n",
    "        print(f\"Input: '{test_text}'\")\n",
    "        print(f\"Embedding dimensions: {len(embeddings)}\")\n",
    "        print(f\"First 5 values: {embeddings[:5]}\")\n",
    "        print(f\"Last 5 values: {embeddings[-5:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Copy and Delete Model\n",
    "\n",
    "**Endpoints:** `POST /api/copy`, `DELETE /api/delete`\n",
    "\n",
    "**Warning:** Delete is permanent! We safely demonstrate by copying first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Copy and Delete Model ===\n",
       "\n",
       "1. Copying 'llama3.2:latest' to 'llama3.2-test-copy:latest'...\n",
       "   Copy successful!\n",
       "\n",
       "2. Verifying 'llama3.2-test-copy:latest' exists...\n",
       "   Found 'llama3.2-test-copy:latest' in model list\n",
       "\n",
       "3. Deleting 'llama3.2-test-copy:latest'...\n",
       "   Delete successful!\n",
       "\n",
       "4. Verifying 'llama3.2-test-copy:latest' is deleted...\n",
       "   'llama3.2-test-copy:latest' successfully removed\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== Copy and Delete Model ===\")\n",
    "\n",
    "if not model_available:\n",
    "    print()\n",
    "    print(\"⚠ Skipping - model not available\")\n",
    "    print(f\"  Run: ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "else:\n",
    "    COPY_NAME = f\"{DEFAULT_MODEL.split(':')[0]}-test-copy:latest\"\n",
    "\n",
    "    # Step 1: Copy the model\n",
    "    print(f\"\\n1. Copying '{DEFAULT_MODEL}' to '{COPY_NAME}'...\")\n",
    "    response = requests.post(\n",
    "        f\"{OLLAMA_HOST}/api/copy\",\n",
    "        json={\n",
    "            \"source\": DEFAULT_MODEL,\n",
    "            \"destination\": COPY_NAME\n",
    "        }\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        print(f\"   Copy successful!\")\n",
    "    else:\n",
    "        print(f\"   Copy failed: {response.text}\")\n",
    "\n",
    "    # Step 2: Verify the copy exists\n",
    "    print(f\"\\n2. Verifying '{COPY_NAME}' exists...\")\n",
    "    response = requests.get(f\"{OLLAMA_HOST}/api/tags\")\n",
    "    models = [m[\"name\"] for m in response.json().get(\"models\", [])]\n",
    "    if COPY_NAME in models:\n",
    "        print(f\"   Found '{COPY_NAME}' in model list\")\n",
    "    else:\n",
    "        print(f\"   '{COPY_NAME}' not found\")\n",
    "\n",
    "    # Step 3: Delete the copy\n",
    "    print(f\"\\n3. Deleting '{COPY_NAME}'...\")\n",
    "    response = requests.delete(\n",
    "        f\"{OLLAMA_HOST}/api/delete\",\n",
    "        json={\"model\": COPY_NAME}\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        print(f\"   Delete successful!\")\n",
    "    else:\n",
    "        print(f\"   Delete failed: {response.text}\")\n",
    "\n",
    "    # Step 4: Verify deletion\n",
    "    print(f\"\\n4. Verifying '{COPY_NAME}' is deleted...\")\n",
    "    response = requests.get(f\"{OLLAMA_HOST}/api/tags\")\n",
    "    models = [m[\"name\"] for m in response.json().get(\"models\", [])]\n",
    "    if COPY_NAME not in models:\n",
    "        print(f\"   '{COPY_NAME}' successfully removed\")\n",
    "    else:\n",
    "        print(f\"   '{COPY_NAME}' still exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Error Handling ===\n",
       "\n",
       "1. Testing non-existent model...\n",
       "   Expected error: 404 - {\"error\":\"model 'nonexistent-model-xyz' not found\"}\n",
       "\n",
       "2. Testing empty prompt...\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "   Empty prompts allowed (returned response)\n",
       "\n",
       "Error handling tests completed!\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== Error Handling ===\")\n",
    "\n",
    "# Test: Non-existent model\n",
    "print(\"\\n1. Testing non-existent model...\")\n",
    "response = requests.post(\n",
    "    f\"{OLLAMA_HOST}/api/generate\",\n",
    "    json={\n",
    "        \"model\": \"nonexistent-model-xyz\",\n",
    "        \"prompt\": \"Hello\",\n",
    "        \"stream\": False\n",
    "    }\n",
    ")\n",
    "if response.status_code != 200:\n",
    "    print(f\"   Expected error: {response.status_code} - {response.text[:100]}\")\n",
    "else:\n",
    "    print(f\"   Unexpected success\")\n",
    "\n",
    "# Test: Empty prompt\n",
    "print(\"\\n2. Testing empty prompt...\")\n",
    "response = requests.post(\n",
    "    f\"{OLLAMA_HOST}/api/generate\",\n",
    "    json={\n",
    "        \"model\": DEFAULT_MODEL,\n",
    "        \"prompt\": \"\",\n",
    "        \"stream\": False\n",
    "    }\n",
    ")\n",
    "if response.status_code == 200:\n",
    "    print(f\"   Empty prompts allowed (returned response)\")\n",
    "else:\n",
    "    print(f\"   Error: {response.status_code}\")\n",
    "\n",
    "print(\"\\nError handling tests completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the Ollama REST API using the `requests` library.\n",
    "\n",
    "### API Endpoints Used\n",
    "\n",
    "| Endpoint | Method | Purpose |\n",
    "|----------|--------|--------|\n",
    "| `/api/tags` | GET | List models |\n",
    "| `/api/show` | POST | Show model details |\n",
    "| `/api/ps` | GET | List running models |\n",
    "| `/api/generate` | POST | Generate text |\n",
    "| `/api/chat` | POST | Chat completion |\n",
    "| `/api/embed` | POST | Generate embeddings |\n",
    "| `/api/copy` | POST | Copy a model |\n",
    "| `/api/delete` | DELETE | Delete a model |\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "# Generate\n",
    "requests.post(\"http://ollama:11434/api/generate\", json={\"model\": \"llama3.2\", \"prompt\": \"...\"})\n",
    "\n",
    "# Chat\n",
    "requests.post(\"http://ollama:11434/api/chat\", json={\"model\": \"llama3.2\", \"messages\": [...]})\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
