{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama Python Library\n",
    "\n",
    "This notebook demonstrates the official `ollama` Python library.\n",
    "\n",
    "## Features Covered\n",
    "\n",
    "- List models\n",
    "- Show model details\n",
    "- List running models\n",
    "- Generate response\n",
    "- Chat completion\n",
    "- Streaming responses\n",
    "- Generate embeddings\n",
    "- Copy and delete models\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Ollama pod running: `ujust ollama start`\n",
    "- Model pulled: `ujust ollama pull llama3.2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama host: http://ollama:11434\n",
      "Default model: llama3.2:latest"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import ollama\n",
    "\n",
    "# === Configuration ===\n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\n",
    "DEFAULT_MODEL = \"llama3.2:latest\"\n",
    "\n",
    "print(f\"Ollama host: {OLLAMA_HOST}\")\n",
    "print(f\"Default model: {DEFAULT_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connection Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ollama server is running!\n",
      "✓ Model 'llama3.2:latest' is available"
     ]
    }
   ],
   "source": [
    "def check_ollama_health() -> tuple[bool, bool]:\n",
    "    \"\"\"Check if Ollama server is running and model is available.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (server_healthy, model_available)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        models = ollama.list()\n",
    "        print(\"✓ Ollama server is running!\")\n",
    "        model_names = [m.get(\"model\", \"\") for m in models.get(\"models\", [])]\n",
    "        \n",
    "        if DEFAULT_MODEL in model_names:\n",
    "            print(f\"✓ Model '{DEFAULT_MODEL}' is available\")\n",
    "            return True, True\n",
    "        else:\n",
    "            print(f\"✗ Model '{DEFAULT_MODEL}' not found!\")\n",
    "            print()\n",
    "            if model_names:\n",
    "                print(\"Available models:\")\n",
    "                for name in model_names:\n",
    "                    print(f\"  - {name}\")\n",
    "            else:\n",
    "                print(\"No models installed.\")\n",
    "            print()\n",
    "            print(\"To fix this, run:\")\n",
    "            print(f\"  ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "            return True, False\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Cannot connect to Ollama server!\")\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"To fix this, run: ujust ollama start\")\n",
    "        return False, False\n",
    "\n",
    "ollama_healthy, model_available = check_ollama_health()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. List Models\n",
    "\n",
    "**Function:** `ollama.list()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== List Available Models ===\n",
      "  - llama3.2:latest (1.88 GB)"
     ]
    }
   ],
   "source": [
    "print(\"=== List Available Models ===\")\n",
    "\n",
    "models = ollama.list()\n",
    "\n",
    "if models.get(\"models\"):\n",
    "    for model in models[\"models\"]:\n",
    "        size_gb = model.get(\"size\", 0) / (1024**3)\n",
    "        print(f\"  - {model['model']} ({size_gb:.2f} GB)\")\n",
    "else:\n",
    "    print(\"  No models found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Show Model Details\n",
    "\n",
    "**Function:** `ollama.show()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Show Model Details ===\n",
      "Model: llama3.2:latest\n",
      "\n",
      "Details:\n",
      "  Family: llama\n",
      "  Parameter Size: 3.2B\n",
      "  Quantization: Q4_K_M\n",
      "\n",
      "Model file preview:\n",
      "  # Modelfile generated by \"ollama show\"\n",
      "# To build a new Modelfile based on this, replace FROM with:\n",
      "# FROM llama3.2:latest\n",
      "\n",
      "FROM /home/jovian/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff\n",
      "TEMPLATE \"\"\"<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting K..."
     ]
    }
   ],
   "source": [
    "print(\"=== Show Model Details ===\")\n",
    "\n",
    "if not model_available:\n",
    "    print()\n",
    "    print(\"⚠ Skipping - model not available\")\n",
    "    print(f\"  Run: ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "else:\n",
    "    try:\n",
    "        model_info = ollama.show(DEFAULT_MODEL)\n",
    "\n",
    "        print(f\"Model: {DEFAULT_MODEL}\")\n",
    "        print(f\"\\nDetails:\")\n",
    "        if \"details\" in model_info:\n",
    "            details = model_info[\"details\"]\n",
    "            print(f\"  Family: {details.get('family', 'N/A')}\")\n",
    "            print(f\"  Parameter Size: {details.get('parameter_size', 'N/A')}\")\n",
    "            print(f\"  Quantization: {details.get('quantization_level', 'N/A')}\")\n",
    "\n",
    "        print(f\"\\nModel file preview:\")\n",
    "        modelfile = model_info.get(\"modelfile\", \"N/A\")\n",
    "        print(f\"  {modelfile[:300]}...\" if len(modelfile) > 300 else f\"  {modelfile}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. List Running Models\n",
    "\n",
    "**Function:** `ollama.ps()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== List Running Models ===\n",
      "  - llama3.2:latest\n",
      "    Size: 2.56 GB | VRAM: 2.56 GB"
     ]
    }
   ],
   "source": [
    "print(\"=== List Running Models ===\")\n",
    "\n",
    "running = ollama.ps()\n",
    "\n",
    "if running.get(\"models\"):\n",
    "    for model in running[\"models\"]:\n",
    "        name = model.get(\"name\", \"Unknown\")\n",
    "        size = model.get(\"size\", 0) / (1024**3)\n",
    "        vram = model.get(\"size_vram\", 0) / (1024**3)\n",
    "        print(f\"  - {name}\")\n",
    "        print(f\"    Size: {size:.2f} GB | VRAM: {vram:.2f} GB\")\n",
    "else:\n",
    "    print(\"  No models currently loaded in memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Response\n",
    "\n",
    "**Function:** `ollama.generate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generate Response ===\n",
      "Prompt: Why is the sky blue? Answer in one sentence.\n",
      "Response: The sky appears blue because of a phenomenon called Rayleigh scattering, where shorter wavelengths of light (such as blue and violet) are scattered more efficiently by the tiny molecules of gases in the Earth's atmosphere than longer wavelengths (like red and orange).\n",
      "\n",
      "Latency: 0.29s\n",
      "Eval tokens: 50"
     ]
    }
   ],
   "source": [
    "print(\"=== Generate Response ===\")\n",
    "\n",
    "if not model_available:\n",
    "    print()\n",
    "    print(\"⚠ Skipping - model not available\")\n",
    "    print(f\"  Run: ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "else:\n",
    "    try:\n",
    "        prompt = \"Why is the sky blue? Answer in one sentence.\"\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print()\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        result = ollama.generate(\n",
    "            model=DEFAULT_MODEL,\n",
    "            prompt=prompt\n",
    "        )\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        print(f\"Response: {result['response']}\")\n",
    "        print()\n",
    "        print(f\"Latency: {end_time - start_time:.2f}s\")\n",
    "        print(f\"Eval tokens: {result.get('eval_count', 'N/A')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Chat Completion\n",
    "\n",
    "**Function:** `ollama.chat()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chat Completion ===Assistant: Python is a high-level, interpreted programming language that is easy to learn and use. It's widely used for various applications, such as web development, data analysis, machine learning, and automation. Created in the late 1980s by Guido van Rossum, Python is known for its simplicity, flexibility, and large community of developers who contribute to its ecosystem."
     ]
    }
   ],
   "source": [
    "print(\"=== Chat Completion ===\")\n",
    "\n",
    "if not model_available:\n",
    "    print()\n",
    "    print(\"⚠ Skipping - model not available\")\n",
    "    print(f\"  Run: ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "else:\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=DEFAULT_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant. Keep responses brief.\"},\n",
    "                {\"role\": \"user\", \"content\": \"What is Python?\"}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        print(f\"Assistant: {response['message']['content']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Multi-turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multi-turn Conversation ===\n",
      "User: What is 2 + 2?\n",
      "Assistant: The answer to 2 + 2 is 4.User: And what is that multiplied by 3?\n",
      "Assistant: To find the result, we multiply 4 by 3.\n",
      "\n",
      "4 × 3 = 12"
     ]
    }
   ],
   "source": [
    "print(\"=== Multi-turn Conversation ===\")\n",
    "\n",
    "if not model_available:\n",
    "    print()\n",
    "    print(\"⚠ Skipping - model not available\")\n",
    "    print(f\"  Run: ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "else:\n",
    "    try:\n",
    "        # Turn 1\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": \"What is 2 + 2?\"},\n",
    "        ]\n",
    "\n",
    "        response = ollama.chat(\n",
    "            model=DEFAULT_MODEL,\n",
    "            messages=messages\n",
    "        )\n",
    "        print(f\"User: What is 2 + 2?\")\n",
    "        print(f\"Assistant: {response['message']['content']}\")\n",
    "\n",
    "        # Continue conversation\n",
    "        messages.append(response[\"message\"])\n",
    "        messages.append({\"role\": \"user\", \"content\": \"And what is that multiplied by 3?\"})\n",
    "\n",
    "        response = ollama.chat(\n",
    "            model=DEFAULT_MODEL,\n",
    "            messages=messages\n",
    "        )\n",
    "        print(f\"User: And what is that multiplied by 3?\")\n",
    "        print(f\"Assistant: {response['message']['content']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Streaming Response\n",
    "\n",
    "**Function:** `ollama.generate(stream=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Streaming Response ===\n",
      "\n",
      "Response: Here it goes:\n",
      "\n",
      "1, 2, 3, 4, 5!"
     ]
    }
   ],
   "source": [
    "print(\"=== Streaming Response ===\")\n",
    "\n",
    "if not model_available:\n",
    "    print()\n",
    "    print(\"⚠ Skipping - model not available\")\n",
    "    print(f\"  Run: ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "else:\n",
    "    try:\n",
    "        print()\n",
    "\n",
    "        stream = ollama.generate(\n",
    "            model=DEFAULT_MODEL,\n",
    "            prompt=\"Count from 1 to 5.\",\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        collected = []\n",
    "        for chunk in stream:\n",
    "            collected.append(chunk[\"response\"])\n",
    "\n",
    "        print(f\"Response: {''.join(collected)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Embeddings\n",
    "\n",
    "**Function:** `ollama.embed()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generate Embeddings ===\n",
      "Input: 'Ollama makes running LLMs locally easy and efficient.'\n",
      "Embedding dimensions: 3072\n",
      "First 5 values: [-0.026683128, -0.0028091324, -0.027384995, -0.009667068, -0.017405545]\n",
      "Last 5 values: [-0.028065814, 0.010568945, -0.028453464, 0.014874469, -0.029712567]"
     ]
    }
   ],
   "source": [
    "print(\"=== Generate Embeddings ===\")\n",
    "\n",
    "if not model_available:\n",
    "    print()\n",
    "    print(\"⚠ Skipping - model not available\")\n",
    "    print(f\"  Run: ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "else:\n",
    "    try:\n",
    "        test_text = \"Ollama makes running LLMs locally easy and efficient.\"\n",
    "\n",
    "        result = ollama.embed(\n",
    "            model=DEFAULT_MODEL,\n",
    "            input=test_text\n",
    "        )\n",
    "\n",
    "        embeddings = result.get(\"embeddings\", [[]])[0]\n",
    "        print(f\"Input: '{test_text}'\")\n",
    "        print(f\"Embedding dimensions: {len(embeddings)}\")\n",
    "        print(f\"First 5 values: {embeddings[:5]}\")\n",
    "        print(f\"Last 5 values: {embeddings[-5:]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Copy and Delete Model\n",
    "\n",
    "**Functions:** `ollama.copy()`, `ollama.delete()`\n",
    "\n",
    "**Warning:** Delete is permanent! We safely demonstrate by copying first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Copy and Delete Model ===\n",
      "\n",
      "1. Copying 'llama3.2:latest' to 'llama3.2-test-copy:latest'...\n",
      "   Copy successful!\n",
      "\n",
      "2. Verifying 'llama3.2-test-copy:latest' exists...\n",
      "   Found 'llama3.2-test-copy:latest' in model list\n",
      "\n",
      "3. Deleting 'llama3.2-test-copy:latest'...\n",
      "   Delete successful!\n",
      "\n",
      "4. Verifying 'llama3.2-test-copy:latest' is deleted...\n",
      "   'llama3.2-test-copy:latest' successfully removed"
     ]
    }
   ],
   "source": [
    "print(\"=== Copy and Delete Model ===\")\n",
    "\n",
    "if not model_available:\n",
    "    print()\n",
    "    print(\"⚠ Skipping - model not available\")\n",
    "    print(f\"  Run: ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "else:\n",
    "    COPY_NAME = f\"{DEFAULT_MODEL.split(':')[0]}-test-copy:latest\"\n",
    "\n",
    "    # Step 1: Copy the model\n",
    "    print(f\"\\n1. Copying '{DEFAULT_MODEL}' to '{COPY_NAME}'...\")\n",
    "    try:\n",
    "        ollama.copy(source=DEFAULT_MODEL, destination=COPY_NAME)\n",
    "        print(f\"   Copy successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Copy failed: {e}\")\n",
    "\n",
    "    # Step 2: Verify the copy exists\n",
    "    print(f\"\\n2. Verifying '{COPY_NAME}' exists...\")\n",
    "    models = ollama.list()\n",
    "    model_names = [m[\"model\"] for m in models.get(\"models\", [])]\n",
    "    if COPY_NAME in model_names:\n",
    "        print(f\"   Found '{COPY_NAME}' in model list\")\n",
    "    else:\n",
    "        print(f\"   '{COPY_NAME}' not found\")\n",
    "\n",
    "    # Step 3: Delete the copy\n",
    "    print(f\"\\n3. Deleting '{COPY_NAME}'...\")\n",
    "    try:\n",
    "        ollama.delete(COPY_NAME)\n",
    "        print(f\"   Delete successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Delete failed: {e}\")\n",
    "\n",
    "    # Step 4: Verify deletion\n",
    "    print(f\"\\n4. Verifying '{COPY_NAME}' is deleted...\")\n",
    "    models = ollama.list()\n",
    "    model_names = [m[\"model\"] for m in models.get(\"models\", [])]\n",
    "    if COPY_NAME not in model_names:\n",
    "        print(f\"   '{COPY_NAME}' successfully removed\")\n",
    "    else:\n",
    "        print(f\"   '{COPY_NAME}' still exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Error Handling ===\n",
      "\n",
      "1. Testing non-existent model...\n",
      "   Expected error: ResponseError: model 'nonexistent-model-xyz' not found (status code: 404)\n",
      "\n",
      "2. Testing empty prompt...\n",
      "   Empty prompts allowed\n",
      "\n",
      "Error handling tests completed!"
     ]
    }
   ],
   "source": [
    "print(\"=== Error Handling ===\")\n",
    "\n",
    "# Test: Non-existent model\n",
    "print(\"\\n1. Testing non-existent model...\")\n",
    "try:\n",
    "    result = ollama.generate(\n",
    "        model=\"nonexistent-model-xyz\",\n",
    "        prompt=\"Hello\"\n",
    "    )\n",
    "    print(f\"   Unexpected success: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Expected error: {type(e).__name__}: {e}\")\n",
    "\n",
    "# Test: Empty prompt\n",
    "print(\"\\n2. Testing empty prompt...\")\n",
    "try:\n",
    "    result = ollama.generate(\n",
    "        model=DEFAULT_MODEL,\n",
    "        prompt=\"\"\n",
    "    )\n",
    "    print(f\"   Empty prompts allowed\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {type(e).__name__}: {e}\")\n",
    "\n",
    "print(\"\\nError handling tests completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the official `ollama` Python library.\n",
    "\n",
    "### Functions Used\n",
    "\n",
    "| Function | Purpose |\n",
    "|----------|--------|\n",
    "| `ollama.list()` | List available models |\n",
    "| `ollama.show()` | Show model details |\n",
    "| `ollama.ps()` | List running models |\n",
    "| `ollama.generate()` | Generate text |\n",
    "| `ollama.chat()` | Chat completion |\n",
    "| `ollama.embed()` | Generate embeddings |\n",
    "| `ollama.copy()` | Copy a model |\n",
    "| `ollama.delete()` | Delete a model |\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```python\n",
    "import ollama\n",
    "\n",
    "# Generate\n",
    "result = ollama.generate(model=\"llama3.2:latest\", prompt=\"...\")\n",
    "\n",
    "# Chat\n",
    "response = ollama.chat(\n",
    "    model=\"llama3.2:latest\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "```\n",
    "\n",
    "### Why Use the Ollama Library?\n",
    "\n",
    "- **Clean API** - Pythonic interface\n",
    "- **Full features** - Access to all Ollama endpoints\n",
    "- **Type hints** - IDE support and autocompletion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
