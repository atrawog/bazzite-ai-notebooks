{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b69389a2",
   "metadata": {},
   "source": [
    "# Importing HuggingFace Models into Ollama\n",
    "\n",
    "This notebook demonstrates how to import GGUF models from HuggingFace into Ollama.\n",
    "\n",
    "## Model: Nous-Hermes-2-Mistral-7B-DPO\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| Parameters | 7B |\n",
    "| Architecture | Mistral (Llama-compatible) |\n",
    "| Prompt Format | ChatML |\n",
    "| License | Apache 2.0 |\n",
    "| Quantization | Q4_K_M (4.37 GB) |\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Ollama pod running: `ujust ollama start`\n",
    "- Internet connection for downloading from HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72651206",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5255181f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama host: http://ollama:11434\n",
      "HuggingFace model: hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import ollama\n",
    "\n",
    "# === Configuration ===\n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\n",
    "HF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n",
    "\n",
    "print(f\"Ollama host: {OLLAMA_HOST}\")\n",
    "print(f\"HuggingFace model: {HF_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95aab39",
   "metadata": {},
   "source": [
    "## 2. Connection Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbf50248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ollama server is running!\n",
      "  Currently installed: 2 model(s)"
     ]
    }
   ],
   "source": [
    "def check_ollama_health() -> bool:\n",
    "    \"\"\"Check if Ollama server is running.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if server is healthy\n",
    "    \"\"\"\n",
    "    try:\n",
    "        models = ollama.list()\n",
    "        print(\"✓ Ollama server is running!\")\n",
    "        model_names = [m.get(\"model\", \"\") for m in models.get(\"models\", [])]\n",
    "        \n",
    "        if model_names:\n",
    "            print(f\"  Currently installed: {len(model_names)} model(s)\")\n",
    "        else:\n",
    "            print(\"  No models currently installed\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Cannot connect to Ollama server!\")\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"To fix this, run: ujust ollama start\")\n",
    "        return False\n",
    "\n",
    "ollama_healthy = check_ollama_health()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaf92cf",
   "metadata": {},
   "source": [
    "## 3. Pull Model from HuggingFace\n",
    "\n",
    "Ollama can directly pull GGUF models from HuggingFace using the `hf.co/` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed9de53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Pulling hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M ===\n",
      "\n",
      "Downloading from HuggingFace...\n",
      "\n",
      "  pulling manifest  pulling 3f99518c1e2c... (4166 MB)\n",
      "  pulling a47b02e00552... (0.0 MB)\n",
      "  pulling b78301c0df4d... (0.0 MB)\n",
      "  pulling 05644d9257f4... (0.0 MB)\n",
      "  verifying sha256 digest\n",
      "  writing manifest\n",
      "  success\n",
      "\n",
      "✓ Model pulled successfully!"
     ]
    }
   ],
   "source": [
    "print(f\"=== Pulling {HF_MODEL} ===\")\n",
    "print()\n",
    "\n",
    "if not ollama_healthy:\n",
    "    print(\"⚠ Skipping - Ollama server not available\")\n",
    "    print(\"  Run: ujust ollama start\")\n",
    "else:\n",
    "    try:\n",
    "        print(\"Downloading from HuggingFace...\")\n",
    "        print()\n",
    "        \n",
    "        last_status = \"\"\n",
    "        layer_count = 0\n",
    "        total_bytes = 0\n",
    "        \n",
    "        for progress in ollama.pull(HF_MODEL, stream=True):\n",
    "            status = progress.get(\"status\", \"\")\n",
    "            digest = progress.get(\"digest\", \"\")\n",
    "            total = progress.get(\"total\")\n",
    "            \n",
    "            # Only print when status changes (not progress updates)\n",
    "            if status != last_status:\n",
    "                # Count completed layers\n",
    "                if last_status.startswith(\"pulling\") and status != last_status:\n",
    "                    layer_count += 1\n",
    "                    if total:\n",
    "                        total_bytes += total\n",
    "                \n",
    "                # Print status changes (skip repetitive pulling messages)\n",
    "                if status == \"pulling manifest\":\n",
    "                    print(f\"  {status}\")\n",
    "                elif status.startswith(\"pulling\") and digest:\n",
    "                    short_digest = digest.split(\":\")[-1][:12] if \":\" in digest else digest[:12]\n",
    "                    size_mb = (total / 1024 / 1024) if total else 0\n",
    "                    if size_mb > 100:\n",
    "                        print(f\"  pulling {short_digest}... ({size_mb:.0f} MB)\")\n",
    "                    elif size_mb > 0:\n",
    "                        print(f\"  pulling {short_digest}... ({size_mb:.1f} MB)\")\n",
    "                elif status in [\"verifying sha256 digest\", \"writing manifest\", \"success\"]:\n",
    "                    print(f\"  {status}\")\n",
    "                \n",
    "                last_status = status\n",
    "        \n",
    "        print()\n",
    "        print(f\"✓ Model pulled successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error pulling model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcf4acdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Verify Model Installation ===\n",
      "✓ Model is installed\n",
      "  Name: hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M"
     ]
    }
   ],
   "source": [
    "# Verify model is installed\n",
    "print(\"=== Verify Model Installation ===\")\n",
    "\n",
    "models = ollama.list()\n",
    "model_names = [m.get(\"model\", \"\") for m in models.get(\"models\", [])]\n",
    "\n",
    "# Check for the HF model (may have different name format)\n",
    "hf_model_installed = any(\"Nous-Hermes-2-Mistral-7B-DPO\" in name or HF_MODEL in name for name in model_names)\n",
    "\n",
    "if hf_model_installed:\n",
    "    print(f\"✓ Model is installed\")\n",
    "    for name in model_names:\n",
    "        if \"Nous-Hermes\" in name or \"hf.co\" in name:\n",
    "            print(f\"  Name: {name}\")\n",
    "else:\n",
    "    print(\"✗ Model not found in list\")\n",
    "    print(\"Available models:\")\n",
    "    for name in model_names:\n",
    "        print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d7654c",
   "metadata": {},
   "source": [
    "## 4. Test Imported Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5342865d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Show Model Details ===\n",
      "Model: hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\n",
      "\n",
      "Details:\n",
      "  Family: llama\n",
      "  Parameter Size: 7.24B\n",
      "  Quantization: unknown\n",
      "\n",
      "Model file preview:\n",
      "  # Modelfile generated by \"ollama show\"\n",
      "# To build a new Modelfile based on this, replace FROM with:\n",
      "# FROM hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\n",
      "\n",
      "FROM /home/jovian/.ollama/models/blobs/sha256-3f99518c1e2c1b2cee14c3cd7c110358ceb89cf2be0be0626d11ebd8571ff0ff\n",
      "TEMPLATE \"<|im_start|..."
     ]
    }
   ],
   "source": [
    "print(\"=== Show Model Details ===\")\n",
    "\n",
    "try:\n",
    "    model_info = ollama.show(HF_MODEL)\n",
    "    \n",
    "    print(f\"Model: {HF_MODEL}\")\n",
    "    print()\n",
    "    if \"details\" in model_info:\n",
    "        details = model_info[\"details\"]\n",
    "        print(\"Details:\")\n",
    "        print(f\"  Family: {details.get('family', 'N/A')}\")\n",
    "        print(f\"  Parameter Size: {details.get('parameter_size', 'N/A')}\")\n",
    "        print(f\"  Quantization: {details.get('quantization_level', 'N/A')}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Model file preview:\")\n",
    "    modelfile = model_info.get(\"modelfile\", \"N/A\")\n",
    "    print(f\"  {modelfile[:300]}...\" if len(modelfile) > 300 else f\"  {modelfile}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc85653c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generate Response ===\n",
      "Prompt: What is the capital of France? Answer in one sentence.\n",
      "Response: The capital of France is Paris.\n",
      "\n",
      "Latency: 3.48s\n",
      "Eval tokens: 8\n",
      "Tokens/second: 146.8"
     ]
    }
   ],
   "source": [
    "print(\"=== Generate Response ===\")\n",
    "\n",
    "try:\n",
    "    prompt = \"What is the capital of France? Answer in one sentence.\"\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print()\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    result = ollama.generate(\n",
    "        model=HF_MODEL,\n",
    "        prompt=prompt\n",
    "    )\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    print(f\"Response: {result['response']}\")\n",
    "    print()\n",
    "    print(f\"Latency: {end_time - start_time:.2f}s\")\n",
    "    print(f\"Eval tokens: {result.get('eval_count', 'N/A')}\")\n",
    "    \n",
    "    if result.get('eval_count') and result.get('eval_duration'):\n",
    "        tokens_per_sec = result['eval_count'] / (result['eval_duration'] / 1e9)\n",
    "        print(f\"Tokens/second: {tokens_per_sec:.1f}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f06d9a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chat Completion (ChatML) ===System: You are Hermes 2, a helpful AI assistant.\n",
      "User: Explain quantum computing in two sentences.\n",
      "\n",
      "Assistant: Quantum computing is an advanced form of computing that operates on the principles of quantum mechanics, specifically utilizing qubits to perform calculations and process information simultaneously at multiple states. It has the potential to solve complex problems much faster than classical computers."
     ]
    }
   ],
   "source": [
    "print(\"=== Chat Completion (ChatML) ===\")\n",
    "\n",
    "try:\n",
    "    # Nous-Hermes-2 uses ChatML format natively\n",
    "    response = ollama.chat(\n",
    "        model=HF_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are Hermes 2, a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Explain quantum computing in two sentences.\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(f\"System: You are Hermes 2, a helpful AI assistant.\")\n",
    "    print(f\"User: Explain quantum computing in two sentences.\")\n",
    "    print()\n",
    "    print(f\"Hermes: {response['message']['content']}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c897f42b",
   "metadata": {},
   "source": [
    "## 5. Cleanup\n",
    "\n",
    "Delete the model to free disk space (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba048bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Delete Model ===\n",
      "⚠ Deletion is commented out to preserve the model.\n",
      "  To delete, uncomment the code above or run:\n",
      "  ollama.delete('hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M')"
     ]
    }
   ],
   "source": [
    "print(\"=== Delete Model ===\")\n",
    "\n",
    "# Uncomment the lines below to delete the model\n",
    "# print(f\"Deleting '{HF_MODEL}'...\")\n",
    "# try:\n",
    "#     ollama.delete(HF_MODEL)\n",
    "#     print(\"✓ Model deleted successfully!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"✗ Error: {e}\")\n",
    "\n",
    "print(\"⚠ Deletion is commented out to preserve the model.\")\n",
    "print(f\"  To delete, uncomment the code above or run:\")\n",
    "print(f\"  ollama.delete('{HF_MODEL}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dd6e23",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated importing a HuggingFace model into Ollama.\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```python\n",
    "import ollama\n",
    "\n",
    "# Pull any GGUF model from HuggingFace\n",
    "HF_MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n",
    "\n",
    "# Download with progress\n",
    "for progress in ollama.pull(HF_MODEL, stream=True):\n",
    "    print(progress.get(\"status\"))\n",
    "\n",
    "# Use like any other Ollama model\n",
    "response = ollama.chat(\n",
    "    model=HF_MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "```\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Ollama Import Docs](https://docs.ollama.com/import)\n",
    "- [HuggingFace Ollama Integration](https://huggingface.co/docs/hub/ollama)\n",
    "- [Model Repository](https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
