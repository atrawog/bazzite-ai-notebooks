{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama GPU Testing Notebook\n",
    "\n",
    "This notebook verifies GPU availability and tests GPU-accelerated inference with Ollama.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Ollama pod must be running: `ujust ollama start`\n",
    "- NVIDIA GPU with drivers installed\n",
    "- At least one model pulled: `ujust ollama pull llama3.2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama host: http://ollama:11434\nDefault model: llama3.2:latest"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import requests\n",
    "\n",
    "# === Configuration ===\n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\n",
    "DEFAULT_MODEL = \"llama3.2:latest\"\n",
    "\n",
    "print(f\"Ollama host: {OLLAMA_HOST}\")\n",
    "print(f\"Default model: {DEFAULT_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connection Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ollama server is running!\n✓ Model 'llama3.2:latest' is available"
     ]
    }
   ],
   "source": [
    "def check_ollama_health() -> tuple[bool, bool]:\n",
    "    \"\"\"Check if Ollama server is running and model is available.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (server_healthy, model_available)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"✓ Ollama server is running!\")\n",
    "            models = response.json()\n",
    "            model_names = [m.get(\"name\", \"\") for m in models.get(\"models\", [])]\n",
    "            \n",
    "            if DEFAULT_MODEL in model_names:\n",
    "                print(f\"✓ Model '{DEFAULT_MODEL}' is available\")\n",
    "                return True, True\n",
    "            else:\n",
    "                print(f\"✗ Model '{DEFAULT_MODEL}' not found!\")\n",
    "                print()\n",
    "                if model_names:\n",
    "                    print(\"Available models:\")\n",
    "                    for name in model_names:\n",
    "                        print(f\"  - {name}\")\n",
    "                else:\n",
    "                    print(\"No models installed.\")\n",
    "                print()\n",
    "                print(\"To fix this, run:\")\n",
    "                print(f\"  ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "                return True, False\n",
    "        else:\n",
    "            print(f\"Ollama returned unexpected status: {response.status_code}\")\n",
    "            return False, False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"✗ Cannot connect to Ollama server!\")\n",
    "        print()\n",
    "        print(\"To fix this, run:\")\n",
    "        print(\"  ujust ollama start\")\n",
    "        return False, False\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"✗ Connection to Ollama timed out!\")\n",
    "        return False, False\n",
    "\n",
    "ollama_healthy, model_available = check_ollama_health()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPU Status Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPU Status ===\nGPU 0: NVIDIA GeForce RTX 4080 SUPER\n  Memory: 6228 MB / 16376 MB\n  Utilization: 13%"
     ]
    }
   ],
   "source": [
    "print(\"=== GPU Status ===\")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"nvidia-smi\", \"--query-gpu=name,memory.used,memory.total,utilization.gpu\", \"--format=csv,noheader,nounits\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=5\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        lines = result.stdout.strip().split(\"\\n\")\n",
    "        for i, line in enumerate(lines):\n",
    "            parts = line.split(\", \")\n",
    "            if len(parts) >= 4:\n",
    "                name, mem_used, mem_total, util = parts\n",
    "                print(f\"GPU {i}: {name}\")\n",
    "                print(f\"  Memory: {mem_used} MB / {mem_total} MB\")\n",
    "                print(f\"  Utilization: {util}%\")\n",
    "    else:\n",
    "        print(\"nvidia-smi returned an error\")\n",
    "        print(result.stderr)\n",
    "except FileNotFoundError:\n",
    "    print(\"nvidia-smi not found - NVIDIA GPU may not be available\")\n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"nvidia-smi timed out\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking GPU: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GPU Usage During Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPU Usage During Inference ===\nRunning inference and checking GPU metrics...\n\nResponse: Microchip whispers\nGlowing screens in silent night\nDigital dreams born\n\nOllama Inference Metrics:\n  Prompt eval count: 32\n  Prompt eval duration: 0.014s\n  Eval count (tokens generated): 16\n  Eval duration: 0.064s\n  Total duration: 0.146s\n  Tokens/second: 250.0"
     ]
    }
   ],
   "source": [
    "print(\"=== GPU Usage During Inference ===\")\n",
    "\n",
    "if not model_available:\n",
    "    print()\n",
    "    print(\"⚠ Skipping inference test - model not available\")\n",
    "    print(f\"  Run: ujust ollama pull {DEFAULT_MODEL.split(':')[0]}\")\n",
    "else:\n",
    "    print(\"Running inference and checking GPU metrics...\")\n",
    "    print()\n",
    "\n",
    "    # Run a generation to load the model\n",
    "    response = requests.post(\n",
    "        f\"{OLLAMA_HOST}/api/generate\",\n",
    "        json={\n",
    "            \"model\": DEFAULT_MODEL,\n",
    "            \"prompt\": \"Write a haiku about computers.\",\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "    result = response.json()\n",
    "\n",
    "    if \"error\" in result:\n",
    "        print(f\"✗ Error: {result['error']}\")\n",
    "    else:\n",
    "        print(f\"Response: {result['response']}\")\n",
    "        print()\n",
    "\n",
    "        # Check Ollama's reported metrics\n",
    "        print(\"Ollama Inference Metrics:\")\n",
    "        print(f\"  Prompt eval count: {result.get('prompt_eval_count', 'N/A')}\")\n",
    "        print(f\"  Prompt eval duration: {result.get('prompt_eval_duration', 0) / 1e9:.3f}s\")\n",
    "        print(f\"  Eval count (tokens generated): {result.get('eval_count', 'N/A')}\")\n",
    "        print(f\"  Eval duration: {result.get('eval_duration', 0) / 1e9:.3f}s\")\n",
    "        print(f\"  Total duration: {result.get('total_duration', 0) / 1e9:.3f}s\")\n",
    "\n",
    "        if result.get('eval_count') and result.get('eval_duration'):\n",
    "            tokens_per_sec = result['eval_count'] / (result['eval_duration'] / 1e9)\n",
    "            print(f\"  Tokens/second: {tokens_per_sec:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. List Running Models (GPU Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Models Loaded in GPU Memory ===\n  - llama3.2:latest\n    Total Size: 2.56 GB\n    VRAM Usage: 2.56 GB\n    Expires: 2025-12-26T22:50:20.453049987Z"
     ]
    }
   ],
   "source": [
    "print(\"=== Models Loaded in GPU Memory ===\")\n",
    "\n",
    "response = requests.get(f\"{OLLAMA_HOST}/api/ps\")\n",
    "running = response.json()\n",
    "\n",
    "if running.get(\"models\"):\n",
    "    for model in running[\"models\"]:\n",
    "        name = model.get(\"name\", \"Unknown\")\n",
    "        size = model.get(\"size\", 0) / (1024**3)\n",
    "        vram = model.get(\"size_vram\", 0) / (1024**3)\n",
    "        expires = model.get(\"expires_at\", \"N/A\")\n",
    "        print(f\"  - {name}\")\n",
    "        print(f\"    Total Size: {size:.2f} GB\")\n",
    "        print(f\"    VRAM Usage: {vram:.2f} GB\")\n",
    "        print(f\"    Expires: {expires}\")\n",
    "else:\n",
    "    print(\"  No models currently loaded in memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook verified:\n",
    "\n",
    "1. **GPU Detection** - nvidia-smi shows available GPUs and memory\n",
    "2. **Ollama Connection** - Server is accessible and responding\n",
    "3. **GPU Inference** - Model runs on GPU with measurable performance\n",
    "4. **VRAM Usage** - Models loaded in GPU memory\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "- **Tokens/second** - Higher is better (GPU acceleration)\n",
    "- **VRAM Usage** - Should match model size\n",
    "- **GPU Utilization** - Shows GPU activity during inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
