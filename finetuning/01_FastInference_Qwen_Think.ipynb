{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54084e44",
   "metadata": {},
   "source": [
    "# Fast Inference Test: Qwen3-4B-Thinking-2507\n",
    "\n",
    "Tests `fast_inference=True` with vLLM backend on Qwen3-4B-Thinking-2507.\n",
    "\n",
    "**Key features tested:**\n",
    "- FastLanguageModel loading with fast_inference mode\n",
    "- Thinking model output with `<think>...</think>` tags\n",
    "- Parsing and displaying thinking vs response separately\n",
    "\n",
    "**Important:** This notebook includes a kernel shutdown cell at the end.\n",
    "vLLM does not release GPU memory in single-process mode (Jupyter), so kernel\n",
    "restart is required between different model tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49c6c9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/pixi/.pixi/envs/default/lib/python3.13/site-packages/trl/__init__.py:203: UserWarning: TRL currently supports vLLM versions: 0.10.2, 0.11.0, 0.11.1, 0.11.2. You have version 0.14.0rc1.dev201+gadcf682fc.cu130 installed. We recommend installing a supported version to avoid compatibility issues.\n",
      "  if is_vllm_available():"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!Environment: unsloth 2025.12.10, vLLM 0.14.0rc1.dev201+gadcf682fc, NVIDIA GeForce RTX 4080 SUPER"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "import vllm\n",
    "import torch\n",
    "\n",
    "# Environment summary\n",
    "gpu = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"Environment: unsloth {unsloth.__version__}, vLLM {vllm.__version__}, {gpu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a123208a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Qwen3-4B-Thinking-2507-unsloth-bnb-4bit with fast_inference=True...WARNING 01-03 11:43:50 [vllm.py:1427] Current vLLM config is not set.INFO 01-03 11:43:50 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=2048.INFO 01-03 11:43:50 [vllm.py:609] Disabling NCCL for DP synchronization when using async scheduling.INFO 01-03 11:43:50 [vllm.py:614] Asynchronous scheduling is enabled.INFO 01-03 11:43:51 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture==((====))==  Unsloth 2025.12.10: Fast Qwen3 patching. Transformers: 5.0.0.1. vLLM: 0.14.0rc1.dev201+gadcf682fc.cu130.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.568 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu130. CUDA: 8.9. CUDA Toolkit: 13.0. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!Unsloth: vLLM loading unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit with actual GPU utilization = 44.45%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 15.57 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 32.\n",
      "Unsloth: vLLM's KV Cache can use up to 4.06 GB. Also swap space = 6 GB.\n",
      "Unsloth: Not an error, but `use_cudagraph` is not supported in vLLM.config.CompilationConfig. Skipping.\n",
      "Unsloth: Not an error, but `use_inductor` is not supported in vLLM.config.CompilationConfig. Skipping.\n",
      "WARNING 01-03 11:43:56 [compilation.py:739] Level is deprecated and will be removed in the next release,either 0.12.0 or 0.11.2 whichever is soonest.Use mode instead.If both level and mode are given,only mode will be used.Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "WARNING 01-03 11:43:56 [attention.py:82] Using VLLM_ATTENTION_BACKEND environment variable is deprecated and will be removed in v0.14.0 or v1.0.0, whichever is soonest. Please use --attention-config.backend command line argument or AttentionConfig(backend=...) config field instead.INFO 01-03 11:43:56 [utils.py:253] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'max_model_len': 1024, 'enable_prefix_caching': True, 'swap_space': 6, 'gpu_memory_utilization': 0.4445226398597971, 'max_num_batched_tokens': 4096, 'max_num_seqs': 32, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {'level': 3, 'mode': 3, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'epilogue_fusion': True, 'max_autotune': False, 'shape_padding': True, 'trace.enabled': False, 'triton.cudagraphs': False, 'debug': False, 'dce': True, 'memory_planning': True, 'coordinate_descent_tuning': False, 'trace.graph_diagram': False, 'compile_threads': 16, 'group_fusion': True, 'disable_progress': False, 'verbose_progress': True, 'triton.multi_kernel': 0, 'triton.use_block_ptr': True, 'triton.enable_persistent_tma_matmul': True, 'triton.autotune_at_compile_time': False, 'triton.cooperative_reductions': False, 'cuda.compile_opt_level': '-O2', 'cuda.enable_cuda_lto': True, 'combo_kernels': False, 'benchmark_combo_kernel': True, 'combo_kernel_foreach_dynamic_shapes': True, 'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}, 'model': 'unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit'}WARNING 01-03 11:43:56 [arg_utils.py:1196] The global random seed is set to 0. Since VLLM_ENABLE_V1_MULTIPROCESSING is set to False, this may affect the random state of the Python process that launched vLLM."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/pixi/.pixi/envs/default/lib/python3.13/site-packages/pydantic/type_adapter.py:605: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `enum` - serialized value may not be as expected [field_name='mode', input_value=3, input_type=int])\n",
      "  return self.serializer.to_python("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-03 11:43:57 [model.py:517] Resolved architecture: Qwen3ForCausalLMINFO 01-03 11:43:57 [model.py:1688] Using max model len 1024WARNING 01-03 11:43:57 [vllm.py:1427] Current vLLM config is not set.INFO 01-03 11:43:57 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=2048.INFO 01-03 11:43:57 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=4096.Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['embed_tokens', 'embedding', 'lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.27.mlp', 'model.layers.34.mlp', 'model.layers.6.self_attn', 'model.layers.0.mlp', 'model.layers.5.mlp', 'model.layers.4.mlp', 'model.layers.5.self_attn', 'model.layers.1.mlp', 'model.layers.6.mlp'], 'llm_int8_threshold': 6.0}INFO 01-03 11:43:59 [core.py:95] Initializing a V1 LLM engine (v0.14.0rc1.dev201+gadcf682fc) with config: model='unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False), seed=0, served_model_name=unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': 3, 'mode': 3, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'epilogue_fusion': True, 'max_autotune': False, 'shape_padding': True, 'trace.enabled': False, 'triton.cudagraphs': False, 'debug': False, 'dce': True, 'memory_planning': True, 'coordinate_descent_tuning': False, 'trace.graph_diagram': False, 'compile_threads': 16, 'group_fusion': True, 'disable_progress': False, 'verbose_progress': True, 'triton.multi_kernel': 0, 'triton.use_block_ptr': True, 'triton.enable_persistent_tma_matmul': True, 'triton.autotune_at_compile_time': False, 'triton.cooperative_reductions': False, 'cuda.compile_opt_level': '-O2', 'cuda.enable_cuda_lto': True, 'combo_kernels': False, 'benchmark_combo_kernel': True, 'combo_kernel_foreach_dynamic_shapes': True, 'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 64, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}INFO 01-03 11:43:59 [parallel_state.py:1210] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.89.0.21:57215 backend=nccl[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0INFO 01-03 11:43:59 [parallel_state.py:1418] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/pixi/.pixi/envs/default/lib/python3.13/site-packages/pydantic/type_adapter.py:605: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `enum` - serialized value may not be as expected [field_name='mode', input_value=3, input_type=int])\n",
      "  return self.serializer.to_python("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-03 11:43:59 [topk_topp_sampler.py:47] Using FlashInfer for top-p & top-k sampling.INFO 01-03 11:43:59 [gpu_model_runner.py:3762] Starting to load model unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit...INFO 01-03 11:44:00 [cuda.py:315] Using AttentionBackendEnum.FLASHINFER backend.INFO 01-03 11:44:00 [bitsandbytes_loader.py:790] Loading weights with BitsAndBytes quantization. May take a while ...INFO 01-03 11:44:01 [weight_utils.py:550] No model.safetensors.index.json found in remote."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c831e6943a49b5bbad7f304971e6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901dfff4e88f4ad089318f96408be703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-03 11:44:01 [punica_selector.py:20] Using PunicaWrapperGPU.INFO 01-03 11:44:02 [gpu_model_runner.py:3859] Model loading took 3.5851 GiB memory and 1.476188 secondsINFO 01-03 11:44:08 [backends.py:644] Using cache directory: /workspace/.cache/vllm/torch_compile_cache/fcdf551ae6/rank_0_0/backbone for vLLM's torch.compileINFO 01-03 11:44:08 [backends.py:704] Dynamo bytecode transform time: 5.36 sINFO 01-03 11:44:11 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 4096) from the cache, took 0.489 sINFO 01-03 11:44:11 [monitor.py:34] torch.compile takes 5.85 s in totalINFO 01-03 11:44:12 [gpu_worker.py:363] Available KV cache memory: 2.90 GiBINFO 01-03 11:44:12 [kv_cache_utils.py:1305] GPU KV cache size: 21,072 tokensINFO 01-03 11:44:12 [kv_cache_utils.py:1310] Maximum concurrency for 1,024 tokens per request: 20.58xINFO 01-03 11:44:12 [kernel_warmup.py:64] Warming up FlashInfer attention.INFO 01-03 11:44:12 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 01-03 11:44:12 [utils.py:256] Using default LoRA kernel configs"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|‚ñç         | 1/22 [00:00<00:02,  7.58it/s]\r",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|‚ñà‚ñé        | 3/22 [00:00<00:01, 12.30it/s]\r",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|‚ñà‚ñà‚ñé       | 5/22 [00:00<00:01, 14.22it/s]\r",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|‚ñà‚ñà‚ñà‚ñè      | 7/22 [00:00<00:00, 15.48it/s]\r",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|‚ñà‚ñà‚ñà‚ñà      | 9/22 [00:00<00:00, 16.02it/s]\r",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 11/22 [00:00<00:00, 16.45it/s]\r",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 13/22 [00:00<00:00, 16.88it/s]\r",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 15/22 [00:00<00:00, 17.20it/s]\r",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 17/22 [00:01<00:00, 16.94it/s]\r",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 19/22 [00:01<00:00, 17.14it/s]\r",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 21/22 [00:01<00:00, 17.14it/s]\r",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:01<00:00, 16.43it/s]\r",
      "Capturing CUDA graphs (decode, FULL):   0%|          | 0/14 [00:00<?, ?it/s]\r",
      "Capturing CUDA graphs (decode, FULL):  14%|‚ñà‚ñç        | 2/14 [00:00<00:00, 14.91it/s]\r",
      "Capturing CUDA graphs (decode, FULL):  29%|‚ñà‚ñà‚ñä       | 4/14 [00:00<00:00, 15.59it/s]\r",
      "Capturing CUDA graphs (decode, FULL):  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 6/14 [00:00<00:00, 15.94it/s]\r",
      "Capturing CUDA graphs (decode, FULL):  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 8/14 [00:00<00:00, 16.36it/s]\r",
      "Capturing CUDA graphs (decode, FULL):  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10/14 [00:00<00:00, 16.87it/s]\r",
      "Capturing CUDA graphs (decode, FULL):  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 12/14 [00:00<00:00, 17.06it/s]\r",
      "Capturing CUDA graphs (decode, FULL): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 17.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-03 11:44:15 [gpu_model_runner.py:4810] Graph capturing finished in 2 secs, took 0.61 GiBINFO 01-03 11:44:15 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 2 secs."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-03 11:44:16 [core.py:272] init engine (profile, create kv cache, warmup model) took 13.55 secondsINFO 01-03 11:44:16 [core.py:184] Batch queue is enabled with size 2INFO 01-03 11:44:16 [llm.py:344] Supported tasks: ('generate',)Unsloth: Just some info: will skip parsing ['layer_norm2', 'layer_norm1', 'norm', 'attention_norm', 'post_feedforward_layernorm', 'norm1', 'post_attention_layernorm', 'ffn_norm', 'k_norm', 'input_layernorm', 'post_layernorm', 'q_norm', 'pre_feedforward_layernorm', 'norm2']"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9005aef5de48b48f5ad96f5be20010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing substitution for additional_keys=set()\n",
      "Unsloth: Just some info: will skip parsing ['cross_attn_post_attention_layernorm', 'layer_norm2', 'layer_norm1', 'norm', 'attention_norm', 'post_feedforward_layernorm', 'norm1', 'post_attention_layernorm', 'ffn_norm', 'k_norm', 'input_layernorm', 'post_layernorm', 'cross_attn_input_layernorm', 'q_norm', 'pre_feedforward_layernorm', 'norm2']Model loaded: Qwen3ForCausalLM"
     ]
    }
   ],
   "source": [
    "# Test Qwen3-4B-Thinking-2507 with fast_inference=True\n",
    "MODEL_NAME = \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\"\n",
    "print(f\"\\nLoading {MODEL_NAME.split('/')[-1]} with fast_inference=True...\")\n",
    "\n",
    "from vllm import SamplingParams\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    max_seq_length=1024,  # Increased for thinking content\n",
    "    load_in_4bit=True,\n",
    "    fast_inference=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "916ade5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e84fe864cf44d99d19586898cc58eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62c5cc7cf5c46d8b2374112b953f8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 8.95s\n",
      "Output tokens: 987\n",
      "\n",
      "============================================================\n",
      "RAW OUTPUT:\n",
      "============================================================\n",
      "Okay, the user is asking \"What is 15 + 27? Show your thinking.\" Hmm, this seems like a very basic arithmetic question, but they specifically want to see the thinking process. \n",
      "\n",
      "First, I should consider who might ask this. Could be a child learning addition, a non-native English speaker, or someone double-checking their math. Since they want \"show your thinking,\" they're probably not just after the answer but want to understand the steps. \n",
      "\n",
      "I'll break it down step by step. The most straightforward way is to do it mentally: 15 + 20 is 35, then +7 more is 42. But I should also show the column method since that's how it's taught in schools. \n",
      "\n",
      "Wait, should I mention other methods? Like counting on? But for 15+27, that might be overkill. The user didn't specify age, so I'll keep it simple. \n",
      "\n",
      "*imagines the user* If this is a kid, I should avoid jargon. If it's an adult, I can be slightly more technical. But since they said \"show your thinking,\" I'll go for clear, step-by-step. \n",
      "\n",
      "I'll do: \n",
      "- Start with 15 + 20 = 35 (since 27 is 20+7) \n",
      "- Then 35 + 7 = 42 \n",
      "- And the column method for thoroughness \n",
      "\n",
      "*checks mental math* 15+27... 10+20=30, 5+7=12, 30+12=42. Yep, same answer. \n",
      "\n",
      "Should I add that 42 is the final answer? The user said \"show your thinking\" so the conclusion is important. \n",
      "\n",
      "*thinks about errors* Oh! I should make sure not to say \"15+20=35\" is wrong - that's correct. Some people might do 15+25=40 then +2=42, but that's more steps. \n",
      "\n",
      "I'll go with the two methods: \n",
      "1. Breaking into tens and ones \n",
      "2. Column addition \n",
      "\n",
      "The user didn't specify method, so both are good. I'll write it as: \n",
      "\n",
      "\"First, I can add 15 and 20 to get 35, then add the remaining 7 to get 42. \n",
      "Alternatively, using column addition: \n",
      "  15\n",
      "+ 27\n",
      "------\n",
      "  42\"\n",
      "\n",
      "*double-checks* 15+27: 5+7=12, write 2 carry 1; 1+2+1=4. So 42. Perfect. \n",
      "\n",
      "I think that's clear. The user gets the answer and the reasoning. No need to overcomplicate it since it's a simple addition. \n",
      "\n",
      "*considers tone* I'll keep it friendly and encouraging - \"Here's how I think about it\" so it feels like a conversation, not a textbook. \n",
      "\n",
      "Final answer: 42, with the two methods shown. Done.\n",
      "\n",
      "\n",
      "Here's my step-by-step thinking for **15 + 27**:\n",
      "\n",
      "1. **I break it down into parts** to make it easier:  \n",
      "   - I know 15 is 10 + 5, and 27 is 20 + 7.  \n",
      "   - So, I can add the \"tens\" first: **10 + 20 = 30**.  \n",
      "   - Then add the \"ones\": **5 + 7 = 12**.  \n",
      "   - Now, combine the results: **30 + 12 = 42**.  \n",
      "\n",
      "2. **I also try column addition** (a common method for addition):  \n",
      "   ```\n",
      "     15\n",
      "   + 27\n",
      "   ----\n",
      "   ```\n",
      "   - Add the **ones place first**: 5 + 7 = 12. Write down the **2** and carry over the **1** to the tens place.  \n",
      "   - Add the **tens place**: 1 (from 15) + 2 (from 27) + 1 (carried over) = 4.  \n",
      "   - So, the result is **42**.\n",
      "\n",
      "**Why this works**:  \n",
      "- Both methods confirm the same answer.  \n",
      "- Breaking it into tens and ones (10+20 and 5+7) is intuitive and helps avoid mistakes.  \n",
      "- Column addition is systematic and works for any two numbers.\n",
      "\n",
      "**Final answer**:  \n",
      "**15 + 27 = 42** üåü"
     ]
    }
   ],
   "source": [
    "# Test generation with thinking model\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Use a prompt that encourages reasoning\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is 15 + 27? Show your thinking.\"}]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.6,  # Recommended for thinking models\n",
    "    top_p=0.95,\n",
    "    top_k=20,\n",
    "    max_tokens=2048,  # Increased to allow full thinking + response\n",
    ")\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "outputs = model.fast_generate([prompt], sampling_params=sampling_params)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "# Get raw output text and token IDs for parsing\n",
    "raw_output = outputs[0].outputs[0].text\n",
    "output_token_ids = outputs[0].outputs[0].token_ids\n",
    "\n",
    "print(f\"Generation time: {elapsed:.2f}s\")\n",
    "print(f\"Output tokens: {len(output_token_ids)}\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RAW OUTPUT:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dc74ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "THINKING CONTENT:\n",
      "============================================================\n",
      "Okay, the user is asking \"What is 15 + 27? Show your thinking.\" Hmm, this seems like a very basic arithmetic question, but they specifically want to see the thinking process. \n",
      "\n",
      "First, I should consider who might ask this. Could be a child learning addition, a non-native English speaker, or someone double-checking their math. Since they want \"show your thinking,\" they're probably not just after the answer but want to understand the steps. \n",
      "\n",
      "I'll break it down step by step. The most straightforward way is to do it mentally: 15 + 20 is 35, then +7 more is 42. But I should also show the column method since that's how it's taught in schools. \n",
      "\n",
      "Wait, should I mention other methods? Like counting on? But for 15+27, that might be overkill. The user didn't specify age, so I'll keep it simple. \n",
      "\n",
      "*imagines the user* If this is a kid, I should avoid jargon. If it's an adult, I can be slightly more technical. But since they said \"show your thinking,\" I'll go for clear, step-by-step. \n",
      "\n",
      "I'll do: \n",
      "- Start with 15 + 20 = 35 (since 27 is 20+7) \n",
      "- Then 35 + 7 = 42 \n",
      "- And the column method for thoroughness \n",
      "\n",
      "*checks mental math* 15+27... 10+20=30, 5+7=12, 30+12=42. Yep, same answer. \n",
      "\n",
      "Should I add that 42 is the final answer? The user said \"show your thinking\" so the conclusion is important. \n",
      "\n",
      "*thinks about errors* Oh! I should make sure not to say \"15+20=35\" is wrong - that's correct. Some people might do 15+25=40 then +2=42, but that's more steps. \n",
      "\n",
      "I'll go with the two methods: \n",
      "1. Breaking into tens and ones \n",
      "2. Column addition \n",
      "\n",
      "The user didn't specify method, so both are good. I'll write it as: \n",
      "\n",
      "\"First, I can add 15 and 20 to get 35, then add the remaining 7 to get 42. \n",
      "Alternatively, using column addition: \n",
      "  15\n",
      "+ 27\n",
      "------\n",
      "  42\"\n",
      "\n",
      "*double-checks* 15+27: 5+7=12, write 2 carry 1; 1+2+1=4. So 42. Perfect. \n",
      "\n",
      "I think that's clear. The user gets the answer and the reasoning. No need to overcomplicate it since it's a simple addition. \n",
      "\n",
      "*considers tone* I'll keep it friendly and encouraging - \"Here's how I think about it\" so it feels like a conversation, not a textbook. \n",
      "\n",
      "Final answer: 42, with the two methods shown. Done.\n",
      "\n",
      "============================================================\n",
      "FINAL RESPONSE:\n",
      "============================================================\n",
      "Here's my step-by-step thinking for **15 + 27**:\n",
      "\n",
      "1. **I break it down into parts** to make it easier:  \n",
      "   - I know 15 is 10 + 5, and 27 is 20 + 7.  \n",
      "   - So, I can add the \"tens\" first: **10 + 20 = 30**.  \n",
      "   - Then add the \"ones\": **5 + 7 = 12**.  \n",
      "   - Now, combine the results: **30 + 12 = 42**.  \n",
      "\n",
      "2. **I also try column addition** (a common method for addition):  \n",
      "   ```\n",
      "     15\n",
      "   + 27\n",
      "   ----\n",
      "   ```\n",
      "   - Add the **ones place first**: 5 + 7 = 12. Write down the **2** and carry over the **1** to the tens place.  \n",
      "   - Add the **tens place**: 1 (from 15) + 2 (from 27) + 1 (carried over) = 4.  \n",
      "   - So, the result is **42**.\n",
      "\n",
      "**Why this works**:  \n",
      "- Both methods confirm the same answer.  \n",
      "- Breaking it into tens and ones (10+20 and 5+7) is intuitive and helps avoid mistakes.  \n",
      "- Column addition is systematic and works for any two numbers.\n",
      "\n",
      "**Final answer**:  \n",
      "**15 + 27 = 42** üåü"
     ]
    }
   ],
   "source": [
    "# Parse thinking content vs final response using token IDs\n",
    "THINK_END_TOKEN_ID = 151668  # </think> token ID for Qwen3-Thinking models\n",
    "\n",
    "def parse_thinking_response_tokens(token_ids, tokenizer):\n",
    "    \"\"\"\n",
    "    Parse thinking model output using token ID 151668 (</think>) as boundary.\n",
    "    \n",
    "    With Thinking-2507 models + add_generation_prompt=True:\n",
    "    - Template adds <think> to prompt\n",
    "    - Model output starts with thinking content (no <think> prefix)\n",
    "    - Model outputs </think> (token 151668) when done, followed by response\n",
    "    \"\"\"\n",
    "    token_list = list(token_ids)\n",
    "    \n",
    "    if THINK_END_TOKEN_ID in token_list:\n",
    "        end_idx = token_list.index(THINK_END_TOKEN_ID)\n",
    "        thinking_tokens = token_list[:end_idx]\n",
    "        response_tokens = token_list[end_idx + 1:]\n",
    "        thinking = tokenizer.decode(thinking_tokens, skip_special_tokens=True).strip()\n",
    "        response = tokenizer.decode(response_tokens, skip_special_tokens=True).strip()\n",
    "    else:\n",
    "        # No </think> found - model may still be thinking (need more tokens)\n",
    "        thinking = tokenizer.decode(token_list, skip_special_tokens=True).strip()\n",
    "        response = \"(Model did not complete thinking - increase max_tokens)\"\n",
    "    \n",
    "    return thinking, response\n",
    "\n",
    "# Parse the output using token IDs\n",
    "thinking_content, response_content = parse_thinking_response_tokens(output_token_ids, tokenizer)\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"THINKING CONTENT:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(thinking_content if thinking_content else \"(No thinking content found)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL RESPONSE:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(response_content if response_content else \"(No response found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cc2d3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VERIFICATION SUMMARY\n",
      "============================================================\n",
      "Model: unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\n",
      "FastInference: ‚úÖ SUPPORTED\n",
      "</think> token found: ‚úÖ YES\n",
      "Thinking content: ‚úÖ YES\n",
      "Response generated: ‚úÖ YES\n",
      "Output tokens: 987\n",
      "Generation time: 8.95s\n",
      "============================================================\n",
      "\n",
      "‚úÖ Qwen3-4B-Thinking-2507 Fast Inference Test PASSED"
     ]
    }
   ],
   "source": [
    "# Verification summary\n",
    "has_thinking = bool(thinking_content) and \"(Model did not complete\" not in response_content\n",
    "has_response = bool(response_content) and \"(Model did not complete\" not in response_content\n",
    "think_tag_found = THINK_END_TOKEN_ID in list(output_token_ids)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"VERIFICATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"FastInference: ‚úÖ SUPPORTED\")\n",
    "print(f\"</think> token found: {'‚úÖ YES' if think_tag_found else '‚ùå NO (increase max_tokens)'}\")\n",
    "print(f\"Thinking content: {'‚úÖ YES' if has_thinking else '‚ùå NO'}\")\n",
    "print(f\"Response generated: {'‚úÖ YES' if has_response else '‚ùå NO'}\")\n",
    "print(f\"Output tokens: {len(output_token_ids)}\")\n",
    "print(f\"Generation time: {elapsed:.2f}s\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if has_thinking and has_response and think_tag_found:\n",
    "    print(\"\\n‚úÖ Qwen3-4B-Thinking-2507 Fast Inference Test PASSED\")\n",
    "elif not think_tag_found:\n",
    "    print(\"\\n‚ùå Model did not complete thinking - increase max_tokens\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Test completed but thinking output may need review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9913161",
   "metadata": {},
   "source": [
    "## Test Complete\n",
    "\n",
    "The Qwen3-4B-Thinking-2507 fast_inference test has completed. The kernel will now shut down to release all GPU memory.\n",
    "\n",
    "### What Was Verified\n",
    "- FastLanguageModel loading with fast_inference mode (vLLM backend)\n",
    "- Thinking model generates `<think>...</think>` content\n",
    "- Parsing separates thinking from final response\n",
    "- Self-questioning reasoning style in thinking block\n",
    "\n",
    "### Ready for Production\n",
    "If this test passed, your environment is ready for:\n",
    "- Thinking model inference with vLLM acceleration\n",
    "- Chain-of-thought reasoning workflows\n",
    "- Training notebooks that require thinking output parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31bb5f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down kernel to release GPU memory..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shutdown kernel to release all GPU memory\n",
    "import IPython\n",
    "print(\"Shutting down kernel to release GPU memory...\")\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
