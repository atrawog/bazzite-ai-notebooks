{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54084e44",
   "metadata": {},
   "source": [
    "# Fast Inference Test: Qwen3-4B-Thinking-2507\n",
    "\n",
    "Tests `fast_inference=True` with vLLM backend on Qwen3-4B-Thinking-2507.\n",
    "\n",
    "**Key features tested:**\n",
    "- FastLanguageModel loading with fast_inference mode\n",
    "- Thinking model output with `<think>...</think>` tags\n",
    "- Parsing and displaying thinking vs response separately\n",
    "\n",
    "**Important:** This notebook includes a kernel shutdown cell at the end.\n",
    "vLLM does not release GPU memory in single-process mode (Jupyter), so kernel\n",
    "restart is required between different model tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c6c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup (quiet mode)\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Suppress all verbose output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "logging.getLogger(\"unsloth\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"vllm\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Suppress unsloth banner during import\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from io import StringIO\n",
    "with redirect_stdout(StringIO()), redirect_stderr(StringIO()):\n",
    "    import unsloth\n",
    "    from unsloth import FastLanguageModel\n",
    "\n",
    "import vllm\n",
    "import torch\n",
    "\n",
    "# Suppress model loading verbosity\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# Single-line environment summary\n",
    "gpu = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"Environment: unsloth {unsloth.__version__}, vLLM {vllm.__version__}, {gpu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a123208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Qwen3-4B-Thinking-2507 with fast_inference=True\n",
    "MODEL_NAME = \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\"\n",
    "print(f\"\\nTesting {MODEL_NAME.split('/')[-1]} with fast_inference=True...\")\n",
    "\n",
    "from vllm import SamplingParams\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Suppress verbose model loading output by redirecting to /dev/null\n",
    "_stdout_fd = os.dup(1)\n",
    "_stderr_fd = os.dup(2)\n",
    "_devnull = os.open(os.devnull, os.O_WRONLY)\n",
    "os.dup2(_devnull, 1)\n",
    "os.dup2(_devnull, 2)\n",
    "\n",
    "try:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        max_seq_length=1024,  # Increased for thinking content\n",
    "        load_in_4bit=True,\n",
    "        fast_inference=True,\n",
    "    )\n",
    "finally:\n",
    "    os.dup2(_stdout_fd, 1)\n",
    "    os.dup2(_stderr_fd, 2)\n",
    "    os.close(_devnull)\n",
    "    os.close(_stdout_fd)\n",
    "    os.close(_stderr_fd)\n",
    "\n",
    "print(f\"Model loaded: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ade5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation with thinking model\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Use a prompt that encourages reasoning\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is 15 + 27? Show your thinking.\"}]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.6,  # Recommended for thinking models\n",
    "    top_p=0.95,\n",
    "    top_k=20,\n",
    "    max_tokens=256,  # Allow room for thinking\n",
    ")\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "outputs = model.fast_generate([prompt], sampling_params=sampling_params)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "# Get the raw output\n",
    "raw_output = outputs[0].outputs[0].text\n",
    "print(f\"Generation time: {elapsed:.2f}s\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RAW OUTPUT:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc74ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse thinking content vs final response\n",
    "import re\n",
    "\n",
    "def parse_thinking_response(text):\n",
    "    \"\"\"\n",
    "    Parse thinking model output into thinking content and final response.\n",
    "    Thinking-2507 models output <think>...</think> followed by response.\n",
    "    \"\"\"\n",
    "    # Try to find thinking block\n",
    "    think_match = re.search(r'<think>(.*?)</think>', text, re.DOTALL)\n",
    "    \n",
    "    if think_match:\n",
    "        thinking = think_match.group(1).strip()\n",
    "        # Response is everything after </think>\n",
    "        response = text[think_match.end():].strip()\n",
    "    else:\n",
    "        # No thinking tags found - check if output starts with thinking content\n",
    "        # (Thinking-2507 models may output only </think> as the template adds <think>)\n",
    "        if '</think>' in text:\n",
    "            parts = text.split('</think>', 1)\n",
    "            thinking = parts[0].strip()\n",
    "            response = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "        else:\n",
    "            thinking = \"\"\n",
    "            response = text.strip()\n",
    "    \n",
    "    return thinking, response\n",
    "\n",
    "# Parse the output\n",
    "thinking_content, response_content = parse_thinking_response(raw_output)\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"THINKING CONTENT:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(thinking_content if thinking_content else \"(No thinking content found)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL RESPONSE:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(response_content if response_content else \"(No response found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc2d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification summary\n",
    "has_thinking = bool(thinking_content)\n",
    "has_response = bool(response_content)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"VERIFICATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"FastInference: ✅ SUPPORTED\")\n",
    "print(f\"Thinking tags present: {'✅ YES' if has_thinking else '❌ NO'}\")\n",
    "print(f\"Response generated: {'✅ YES' if has_response else '❌ NO'}\")\n",
    "print(f\"Generation time: {elapsed:.2f}s\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if has_thinking and has_response:\n",
    "    print(\"\\n✅ Qwen3-4B-Thinking-2507 Fast Inference Test PASSED\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Test completed but thinking output may need review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9913161",
   "metadata": {},
   "source": [
    "## Test Complete\n",
    "\n",
    "The Qwen3-4B-Thinking-2507 fast_inference test has completed. The kernel will now shut down to release all GPU memory.\n",
    "\n",
    "### What Was Verified\n",
    "- FastLanguageModel loading with fast_inference mode (vLLM backend)\n",
    "- Thinking model generates `<think>...</think>` content\n",
    "- Parsing separates thinking from final response\n",
    "- Self-questioning reasoning style in thinking block\n",
    "\n",
    "### Ready for Production\n",
    "If this test passed, your environment is ready for:\n",
    "- Thinking model inference with vLLM acceleration\n",
    "- Chain-of-thought reasoning workflows\n",
    "- Training notebooks that require thinking output parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bb5f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown kernel to release all GPU memory\n",
    "import IPython\n",
    "print(\"Shutting down kernel to release GPU memory...\")\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
