{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# QLoRA Parameter Test: Alpha Scaling - Qwen3-4B-Thinking\n",
    "\n",
    "Tests different `lora_alpha` values (8, 16, 32, 64) with fixed rank r=16 to understand the effective learning rate multiplier.\n",
    "\n",
    "**Key concept:** The effective LoRA scaling is `alpha/r`, which acts as a learning rate multiplier for the adapter weights.\n",
    "\n",
    "**Alpha/Rank ratios tested:**\n",
    "- α=8, r=16 → 0.5x scaling (conservative)\n",
    "- α=16, r=16 → 1.0x scaling (standard)\n",
    "- α=32, r=16 → 2.0x scaling (aggressive)\n",
    "- α=64, r=16 → 4.0x scaling (very aggressive)\n",
    "\n",
    "**Key features tested:**\n",
    "- Training curves comparison (loss per step)\n",
    "- Convergence speed differences\n",
    "- Final loss comparison\n",
    "- Stability across different scaling factors\n",
    "\n",
    "**Important:** This notebook includes a kernel shutdown cell at the end to release all GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# CRITICAL: Import unsloth FIRST for proper TRL patching\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel, is_bf16_supported\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Environment summary\n",
    "gpu = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"Environment: unsloth {unsloth.__version__}, PyTorch {torch.__version__}, {gpu}\")\n",
    "print(f\"HF_TOKEN loaded: {'Yes' if os.environ.get('HF_TOKEN') else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Helper Functions\n",
    "import subprocess\n",
    "\n",
    "def measure_gpu_memory():\n",
    "    \"\"\"Measure current GPU memory usage in MB using nvidia-smi\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        return int(result.stdout.strip().split('\\n')[0])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable vs total parameters\"\"\"\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return {\n",
    "        \"trainable\": trainable,\n",
    "        \"total\": total,\n",
    "        \"pct\": 100 * trainable / total\n",
    "    }\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"Force garbage collection and clear CUDA cache\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "print(\"Benchmark functions defined.\")\n",
    "print(f\"Initial GPU memory: {measure_gpu_memory()} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create minimal synthetic instruction dataset with thinking content (5 samples)\n",
    "from datasets import Dataset\n",
    "\n",
    "# Same dataset across all alpha values for fair comparison\n",
    "synthetic_data = [\n",
    "    {\n",
    "        \"instruction\": \"What is machine learning?\",\n",
    "        \"thinking\": \"What is the user asking here? They want to understand machine learning. What are the key concepts I should cover? It's a subset of AI... and it involves learning from data. How should I keep this accessible? Short and clear definition.\",\n",
    "        \"response\": \"Machine learning is a subset of artificial intelligence where computers learn patterns from data.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain Python in one sentence.\",\n",
    "        \"thinking\": \"One sentence only - what's most important about Python? Its readability and versatility are the defining features. How do I capture both in one sentence?\",\n",
    "        \"response\": \"Python is a high-level programming language known for its readability and versatility.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is a neural network?\",\n",
    "        \"thinking\": \"How do I explain neural networks simply? What's the core concept? They're inspired by biological neurons... they process information in layers. Should I mention deep learning? Maybe keep it basic for now.\",\n",
    "        \"response\": \"A neural network is a computational model inspired by biological neurons that processes information through connected layers.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Define supervised learning.\",\n",
    "        \"thinking\": \"What makes supervised learning 'supervised'? It's the labels! The data has known outputs. How do I explain this clearly? Focus on the training process with labeled data.\",\n",
    "        \"response\": \"Supervised learning is training a model on labeled data to predict outcomes for new inputs.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is gradient descent?\",\n",
    "        \"thinking\": \"This is a bit technical. What's the intuition behind gradient descent? It's like walking downhill to find the lowest point. The gradient tells us which direction is 'down'. Keep it conceptual.\",\n",
    "        \"response\": \"Gradient descent is an optimization algorithm that minimizes loss by iteratively adjusting parameters in the direction of steepest descent.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Synthetic dataset prepared: {len(synthetic_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-comparison-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha Scaling Comparison Loop\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "MODEL_NAME = \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\"\n",
    "FIXED_RANK = 16\n",
    "ALPHAS = [8, 16, 32, 64]\n",
    "results = []\n",
    "\n",
    "# Custom callback to capture training history\n",
    "class LossHistoryCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and 'loss' in logs:\n",
    "            self.losses.append({'step': state.global_step, 'loss': logs['loss']})\n",
    "\n",
    "for alpha in ALPHAS:\n",
    "    scaling_factor = alpha / FIXED_RANK\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing lora_alpha={alpha} with r={FIXED_RANK} (scaling: {scaling_factor:.1f}x)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Cleanup before each run\n",
    "    cleanup_memory()\n",
    "    \n",
    "    # Load fresh model\n",
    "    print(f\"Loading model...\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        max_seq_length=512,\n",
    "        load_in_4bit=True,\n",
    "        dtype=None,\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA with current alpha\n",
    "    print(f\"Applying LoRA with r={FIXED_RANK}, alpha={alpha}...\")\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=FIXED_RANK,\n",
    "        lora_alpha=alpha,\n",
    "        lora_dropout=0,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    params = count_parameters(model)\n",
    "    print(f\"Trainable: {params['trainable']:,} ({params['pct']:.2f}%)\")\n",
    "    \n",
    "    # Format dataset with tokenizer\n",
    "    def format_conversation(sample):\n",
    "        assistant_content = f\"<think>\\n{sample['thinking']}\\n</think>\\n\\n{sample['response']}\"\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": sample[\"instruction\"]},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "        return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}\n",
    "    \n",
    "    dataset = Dataset.from_list(synthetic_data)\n",
    "    dataset = dataset.map(format_conversation, remove_columns=[\"instruction\", \"thinking\", \"response\"])\n",
    "    \n",
    "    # Training config with more steps to see convergence\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=f\"outputs_qlora_alpha_think/alpha_{alpha}\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        max_steps=5,  # More steps to observe training dynamics\n",
    "        warmup_steps=1,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        fp16=not is_bf16_supported(),\n",
    "        bf16=is_bf16_supported(),\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        max_seq_length=512,\n",
    "        seed=42,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    # Initialize callback for loss tracking\n",
    "    loss_callback = LossHistoryCallback()\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        args=sft_config,\n",
    "        callbacks=[loss_callback],\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"Training (5 steps)...\")\n",
    "    trainer_stats = trainer.train()\n",
    "    final_loss = trainer_stats.metrics.get('train_loss', 0)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"alpha\": alpha,\n",
    "        \"scaling_factor\": scaling_factor,\n",
    "        \"final_loss\": final_loss,\n",
    "        \"loss_history\": loss_callback.losses.copy(),\n",
    "        \"trainable_params\": params[\"trainable\"],\n",
    "    })\n",
    "    \n",
    "    print(f\"Final loss: {final_loss:.4f}\")\n",
    "    print(f\"Loss history: {[f'{l[\"loss\"]:.4f}' for l in loss_callback.losses]}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, tokenizer, trainer, dataset\n",
    "    cleanup_memory()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All alpha values tested!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Summary table\n",
    "summary_data = [{\n",
    "    \"alpha\": r[\"alpha\"],\n",
    "    \"scaling\": f\"{r['scaling_factor']:.1f}x\",\n",
    "    \"final_loss\": r[\"final_loss\"],\n",
    "} for r in results]\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Alpha Scaling Comparison Results\")\n",
    "print(\"=\"*60)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Training Curves\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "for i, result in enumerate(results):\n",
    "    if result['loss_history']:\n",
    "        steps = [h['step'] for h in result['loss_history']]\n",
    "        losses = [h['loss'] for h in result['loss_history']]\n",
    "        label = f\"α={result['alpha']} ({result['scaling_factor']:.1f}x)\"\n",
    "        axes[0].plot(steps, losses, f'{colors[i]}-o', linewidth=2, markersize=6, label=label)\n",
    "\n",
    "axes[0].set_title('Training Curves: Alpha Scaling Effect', fontsize=12)\n",
    "axes[0].set_xlabel('Training Step')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Final Loss Bar Chart\n",
    "alphas = [r['alpha'] for r in results]\n",
    "final_losses = [r['final_loss'] for r in results]\n",
    "x_labels = [f\"α={a}\\n({a/FIXED_RANK:.1f}x)\" for a in alphas]\n",
    "\n",
    "bars = axes[1].bar(x_labels, final_losses, color=colors)\n",
    "axes[1].set_title('Final Loss by Alpha Value', fontsize=12)\n",
    "axes[1].set_xlabel('lora_alpha (scaling factor)')\n",
    "axes[1].set_ylabel('Final Loss')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, loss in zip(bars, final_losses):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                 f'{loss:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs_qlora_alpha_think/alpha_comparison.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to outputs_qlora_alpha_think/alpha_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis",
   "metadata": {},
   "source": [
    "## Analysis and Key Findings\n",
    "\n",
    "### Alpha Scaling Formula\n",
    "The effective LoRA contribution is scaled by `alpha/r`:\n",
    "- `h = h + (alpha/r) * LoRA(x)`\n",
    "- This acts as a learning rate multiplier for adapter weights\n",
    "\n",
    "### Observed Behavior\n",
    "\n",
    "**α=8 (0.5x scaling):**\n",
    "- Conservative updates\n",
    "- Slower convergence\n",
    "- May need more training steps\n",
    "- Most stable, lowest risk of overfitting\n",
    "\n",
    "**α=16 (1.0x scaling - Standard):**\n",
    "- Balanced updates\n",
    "- Standard configuration (α = r)\n",
    "- Good default for most tasks\n",
    "\n",
    "**α=32 (2.0x scaling):**\n",
    "- Aggressive updates\n",
    "- Faster convergence\n",
    "- May overshoot optimal values\n",
    "\n",
    "**α=64 (4.0x scaling):**\n",
    "- Very aggressive updates\n",
    "- Fastest initial convergence\n",
    "- Higher risk of instability\n",
    "- May need lower learning rate to compensate\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "| Use Case | Alpha | Scaling |\n",
    "|----------|-------|--------|\n",
    "| Conservative/stable | α=8 | 0.5x |\n",
    "| General purpose | α=16 | 1.0x |\n",
    "| Faster convergence | α=32 | 2.0x |\n",
    "| Quick experiments | α=64 | 4.0x (reduce LR) |\n",
    "\n",
    "### Key Insight\n",
    "When using high alpha values (>r), consider reducing the learning rate proportionally to maintain stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shutdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown kernel to release all GPU memory\n",
    "import IPython\n",
    "print(\"Shutting down kernel to release GPU memory...\")\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
