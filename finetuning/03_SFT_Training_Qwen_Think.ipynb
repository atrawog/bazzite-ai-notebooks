{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97dbd5a2",
   "metadata": {},
   "source": [
    "# SFT Training Test: Qwen3-4B-Thinking-2507\n",
    "\n",
    "Tests Supervised Fine-Tuning with Unsloth's optimized SFTTrainer on Qwen3-4B-Thinking-2507.\n",
    "\n",
    "**Key features tested:**\n",
    "- FastLanguageModel loading with 4-bit quantization\n",
    "- LoRA adapter configuration\n",
    "- SFTTrainer with synthetic dataset including `<think>` content\n",
    "- Training the model to produce self-questioning reasoning\n",
    "- Post-training inference verification\n",
    "\n",
    "**Thinking Style:** Self-questioning internal dialogue\n",
    "- \"What is the user asking here?\"\n",
    "- \"Let me think about the key concepts...\"\n",
    "- \"How should I structure this explanation?\"\n",
    "\n",
    "**Important:** This notebook includes a kernel shutdown cell at the end to release all GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b4f079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/pixi/.pixi/envs/default/lib/python3.13/site-packages/trl/__init__.py:203: UserWarning: TRL currently supports vLLM versions: 0.10.2, 0.11.0, 0.11.1, 0.11.2. You have version 0.14.0rc1.dev201+gadcf682fc.cu130 installed. We recommend installing a supported version to avoid compatibility issues.\n",
      "  if is_vllm_available():"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!Environment: unsloth 2025.12.10, PyTorch 2.9.1+cu130, NVIDIA GeForce RTX 4080 SUPER\n",
      "HF_TOKEN loaded: Yes"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Force text-based progress instead of HTML widgets\n",
    "os.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n",
    "\n",
    "# CRITICAL: Import unsloth FIRST for proper TRL patching\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel, is_bf16_supported\n",
    "\n",
    "import torch\n",
    "\n",
    "# Environment summary\n",
    "gpu = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"Environment: unsloth {unsloth.__version__}, PyTorch {torch.__version__}, {gpu}\")\n",
    "print(f\"HF_TOKEN loaded: {'Yes' if os.environ.get('HF_TOKEN') else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430ba34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Qwen3-4B-Thinking-2507-unsloth-bnb-4bit...==((====))==  Unsloth 2025.12.10: Fast Qwen3 patching. Transformers: 5.0.0.1. vLLM: 0.14.0rc1.dev201+gadcf682fc.cu130.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.568 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu130. CUDA: 8.9. CUDA Toolkit: 13.0. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59068d19d1da4979a3e9488e60a04ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Qwen3ForCausalLM"
     ]
    }
   ],
   "source": [
    "# Load Qwen3-4B-Thinking-2507 with 4-bit quantization\n",
    "MODEL_NAME = \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\"\n",
    "print(f\"\\nLoading {MODEL_NAME.split('/')[-1]}...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    max_seq_length=1024,  # Increased for thinking content\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,  # Auto-detect\n",
    ")\n",
    "print(f\"Model loaded: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb35d151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.12.10 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA applied: 33,030,144 trainable / 2,526,543,360 total (1.31%)"
     ]
    }
   ],
   "source": [
    "# Apply LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"LoRA applied: {trainable:,} trainable / {total:,} total ({100*trainable/total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c79e243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10288c1ad4842e29b831fb8f0a37c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created: 5 samples\n",
      "\n",
      "Sample formatted text:\n",
      "<|im_start|>user\n",
      "What is machine learning?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "What is the user asking here? They want to understand machine learning. What are the key concepts I should cover? It's a subset of AI... and it involves learning from data. How should I keep this accessible? Short and clear definition.\n",
      "</think>\n",
      "\n",
      "Machine learning is a subset of artificial intelligence where computers learn patterns from data.<|im_end|>\n",
      "..."
     ]
    }
   ],
   "source": [
    "# Create minimal synthetic instruction dataset with thinking content (5 samples)\n",
    "# Using self-questioning internal dialogue style for thinking\n",
    "from datasets import Dataset\n",
    "\n",
    "synthetic_data = [\n",
    "    {\n",
    "        \"instruction\": \"What is machine learning?\",\n",
    "        \"thinking\": \"What is the user asking here? They want to understand machine learning. What are the key concepts I should cover? It's a subset of AI... and it involves learning from data. How should I keep this accessible? Short and clear definition.\",\n",
    "        \"response\": \"Machine learning is a subset of artificial intelligence where computers learn patterns from data.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain Python in one sentence.\",\n",
    "        \"thinking\": \"One sentence only - what's most important about Python? Its readability and versatility are the defining features. How do I capture both in one sentence?\",\n",
    "        \"response\": \"Python is a high-level programming language known for its readability and versatility.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is a neural network?\",\n",
    "        \"thinking\": \"How do I explain neural networks simply? What's the core concept? They're inspired by biological neurons... they process information in layers. Should I mention deep learning? Maybe keep it basic for now.\",\n",
    "        \"response\": \"A neural network is a computational model inspired by biological neurons that processes information through connected layers.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Define supervised learning.\",\n",
    "        \"thinking\": \"What makes supervised learning 'supervised'? It's the labels! The data has known outputs. How do I explain this clearly? Focus on the training process with labeled data.\",\n",
    "        \"response\": \"Supervised learning is training a model on labeled data to predict outcomes for new inputs.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is gradient descent?\",\n",
    "        \"thinking\": \"This is a bit technical. What's the intuition behind gradient descent? It's like walking downhill to find the lowest point. The gradient tells us which direction is 'down'. Keep it conceptual.\",\n",
    "        \"response\": \"Gradient descent is an optimization algorithm that minimizes loss by iteratively adjusting parameters in the direction of steepest descent.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Format as chat conversations with thinking content\n",
    "def format_conversation(sample):\n",
    "    # Combine thinking and response with proper tags\n",
    "    assistant_content = f\"<think>\\n{sample['thinking']}\\n</think>\\n\\n{sample['response']}\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": sample[\"instruction\"]},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "    ]\n",
    "    return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}\n",
    "\n",
    "dataset = Dataset.from_list(synthetic_data)\n",
    "dataset = dataset.map(format_conversation, remove_columns=[\"instruction\", \"thinking\", \"response\"])\n",
    "print(f\"Dataset created: {len(dataset)} samples\")\n",
    "print(f\"\\nSample formatted text:\")\n",
    "print(dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc0836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT Training (minimal steps for testing)\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"outputs_sft_qwen_think_test\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    max_steps=3,  # Minimal steps for testing\n",
    "    warmup_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=1,\n",
    "    fp16=not is_bf16_supported(),\n",
    "    bf16=is_bf16_supported(),\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    max_seq_length=1024,  # Increased for thinking content\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    args=sft_config,\n",
    ")\n",
    "\n",
    "print(\"Starting SFT training with thinking content (3 steps)...\")\n",
    "trainer_stats = trainer.train()\n",
    "final_loss = trainer_stats.metrics.get('train_loss', 'N/A')\n",
    "print(f\"Training completed. Final loss: {final_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0fd7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SFT Training Pipeline Test (Thinking Mode)\n",
      "============================================================\n",
      "</think> token found: ‚úÖ YES\n",
      "Output tokens: 1024\n",
      "\n",
      "THINKING CONTENT:\n",
      "Okay, the user is asking \"What is deep learning?\" Hmm, this seems like a pretty basic question about AI, but I should be careful not to assume their level of knowledge. Maybe they're a complete beginner? Or could they be a student studying machine learning? \n",
      "\n",
      "First, I need to define deep learning clearly but simply. I should avoid jargon overload. The key is to connect it to something they might k...\n",
      "\n",
      "FINAL RESPONSE:\n",
      "**Deep learning** is a **subset of machine learning** that uses **artificial neural networks** with multiple layers (hence \"deep\") to automatically learn complex patterns from large amounts of data. It‚Äôs designed to mimic the human brain‚Äôs ability to recognize patterns in unstructured data (like ima...\n",
      "\n",
      "‚úÖ SFT Training Pipeline Test PASSED"
     ]
    }
   ],
   "source": [
    "# Post-training inference test\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is deep learning?\"}]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,  # Increased to allow full thinking + response\n",
    "        temperature=0.6,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "# Get generated token IDs only (exclude prompt)\n",
    "input_length = inputs[\"input_ids\"].shape[1]\n",
    "generated_ids = outputs[0][input_length:].tolist()\n",
    "\n",
    "# Token-based parsing using </think> token ID\n",
    "THINK_END_TOKEN_ID = 151668\n",
    "\n",
    "if THINK_END_TOKEN_ID in generated_ids:\n",
    "    end_idx = generated_ids.index(THINK_END_TOKEN_ID)\n",
    "    thinking = tokenizer.decode(generated_ids[:end_idx], skip_special_tokens=True).strip()\n",
    "    final_response = tokenizer.decode(generated_ids[end_idx + 1:], skip_special_tokens=True).strip()\n",
    "    think_tag_found = True\n",
    "else:\n",
    "    thinking = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "    final_response = \"(Model did not complete thinking - increase max_new_tokens)\"\n",
    "    think_tag_found = False\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SFT Training Pipeline Test (Thinking Mode)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"</think> token found: {'‚úÖ YES' if think_tag_found else '‚ùå NO'}\")\n",
    "print(f\"Output tokens: {len(generated_ids)}\")\n",
    "print(f\"\\nTHINKING CONTENT:\")\n",
    "print(thinking[:400] + \"...\" if len(thinking) > 400 else thinking)\n",
    "print(f\"\\nFINAL RESPONSE:\")\n",
    "print(final_response[:300] + \"...\" if len(final_response) > 300 else final_response)\n",
    "\n",
    "if think_tag_found and thinking and final_response:\n",
    "    print(\"\\n‚úÖ SFT Training Pipeline Test PASSED\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Test completed but output may need review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de569d4d",
   "metadata": {},
   "source": [
    "## Test Complete\n",
    "\n",
    "The SFT Training Pipeline test with thinking content has completed successfully. The kernel will now shut down to release all GPU memory.\n",
    "\n",
    "### What Was Verified\n",
    "- FastLanguageModel loading with 4-bit quantization (Qwen3-4B-Thinking-2507)\n",
    "- LoRA adapter configuration (r=16, all projection modules)\n",
    "- Synthetic dataset creation with `<think>` tags and self-questioning style\n",
    "- SFTTrainer training loop (3 steps)\n",
    "- Post-training inference with thinking output\n",
    "\n",
    "### Thinking Style Trained\n",
    "- Self-questioning internal dialogue\n",
    "- \"What is the user asking?\" pattern\n",
    "- \"How should I structure this?\" reasoning\n",
    "\n",
    "### Ready for Production\n",
    "If this test passed, your environment is ready for:\n",
    "- Full SFT fine-tuning on larger datasets with thinking content\n",
    "- Chain-of-thought training workflows\n",
    "- Model saving and deployment with thinking capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0820b553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down kernel to release GPU memory..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shutdown kernel to release all GPU memory\n",
    "import IPython\n",
    "print(\"Shutting down kernel to release GPU memory...\")\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
