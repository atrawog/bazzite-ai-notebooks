{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# QLoRA Advanced: Multi-Adapter Training - Qwen3-4B-Thinking\n",
    "\n",
    "Demonstrates training multiple task-specific LoRA adapters and switching between them at inference.\n",
    "\n",
    "**Tasks/Adapters:**\n",
    "1. **Technical Q&A** - Precise technical explanations\n",
    "2. **Creative Writing** - Imaginative, expressive responses\n",
    "3. **Code Explanation** - Code-focused analysis\n",
    "\n",
    "**Key features demonstrated:**\n",
    "- Training independent adapters on different datasets\n",
    "- Saving lightweight adapter weights (~33M params each)\n",
    "- Switching adapters at inference time\n",
    "- Same prompt â†’ different responses per adapter\n",
    "- Thinking capability preserved across all adapters\n",
    "\n",
    "**Benefits of multi-adapter approach:**\n",
    "- Modular specialization without retraining base model\n",
    "- Hot-swap capabilities at runtime\n",
    "- Storage efficient (adapters are ~130MB each vs ~8GB base model)\n",
    "\n",
    "**Important:** This notebook includes a kernel shutdown cell at the end to release all GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "env-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/pixi/.pixi/envs/default/lib/python3.13/site-packages/trl/__init__.py:203: UserWarning: TRL currently supports vLLM versions: 0.10.2, 0.11.0, 0.11.1, 0.11.2. You have version 0.14.0rc1.dev201+gadcf682fc.cu130 installed. We recommend installing a supported version to avoid compatibility issues.\n  if is_vllm_available():"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!",
      "Environment: unsloth 2025.12.10, PyTorch 2.9.1+cu130, NVIDIA GeForce RTX 4080 SUPER\nHF_TOKEN loaded: Yes"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# CRITICAL: Import unsloth FIRST for proper TRL patching\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel, is_bf16_supported\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Environment summary\n",
    "gpu = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"Environment: unsloth {unsloth.__version__}, PyTorch {torch.__version__}, {gpu}\")\n",
    "print(f\"HF_TOKEN loaded: {'Yes' if os.environ.get('HF_TOKEN') else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "helper-functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined."
     ]
    }
   ],
   "source": [
    "# Helper Functions\n",
    "import subprocess\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"Force garbage collection and clear CUDA cache\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable vs total parameters\"\"\"\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return {\"trainable\": trainable, \"total\": total, \"pct\": 100 * trainable / total}\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "task-datasets",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task datasets prepared:\n  - technical: 5 samples\n  - creative: 5 samples\n  - code: 5 samples"
     ]
    }
   ],
   "source": [
    "# Task-Specific Datasets (5 samples each)\n",
    "from datasets import Dataset\n",
    "\n",
    "# Task 1: Technical Q&A - precise, factual responses\n",
    "technical_data = [\n",
    "    {\n",
    "        \"instruction\": \"What is a hash table?\",\n",
    "        \"thinking\": \"The user wants a technical explanation. What are the key aspects? It's a data structure... uses hashing for O(1) lookups... handles collisions. Keep it precise but accessible.\",\n",
    "        \"response\": \"A hash table is a data structure that maps keys to values using a hash function. It provides O(1) average-case time complexity for insertions, deletions, and lookups by computing an index from the key.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain TCP vs UDP.\",\n",
    "        \"thinking\": \"Compare two network protocols. TCP is reliable, connection-oriented. UDP is fast, connectionless. What are the key differences to highlight?\",\n",
    "        \"response\": \"TCP provides reliable, ordered delivery with connection establishment and error checking. UDP is connectionless and faster but unreliable. TCP is used for web traffic; UDP for streaming and gaming.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is Big O notation?\",\n",
    "        \"thinking\": \"Fundamental CS concept. It describes algorithmic complexity. How do I explain it simply? Focus on growth rate and worst-case analysis.\",\n",
    "        \"response\": \"Big O notation describes the upper bound of an algorithm's time or space complexity as input size grows. O(n) means linear growth, O(nÂ²) means quadratic, and O(1) means constant time.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is recursion?\",\n",
    "        \"thinking\": \"Classic programming concept. A function calling itself with a base case. Should I mention stack frames? Keep it conceptual first.\",\n",
    "        \"response\": \"Recursion is when a function calls itself to solve smaller instances of the same problem. It requires a base case to terminate and recursive cases that reduce toward the base case.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain database indexing.\",\n",
    "        \"thinking\": \"Database performance topic. Indexes speed up queries... B-trees are common. Trade-off with insert performance. Focus on the core concept.\",\n",
    "        \"response\": \"Database indexing creates data structures that speed up query operations by allowing the database to find rows without scanning entire tables. Indexes trade storage space and insert speed for faster reads.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Task 2: Creative Writing - imaginative, expressive responses\n",
    "creative_data = [\n",
    "    {\n",
    "        \"instruction\": \"Write a haiku about coding.\",\n",
    "        \"thinking\": \"Haiku: 5-7-5 syllables. Theme is coding. What imagery captures the experience? Bugs, late nights, the flow state... Let me craft something evocative.\",\n",
    "        \"response\": \"Lines of logic flow\\nBugs dance in the midnight glow\\nFixedâ€”the dawn says so\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Describe a sunset poetically.\",\n",
    "        \"thinking\": \"Poetry mode. What makes sunsets beautiful? Colors, transition, emotion. Use metaphor and sensory language.\",\n",
    "        \"response\": \"The sun dips low, painting the sky in watercolors of amber and rose. Day surrenders to twilight's embrace as the horizon glows like embers of a dying fire, beautiful in its farewell.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Create a metaphor for learning.\",\n",
    "        \"thinking\": \"What is learning like? A journey? Growing? Building? Something that captures the gradual, sometimes difficult process...\",\n",
    "        \"response\": \"Learning is climbing a mountain in fogâ€”each step reveals new terrain, the path unclear until you've walked it. The summit exists, but the journey shapes you more than arrival.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a short opening line for a mystery novel.\",\n",
    "        \"thinking\": \"Mystery opening needs intrigue. Something that raises questions. A hook that makes readers want more. Tension from the first sentence.\",\n",
    "        \"response\": \"The letter arrived three years after her death, written in handwriting that was unmistakably hers.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Describe rain in an unusual way.\",\n",
    "        \"thinking\": \"Avoid clichÃ©s. What's a fresh perspective on rain? Sound, feeling, unusual comparisons... Make it memorable.\",\n",
    "        \"response\": \"The sky plays percussion on the rooftopsâ€”a million tiny drummers performing chaos into rhythm, washing the world in liquid applause.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Task 3: Code Explanation - code-focused analysis\n",
    "code_data = [\n",
    "    {\n",
    "        \"instruction\": \"Explain: def fib(n): return n if n<2 else fib(n-1)+fib(n-2)\",\n",
    "        \"thinking\": \"Classic recursive Fibonacci. Base case n<2, recursive case adds previous two. What should I highlight? The elegance but also the O(2^n) complexity.\",\n",
    "        \"response\": \"This is a recursive Fibonacci implementation. Base case: returns n for 0 or 1. Recursive case: sums the two preceding values. Elegant but O(2^n) timeâ€”use memoization for efficiency.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What does [x*2 for x in range(5)] do?\",\n",
    "        \"thinking\": \"Python list comprehension. Creates a list of doubled values. range(5) gives 0-4. Result is [0,2,4,6,8]. Explain the syntax pattern.\",\n",
    "        \"response\": \"This list comprehension creates [0, 2, 4, 6, 8]. It iterates x through range(5) (0-4) and multiplies each by 2. Syntax: [expression for item in iterable].\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain Python decorators.\",\n",
    "        \"thinking\": \"Decorators wrap functions. They're higher-order functions. @syntax is syntactic sugar. Use cases: logging, timing, auth. Start with the concept.\",\n",
    "        \"response\": \"Decorators are functions that modify other functions. The @decorator syntax wraps a function, adding behavior before/after execution. Common uses: logging, authentication, caching, timing.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is async/await in Python?\",\n",
    "        \"thinking\": \"Asynchronous programming. Coroutines, event loop, non-blocking I/O. How do I explain this concisely? Focus on the why and basic usage.\",\n",
    "        \"response\": \"async/await enables non-blocking I/O. 'async def' creates coroutines; 'await' pauses execution until a task completes. The event loop manages concurrent operations without threads.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain: lambda x,y: x if x>y else y\",\n",
    "        \"thinking\": \"Lambda function returning the max of two values. Anonymous function, single expression. Equivalent to max() for two values.\",\n",
    "        \"response\": \"This lambda returns the larger of x and y using a ternary expression. It's equivalent to max(x,y). Lambda creates anonymous, single-expression functions.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "TASK_DATASETS = {\n",
    "    \"technical\": technical_data,\n",
    "    \"creative\": creative_data,\n",
    "    \"code\": code_data,\n",
    "}\n",
    "\n",
    "print(f\"Task datasets prepared:\")\n",
    "for name, data in TASK_DATASETS.items():\n",
    "    print(f\"  - {name}: {len(data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "train-adapters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n============================================================\nTraining adapter: technical\n============================================================\nLoading model...",
      "==((====))==  Unsloth 2025.12.10: Fast Qwen3 patching. Transformers: 5.0.0.1. vLLM: 0.14.0rc1.dev201+gadcf682fc.cu130.\n   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.568 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.9.1+cu130. CUDA: 8.9. CUDA Toolkit: 13.0. Triton: 3.5.1\n\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d1b215bce943e3b04f0f78d767d476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.12.10 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 33,030,144 (1.31%)"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9158e97f92674f6ab642c612066eb8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.",
      "[trl.trainer.sft_trainer|WARNING]You are using a per_device_train_batch_size of 1 with padding-free training. Using a batch size of 1 anihilate the benefits of padding-free training. Please consider increasing the batch size to at least 2.",
      "num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.",
      "[datasets.arrow_dataset|WARNING]num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d23fd0b5ed41b99260cbaa5c8ffe97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=5):   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Padding-free auto-enabled, enabling faster training.\nTraining (5 steps)..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 5 | Num Epochs = 1 | Total steps = 5\nO^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 1\n\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 1 x 1) = 1\n \"-____-\"     Trainable parameters = 33,030,144 of 4,055,498,240 (0.81% trained)"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/5 : < :, Epoch 0.20/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 2.9803",
      "Adapter saved to: outputs_qlora_multi_adapter_think/technical_adapter\n\n============================================================\nTraining adapter: creative\n============================================================",
      "Loading model...",
      "==((====))==  Unsloth 2025.12.10: Fast Qwen3 patching. Transformers: 5.0.0.1. vLLM: 0.14.0rc1.dev201+gadcf682fc.cu130.\n   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.568 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.9.1+cu130. CUDA: 8.9. CUDA Toolkit: 13.0. Triton: 3.5.1\n\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24e8fe59a104693a25b9906f6dd89f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA...",
      "Trainable: 33,030,144 (1.31%)"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1cf1a51f0940319a70de8ea028611c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.",
      "num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.",
      "[datasets.arrow_dataset|WARNING]num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c92410a725741a9899965b1b2dbbbe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=5):   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (5 steps)..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 5 | Num Epochs = 1 | Total steps = 5\nO^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 1\n\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 1 x 1) = 1\n \"-____-\"     Trainable parameters = 33,030,144 of 4,055,498,240 (0.81% trained)"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/5 : < :, Epoch 0.20/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 3.6876",
      "Adapter saved to: outputs_qlora_multi_adapter_think/creative_adapter\n\n============================================================\nTraining adapter: code\n============================================================",
      "Loading model...",
      "==((====))==  Unsloth 2025.12.10: Fast Qwen3 patching. Transformers: 5.0.0.1. vLLM: 0.14.0rc1.dev201+gadcf682fc.cu130.\n   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.568 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.9.1+cu130. CUDA: 8.9. CUDA Toolkit: 13.0. Triton: 3.5.1\n\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd29ba9f8cb495a80252ac6810a115a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA...",
      "Trainable: 33,030,144 (1.31%)"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4578c5540f0f49abb5fc3624923aa71f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.",
      "num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.",
      "[datasets.arrow_dataset|WARNING]num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb3c1bfa1b645bebff746db84c274aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=5):   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (5 steps)..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 5 | Num Epochs = 1 | Total steps = 5\nO^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 1\n\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 1 x 1) = 1\n \"-____-\"     Trainable parameters = 33,030,144 of 4,055,498,240 (0.81% trained)"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/5 : < :, Epoch 0.20/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 2.8311",
      "Adapter saved to: outputs_qlora_multi_adapter_think/code_adapter\n\n============================================================\nAll adapters trained and saved!\n============================================================\n\nAdapter paths:\n  - technical: outputs_qlora_multi_adapter_think/technical_adapter\n  - creative: outputs_qlora_multi_adapter_think/creative_adapter\n  - code: outputs_qlora_multi_adapter_think/code_adapter"
     ]
    }
   ],
   "source": [
    "# Train Multiple Adapters\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "MODEL_NAME = \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\"\n",
    "OUTPUT_BASE = \"outputs_qlora_multi_adapter_think\"\n",
    "adapter_paths = {}\n",
    "\n",
    "for task_name, task_data in TASK_DATASETS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training adapter: {task_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    cleanup_memory()\n",
    "    \n",
    "    # Load fresh model\n",
    "    print(f\"Loading model...\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        max_seq_length=512,\n",
    "        load_in_4bit=True,\n",
    "        dtype=None,\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA\n",
    "    print(f\"Applying LoRA...\")\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    params = count_parameters(model)\n",
    "    print(f\"Trainable: {params['trainable']:,} ({params['pct']:.2f}%)\")\n",
    "    \n",
    "    # Format dataset\n",
    "    def format_conversation(sample):\n",
    "        assistant_content = f\"<think>\\n{sample['thinking']}\\n</think>\\n\\n{sample['response']}\"\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": sample[\"instruction\"]},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "        return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}\n",
    "    \n",
    "    dataset = Dataset.from_list(task_data)\n",
    "    dataset = dataset.map(format_conversation, remove_columns=[\"instruction\", \"thinking\", \"response\"])\n",
    "    \n",
    "    # Training config\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=f\"{OUTPUT_BASE}/{task_name}_adapter\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        max_steps=5,  # Few steps for each task\n",
    "        warmup_steps=1,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        fp16=not is_bf16_supported(),\n",
    "        bf16=is_bf16_supported(),\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        max_seq_length=512,\n",
    "        seed=42,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        args=sft_config,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"Training (5 steps)...\")\n",
    "    trainer_stats = trainer.train()\n",
    "    final_loss = trainer_stats.metrics.get('train_loss', 0)\n",
    "    print(f\"Final loss: {final_loss:.4f}\")\n",
    "    \n",
    "    # Save adapter\n",
    "    adapter_path = f\"{OUTPUT_BASE}/{task_name}_adapter\"\n",
    "    model.save_pretrained(adapter_path)\n",
    "    tokenizer.save_pretrained(adapter_path)\n",
    "    adapter_paths[task_name] = adapter_path\n",
    "    print(f\"Adapter saved to: {adapter_path}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, tokenizer, trainer, dataset\n",
    "    cleanup_memory()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All adapters trained and saved!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"\\nAdapter paths:\")\n",
    "for name, path in adapter_paths.items():\n",
    "    print(f\"  - {name}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adapter-switching-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\nAdapter Switching Demonstration\n============================================================\n\nSame prompt with different adapters:\nPrompt: 'Explain what a variable is.'\n============================================================\n\nLoading base model...",
      "==((====))==  Unsloth 2025.12.10: Fast Qwen3 patching. Transformers: 5.0.0.1. vLLM: 0.14.0rc1.dev201+gadcf682fc.cu130.\n   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.568 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.9.1+cu130. CUDA: 8.9. CUDA Toolkit: 13.0. Triton: 3.5.1\n\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460e56a72b5d42cfa3b858aca01eefa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Loading technical adapter ---",
      "Response preview: Okay, the user asked me to explain what a variable is. Hmm, this seems like a basic programming question, but I should be careful not to assume their level of knowledge. Maybe they're a complete begin...\n\n--- Loading creative adapter ---"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/pixi/.pixi/envs/default/lib/python3.13/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response preview: Okay, the user asked me to \"explain what a variable is.\" Hmm, this seems like a pretty basic programming question, but I should be careful not to assume their level of knowledge. Maybe they're a compl...\n\n--- Loading code adapter ---",
      "Response preview: Okay, the user asked me to explain what a variable is. Hmm, this seems like a pretty basic programming question, but I should be careful not to assume their level. Maybe they're a total beginner or ju...",
      "\n============================================================\nDemonstration complete!\n============================================================"
     ]
    }
   ],
   "source": [
    "# Adapter Switching Demonstration\n",
    "from peft import PeftModel\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Adapter Switching Demonstration\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nSame prompt with different adapters:\")\n",
    "print(\"Prompt: 'Explain what a variable is.'\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "TEST_PROMPT = \"Explain what a variable is.\"\n",
    "results = {}\n",
    "\n",
    "# Load base model once\n",
    "cleanup_memory()\n",
    "print(\"\\nLoading base model...\")\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "for task_name, adapter_path in adapter_paths.items():\n",
    "    print(f\"\\n--- Loading {task_name} adapter ---\")\n",
    "    \n",
    "    # Load adapter on base model\n",
    "    adapted_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    FastLanguageModel.for_inference(adapted_model)\n",
    "    \n",
    "    # Generate response\n",
    "    messages = [{\"role\": \"user\", \"content\": TEST_PROMPT}]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = adapted_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.6,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    results[task_name] = response\n",
    "    \n",
    "    print(f\"Response preview: {response[:200]}...\")\n",
    "    \n",
    "    # Unload adapter\n",
    "    del adapted_model\n",
    "    cleanup_memory()\n",
    "\n",
    "# Cleanup base model\n",
    "del base_model, tokenizer\n",
    "cleanup_memory()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Demonstration complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "compare-responses",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\nResponse Comparison: Same Prompt, Different Adapters\n============================================================\n\nPrompt: 'Explain what a variable is.'\n\n\n[TECHNICAL ADAPTER]\n----------------------------------------\nOkay, the user asked me to explain what a variable is. Hmm, this seems like a basic programming question, but I should be careful not to assume their level of knowledge. Maybe they're a complete beginner, or perhaps they're just brushing up on fundamentals. \n\nFirst, I recall that in programming, variables are fundamental concepts. But the user didn't specify contextâ€”could they mean math variables too? Wait, the phrasing \"a variable\" without context usually implies programming in modern usage, but I should cover both to be safe. \n\nI wonder about their background. If they're a student, they might need concrete examples. If they're a professional, maybe they want concise technical details. Since they didn't specify, I'll aim\n\n\n[CREATIVE ADAPTER]\n----------------------------------------\nOkay, the user asked me to \"explain what a variable is.\" Hmm, this seems like a pretty basic programming question, but I should be careful not to assume their level of knowledge. Maybe they're a complete beginner, or perhaps they're just brushing up on fundamentals. \n\nFirst, I recall that variables are fundamental in programming, but the term \"variable\" has different meanings in math vs. programming. Since they didn't specify context, I should cover both briefly but emphasize programming since that's where the term is most commonly used in modern tech contexts. \n\nI wonder if they're asking because they're trying to write code and got stuck on something simple? Like maybe they saw \"x = 5\" in a tutorial and got confused.\n\n\n[CODE ADAPTER]\n----------------------------------------\nOkay, the user asked me to explain what a variable is. Hmm, this seems like a pretty basic programming question, but I should be careful not to assume their level. Maybe they're a total beginner or just someone brushing up on fundamentals. \n\nFirst, I recall that in programming, a variable is a fundamental concept. But I wonder if they mean variables in math too? The question doesn't specify context. Better cover both to be safe. \n\n*checks mental notes* Right, in math we have variables like x in equations, and in programming they're like containers for data. Should I mention both? Yeah, probably better to clarify the contexts upfront so they don't get confused. \n\nWait, the user might be a student doing homework\n"
     ]
    }
   ],
   "source": [
    "# Compare Responses Side by Side\n",
    "print(\"=\"*60)\n",
    "print(\"Response Comparison: Same Prompt, Different Adapters\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPrompt: '{TEST_PROMPT}'\\n\")\n",
    "\n",
    "for task_name, response in results.items():\n",
    "    print(f\"\\n[{task_name.upper()} ADAPTER]\")\n",
    "    print(\"-\" * 40)\n",
    "    # Show full response\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis",
   "metadata": {},
   "source": [
    "## Analysis and Key Findings\n",
    "\n",
    "### Multi-Adapter Benefits\n",
    "\n",
    "1. **Modularity**: Each adapter specializes in a specific task\n",
    "2. **Efficiency**: Adapters are ~130MB each vs ~8GB base model\n",
    "3. **Flexibility**: Hot-swap adapters at runtime\n",
    "4. **Preservation**: Base model capabilities remain intact\n",
    "\n",
    "### Response Style Differences\n",
    "\n",
    "| Adapter | Expected Style |\n",
    "|---------|---------------|\n",
    "| Technical | Precise, factual, structured |\n",
    "| Creative | Expressive, metaphorical, imaginative |\n",
    "| Code | Code-focused, syntax-aware, practical |\n",
    "\n",
    "### Thinking Preservation\n",
    "\n",
    "All adapters should maintain the `<think>...</think>` reasoning pattern from the base Qwen3-4B-Thinking model, but the thinking content may reflect the adapter's specialization.\n",
    "\n",
    "### Production Usage\n",
    "\n",
    "```python\n",
    "# Load base model once\n",
    "base_model = load_model()\n",
    "\n",
    "# Switch adapters based on task\n",
    "if user_task == \"technical\":\n",
    "    model = PeftModel.from_pretrained(base_model, \"technical_adapter\")\n",
    "elif user_task == \"creative\":\n",
    "    model = PeftModel.from_pretrained(base_model, \"creative_adapter\")\n",
    "# etc.\n",
    "```\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "Multi-adapter training enables creating specialized \"modes\" for an LLM without the overhead of multiple full models. The same base model can serve different use cases by loading the appropriate adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "shutdown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down kernel to release GPU memory..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shutdown kernel to release all GPU memory\n",
    "import IPython\n",
    "print(\"Shutting down kernel to release GPU memory...\")\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
