{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f361bbe0",
   "metadata": {},
   "source": [
    "# Fast Inference Test: Llama-3.2-1B\n",
    "\n",
    "Tests `fast_inference=True` with vLLM backend on Llama-3.2-1B-Instruct.\n",
    "\n",
    "**Important:** This notebook includes a kernel shutdown cell at the end.\n",
    "vLLM does not release GPU memory in single-process mode (Jupyter), so kernel\n",
    "restart is required between different model tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e4bce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: unsloth 2025.12.10, vLLM 0.14.0rc1.dev201+gadcf682fc, NVIDIA GeForce RTX 4080 SUPER"
     ]
    }
   ],
   "source": [
    "# Environment Setup (quiet mode)\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Suppress all verbose output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "os.environ[\"UNSLOTH_IS_PRESENT\"] = \"1\"  # May reduce some banners\n",
    "logging.getLogger(\"unsloth\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"vllm\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Suppress unsloth banner during import\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from io import StringIO\n",
    "with redirect_stdout(StringIO()), redirect_stderr(StringIO()):\n",
    "    import unsloth\n",
    "    from unsloth import FastLanguageModel\n",
    "\n",
    "import vllm\n",
    "import torch\n",
    "\n",
    "# Suppress model loading verbosity\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# Single-line environment summary\n",
    "gpu = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"Environment: unsloth {unsloth.__version__}, vLLM {vllm.__version__}, {gpu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c0bc1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTesting Llama-3.2-1B-Instruct with fast_inference=True...",
      "==((====))==  Unsloth 2025.12.10: Fast Llama patching. Transformers: 5.0.0.1. vLLM: 0.14.0rc1.dev201+gadcf682fc.cu130.\n   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.568 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.9.1+cu130. CUDA: 8.9. CUDA Toolkit: 13.0. Triton: 3.5.1\n\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!",
      "Unsloth: vLLM loading unsloth/llama-3.2-1b-instruct-unsloth-bnb-4bit with actual GPU utilization = 45.07%\nUnsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 15.57 GB.\nUnsloth: Using conservativeness = 1.0. Chunked prefill tokens = 512. Num Sequences = 32.\nUnsloth: vLLM's KV Cache can use up to 5.88 GB. Also swap space = 6 GB.\nUnsloth: Not an error, but `use_cudagraph` is not supported in vLLM.config.CompilationConfig. Skipping.\nUnsloth: Not an error, but `use_inductor` is not supported in vLLM.config.CompilationConfig. Skipping.\nUnsloth: Not an error, but `device` is not supported in vLLM. Skipping.",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.1.mlp'], 'llm_int8_threshold': 6.0}"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab32e5e3c6344fda5378918017fe717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e69c8bc577942548cab1dc0b5342505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/22 [00:00<?, ?it/s]",
      "\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 2/22 [00:00<00:01, 19.59it/s]",
      "\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 5/22 [00:00<00:00, 23.74it/s]",
      "\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 9/22 [00:00<00:00, 27.17it/s]",
      "\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 13/22 [00:00<00:00, 30.30it/s]",
      "\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 17/22 [00:00<00:00, 32.44it/s]",
      "\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▌| 21/22 [00:00<00:00, 33.59it/s]",
      "\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 22/22 [00:00<00:00, 31.52it/s]",
      "",
      "\rCapturing CUDA graphs (decode, FULL):   0%|          | 0/14 [00:00<?, ?it/s]",
      "\rCapturing CUDA graphs (decode, FULL):  29%|██▊       | 4/14 [00:00<00:00, 36.78it/s]",
      "\rCapturing CUDA graphs (decode, FULL):  64%|██████▍   | 9/14 [00:00<00:00, 37.70it/s]",
      "\rCapturing CUDA graphs (decode, FULL): 100%|██████████| 14/14 [00:00<00:00, 41.00it/s]",
      "\rCapturing CUDA graphs (decode, FULL): 100%|██████████| 14/14 [00:00<00:00, 39.97it/s]",
      ""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Just some info: will skip parsing ['post_layernorm', 'pre_feedforward_layernorm', 'post_attention_layernorm', 'norm1', 'post_feedforward_layernorm', 'norm2', 'ffn_norm', 'input_layernorm', 'q_norm', 'layer_norm2', 'norm', 'layer_norm1', 'k_norm', 'attention_norm']",
      "Performing substitution for additional_keys=set()\nUnsloth: Just some info: will skip parsing ['post_layernorm', 'pre_feedforward_layernorm', 'post_attention_layernorm', 'norm1', 'post_feedforward_layernorm', 'norm2', 'ffn_norm', 'input_layernorm', 'q_norm', 'layer_norm2', 'cross_attn_post_attention_layernorm', 'norm', 'cross_attn_input_layernorm', 'layer_norm1', 'k_norm', 'attention_norm']"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483b2a97b2ed4e8b832924844a53f2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd2e248489a486da6b7c8b81f4f4c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n============================================================\nModel: unsloth/Llama-3.2-1B-Instruct\nFastInference: ✅ SUPPORTED\nGeneration: 0.02s\n============================================================"
     ]
    }
   ],
   "source": [
    "# Test Llama-3.2-1B with fast_inference=True\n",
    "MODEL_NAME = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "print(f\"\\nTesting {MODEL_NAME.split('/')[-1]} with fast_inference=True...\")\n",
    "\n",
    "from vllm import SamplingParams\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Suppress verbose model loading output by redirecting to /dev/null\n",
    "_stdout_fd = os.dup(1)\n",
    "_stderr_fd = os.dup(2)\n",
    "_devnull = os.open(os.devnull, os.O_WRONLY)\n",
    "os.dup2(_devnull, 1)\n",
    "os.dup2(_devnull, 2)\n",
    "\n",
    "try:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        max_seq_length=512,\n",
    "        load_in_4bit=True,\n",
    "        fast_inference=True,\n",
    "        gpu_memory_utilization=0.5,\n",
    "    )\n",
    "finally:\n",
    "    os.dup2(_stdout_fd, 1)\n",
    "    os.dup2(_stderr_fd, 2)\n",
    "    os.close(_devnull)\n",
    "    os.close(_stdout_fd)\n",
    "    os.close(_stderr_fd)\n",
    "\n",
    "# Test generation\n",
    "FastLanguageModel.for_inference(model)\n",
    "messages = [{\"role\": \"user\", \"content\": \"Say hello in one word.\"}]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "sampling_params = SamplingParams(temperature=0.1, max_tokens=10)\n",
    "\n",
    "start = time.time()\n",
    "outputs = model.fast_generate([prompt], sampling_params=sampling_params)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "# Clear result\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"FastInference: ✅ SUPPORTED\")\n",
    "print(f\"Generation: {elapsed:.2f}s\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb4cc51",
   "metadata": {},
   "source": [
    "## Test Complete\n",
    "\n",
    "The Llama-3.2-1B fast_inference test has completed. The kernel will now shut down to release all GPU memory.\n",
    "\n",
    "**Next:** Run `02_FastInference_Qwen.ipynb` for Qwen3-4B testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb5e44ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down kernel to release GPU memory..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shutdown kernel to release all GPU memory\n",
    "import IPython\n",
    "print(\"Shutting down kernel to release GPU memory...\")\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
