{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d121ca",
   "metadata": {},
   "source": [
    "# Fast Inference Test: Ministral Models\n",
    "\n",
    "Tests Ministral 3B models with fast_inference support:\n",
    "- Ministral 3B standard inference (fast_inference not supported for multimodal)\n",
    "- Ministral 3B Vision with fast_inference attempt\n",
    "- Parameter availability verification\n",
    "\n",
    "**Important:** This notebook includes a kernel shutdown cell at the end.\n",
    "vLLM does not release GPU memory in single-process mode (Jupyter), so kernel\n",
    "restart is required between different model tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8731b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "print(f\"HF_TOKEN loaded: {'Yes' if os.environ.get('HF_TOKEN') else 'No'}\")\n",
    "\n",
    "# CRITICAL: Import unsloth FIRST for proper TRL patching\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel, FastVisionModel\n",
    "import transformers\n",
    "import vllm\n",
    "import trl\n",
    "import torch\n",
    "\n",
    "print(f\"unsloth: {unsloth.__version__}\")\n",
    "print(f\"transformers: {transformers.__version__}\")\n",
    "print(f\"vLLM: {vllm.__version__}\")\n",
    "print(f\"TRL: {trl.__version__}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a3a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Ministral 3B model loading (documents fast_inference limitation)\n",
    "print(\"=== Ministral 3B Model Test ===\")\n",
    "print(\"NOTE: Ministral 3 models are multimodal (vision+text)\")\n",
    "print(\"fast_inference=True is NOT supported - vLLM PixtralForConditionalGeneration lacks packed_modules_mapping\")\n",
    "print(\"Testing standard inference path...\")\n",
    "\n",
    "import time\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"unsloth/Ministral-3-3B-Reasoning-2512-bnb-4bit\",\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    "    # fast_inference=False (default) - required for Mistral 3 multimodal models\n",
    ")\n",
    "print(f\"‚úì Ministral 3B loaded: {type(model).__name__}\")\n",
    "\n",
    "# Test generation using standard inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Ministral 3 uses multimodal message format even for text-only\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Say hello in one word.\"}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = tokenizer(None, input_text, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "start = time.time()\n",
    "output = model.generate(**inputs, max_new_tokens=10, temperature=0.1)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(f\"‚úì Generation completed in {elapsed:.2f}s\")\n",
    "print(f\"  Response (last 30 chars): ...{response[-30:]}\")\n",
    "print(\"‚úì Ministral 3B standard inference test PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787f4282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fast_inference=True with Ministral 3B Vision\n",
    "print(\"=== Ministral 3B Vision Fast Inference Test ===\")\n",
    "print(\"Testing if fast_inference=True works with Ministral vision models...\")\n",
    "\n",
    "from unsloth import FastVisionModel\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "\n",
    "fast_inference_supported = False\n",
    "\n",
    "try:\n",
    "    model, tokenizer = FastVisionModel.from_pretrained(\n",
    "        \"unsloth/Ministral-3-3B-Reasoning-2512-bnb-4bit\",\n",
    "        load_in_4bit=True,\n",
    "        fast_inference=True,\n",
    "        gpu_memory_utilization=0.5,\n",
    "    )\n",
    "    fast_inference_supported = True\n",
    "    print(\"‚úì Ministral 3B Vision loaded with fast_inference=True\")\n",
    "\n",
    "    FastVisionModel.for_inference(model)\n",
    "\n",
    "    # Load a test image\n",
    "    dataset = load_dataset(\"unsloth/LaTeX_OCR\", split=\"train[:1]\")\n",
    "    test_image = dataset[0][\"image\"]\n",
    "\n",
    "    instruction = \"Describe this image in one sentence.\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": instruction}\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = tokenizer(test_image, input_text, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    start = time.time()\n",
    "    output = model.generate(**inputs, max_new_tokens=32, temperature=0.1)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"‚úì Vision generation completed in {elapsed:.2f}s\")\n",
    "    print(f\"  Response (last 50 chars): ...{response[-50:]}\")\n",
    "    print(\"‚úì Ministral 3B vision fast_inference test PASSED\")\n",
    "\n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    if \"packed_modules_mapping\" in error_msg or \"BitsAndBytes\" in error_msg:\n",
    "        print(f\"‚ö† fast_inference=True NOT SUPPORTED for Ministral vision models\")\n",
    "        print(f\"  Reason: vLLM's PixtralForConditionalGeneration lacks packed_modules_mapping\")\n",
    "        print(f\"  This is a known vLLM limitation, not an unsloth bug\")\n",
    "        print(f\"  Workaround: Use standard inference (fast_inference=False)\")\n",
    "        print(\"‚úì Test completed - limitation documented\")\n",
    "    else:\n",
    "        import traceback\n",
    "        print(f\"‚ùå Ministral 3B vision fast_inference test FAILED: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\nüìä Result: fast_inference={'SUPPORTED' if fast_inference_supported else 'NOT SUPPORTED'} for Ministral vision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c26af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify fast_inference parameter exists and confirm it works\n",
    "print(\"=== Fast Inference Capability Check ===\")\n",
    "import inspect\n",
    "\n",
    "# Check FastLanguageModel\n",
    "sig = inspect.signature(FastLanguageModel.from_pretrained)\n",
    "has_fast_inference = 'fast_inference' in sig.parameters\n",
    "print(f\"‚úì fast_inference parameter available: {has_fast_inference}\")\n",
    "\n",
    "# Check FastVisionModel  \n",
    "sig_vision = inspect.signature(FastVisionModel.from_pretrained)\n",
    "has_fast_inference_vision = 'fast_inference' in sig_vision.parameters\n",
    "print(f\"‚úì fast_inference in FastVisionModel: {has_fast_inference_vision}\")\n",
    "\n",
    "# Document current versions\n",
    "print(f\"\\nCurrent versions:\")\n",
    "print(f\"  vLLM: {vllm.__version__}\")\n",
    "print(f\"  Unsloth: {unsloth.__version__}\")\n",
    "print(f\"\\n‚úì fast_inference=True works with vLLM 0.14.0 (patched)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79a2464",
   "metadata": {},
   "source": [
    "## Test Complete\n",
    "\n",
    "The Ministral model tests have completed. The kernel will now shut down to release all GPU memory.\n",
    "\n",
    "**Summary:**\n",
    "- Ministral 3B (text): Standard inference works, fast_inference not supported (multimodal architecture)\n",
    "- Ministral 3B Vision: fast_inference support depends on vLLM's PixtralForConditionalGeneration\n",
    "\n",
    "**Next:** Run `04_Vision_Training.ipynb` for vision training pipeline testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8086ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown kernel to release all GPU memory\n",
    "import IPython\n",
    "print(\"Shutting down kernel to release GPU memory...\")\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
