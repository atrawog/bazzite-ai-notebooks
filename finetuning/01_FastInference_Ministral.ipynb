{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d121ca",
   "metadata": {},
   "source": [
    "# Fast Inference Test: Ministral Models\n",
    "\n",
    "Tests Ministral 3B models with fast_inference support:\n",
    "- Ministral 3B standard inference (fast_inference not supported for multimodal)\n",
    "- Ministral 3B Vision with fast_inference attempt\n",
    "- Parameter availability verification\n",
    "\n",
    "**Important:** This notebook includes a kernel shutdown cell at the end.\n",
    "vLLM does not release GPU memory in single-process mode (Jupyter), so kernel\n",
    "restart is required between different model tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b8731b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN loaded: Yes",
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/pixi/.pixi/envs/default/lib/python3.13/site-packages/trl/__init__.py:203: UserWarning: TRL currently supports vLLM versions: 0.10.2, 0.11.0, 0.11.1, 0.11.2. You have version 0.14.0rc1.dev201+gadcf682fc.cu130 installed. We recommend installing a supported version to avoid compatibility issues.\n  if is_vllm_available():"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!",
      "unsloth: 2025.12.10\ntransformers: 5.0.0rc1\nvLLM: 0.14.0rc1.dev201+gadcf682fc\nTRL: 0.26.2\nPyTorch: 2.9.1+cu130\nCUDA: True\nGPU: NVIDIA GeForce RTX 4080 SUPER"
     ]
    }
   ],
   "source": [
    "# Environment Setup (quiet mode)\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Suppress all verbose output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "logging.getLogger(\"unsloth\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"vllm\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Suppress unsloth banner during import\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from io import StringIO\n",
    "with redirect_stdout(StringIO()), redirect_stderr(StringIO()):\n",
    "    import unsloth\n",
    "    from unsloth import FastLanguageModel, FastVisionModel\n",
    "\n",
    "import vllm\n",
    "import torch\n",
    "\n",
    "# Suppress model loading verbosity\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# Single-line environment summary\n",
    "gpu = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"Environment: unsloth {unsloth.__version__}, vLLM {vllm.__version__}, {gpu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430a3a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ministral 3B Model Test ===\nNOTE: Ministral 3 models are multimodal (vision+text)\nfast_inference=True is NOT supported - vLLM PixtralForConditionalGeneration lacks packed_modules_mapping\nTesting standard inference path...",
      "==((====))==  Unsloth 2025.12.10: Fast Ministral3 patching. Transformers: 5.0.0rc1. vLLM: 0.14.0rc1.dev201+gadcf682fc.cu130.\n   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.568 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.9.1+cu130. CUDA: 8.9. CUDA Toolkit: 13.0. Triton: 3.5.1\n\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c192254a224791b31c84ee77cc0747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/458 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Ministral 3B loaded: Mistral3ForConditionalGeneration",
      "‚úì Generation completed in 1.67s\n  Response (last 30 chars): ...theƒ†userƒ†wantsƒ†meƒ†toƒ†sayƒ†hello\n‚úì Ministral 3B standard inference test PASSED"
     ]
    }
   ],
   "source": [
    "# Test Ministral 3B (multimodal architecture - fast_inference NOT supported)\n",
    "MODEL_NAME = \"unsloth/Ministral-3-3B-Reasoning-2512\"\n",
    "print(f\"\\nTesting {MODEL_NAME.split('/')[-1]} (multimodal architecture)...\")\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Suppress verbose model loading output by redirecting to /dev/null\n",
    "_stdout_fd = os.dup(1)\n",
    "_stderr_fd = os.dup(2)\n",
    "_devnull = os.open(os.devnull, os.O_WRONLY)\n",
    "os.dup2(_devnull, 1)\n",
    "os.dup2(_devnull, 2)\n",
    "\n",
    "try:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        max_seq_length=512,\n",
    "        load_in_4bit=True,\n",
    "        # fast_inference=False (default) - multimodal models don't support vLLM backend\n",
    "    )\n",
    "finally:\n",
    "    os.dup2(_stdout_fd, 1)\n",
    "    os.dup2(_stderr_fd, 2)\n",
    "    os.close(_devnull)\n",
    "    os.close(_stdout_fd)\n",
    "    os.close(_stderr_fd)\n",
    "\n",
    "# Test standard generation\n",
    "FastLanguageModel.for_inference(model)\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Say hello in one word.\"}]}]\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = tokenizer(None, input_text, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "start = time.time()\n",
    "output = model.generate(**inputs, max_new_tokens=10, temperature=0.1)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "# Clear result\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"FastInference: ‚ùå NOT SUPPORTED (multimodal architecture)\")\n",
    "print(f\"Reason: vLLM PixtralForConditionalGeneration lacks packed_modules_mapping\")\n",
    "print(f\"Standard inference: {elapsed:.2f}s\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "787f4282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ministral 3B Vision Fast Inference Test ===\nTesting if fast_inference=True works with Ministral vision models...",
      "==((====))==  Unsloth 2025.12.10: Fast Ministral3 patching. Transformers: 5.0.0rc1. vLLM: 0.14.0rc1.dev201+gadcf682fc.cu130.\n   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.568 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.9.1+cu130. CUDA: 8.9. CUDA Toolkit: 13.0. Triton: 3.5.1\n\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!",
      "WARNING 01-02 23:15:55 [vllm.py:1427] Current vLLM config is not set.",
      "INFO 01-02 23:15:55 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=2048.",
      "INFO 01-02 23:15:55 [vllm.py:609] Disabling NCCL for DP synchronization when using async scheduling.",
      "INFO 01-02 23:15:55 [vllm.py:614] Asynchronous scheduling is enabled.",
      "INFO 01-02 23:15:55 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture",
      "Unsloth: Your GPU cannot handle sequence lengths of 256 due to limited GPU memory.\nUnsloth: Your GPU can only handle approximately the maximum sequence length of 256.\nUnsloth: Vision model detected, setting approx_max_num_seqs to 1\nUnsloth: vLLM loading unsloth/Ministral-3-3B-Reasoning-2512 with actual GPU utilization = 15.16%\nUnsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 15.57 GB.\nUnsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 1.\nUnsloth: vLLM's KV Cache can use up to 0.0 GB. Also swap space = 6 GB.\nUnsloth: Not an error, but `use_cudagraph` is not supported in vLLM.config.CompilationConfig. Skipping.\nUnsloth: Not an error, but `use_inductor` is not supported in vLLM.config.CompilationConfig. Skipping.\nWARNING 01-02 23:15:55 [compilation.py:739] Level is deprecated and will be removed in the next release,either 0.12.0 or 0.11.2 whichever is soonest.Use mode instead.If both level and mode are given,only mode will be used.",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\nWARNING 01-02 23:15:55 [attention.py:82] Using VLLM_ATTENTION_BACKEND environment variable is deprecated and will be removed in v0.14.0 or v1.0.0, whichever is soonest. Please use --attention-config.backend command line argument or AttentionConfig(backend=...) config field instead.",
      "INFO 01-02 23:15:55 [utils.py:253] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'max_model_len': 256, 'enable_prefix_caching': True, 'swap_space': 6, 'gpu_memory_utilization': 0.15157081302757403, 'max_num_batched_tokens': 8192, 'max_num_seqs': 1, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'limit_mm_per_prompt': {'image': 1, 'video': 0}, 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {'level': 3, 'mode': 3, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'epilogue_fusion': True, 'max_autotune': False, 'shape_padding': True, 'trace.enabled': False, 'triton.cudagraphs': False, 'debug': False, 'dce': True, 'memory_planning': True, 'coordinate_descent_tuning': False, 'trace.graph_diagram': False, 'compile_threads': 16, 'group_fusion': True, 'disable_progress': False, 'verbose_progress': True, 'triton.multi_kernel': 0, 'triton.use_block_ptr': True, 'triton.enable_persistent_tma_matmul': True, 'triton.autotune_at_compile_time': False, 'triton.cooperative_reductions': False, 'cuda.compile_opt_level': '-O2', 'cuda.enable_cuda_lto': True, 'combo_kernels': False, 'benchmark_combo_kernel': True, 'combo_kernel_foreach_dynamic_shapes': True, 'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}, 'model': 'unsloth/Ministral-3-3B-Reasoning-2512'}",
      "WARNING 01-02 23:15:56 [arg_utils.py:1196] The global random seed is set to 0. Since VLLM_ENABLE_V1_MULTIPROCESSING is set to False, this may affect the random state of the Python process that launched vLLM."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/pixi/.pixi/envs/default/lib/python3.13/site-packages/pydantic/type_adapter.py:605: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `enum` - serialized value may not be as expected [field_name='mode', input_value=3, input_type=int])\n  return self.serializer.to_python(",
      "Unrecognized keys in `rope_parameters` for 'rope_type'='yarn': {'apply_yarn_scaling'}",
      "`rope_parameters`'s factor field must be a float >= 1, got 16",
      "`rope_parameters`'s beta_fast field must be a float, got 32",
      "`rope_parameters`'s beta_slow field must be a float, got 1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-02 23:15:56 [model.py:517] Resolved architecture: PixtralForConditionalGeneration",
      "INFO 01-02 23:15:56 [model.py:1688] Using max model len 256",
      "INFO 01-02 23:15:56 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=8192.",
      "WARNING 01-02 23:15:56 [scheduler.py:273] max_num_batched_tokens (8192) exceeds max_num_seqs * max_model_len (256). This may lead to unexpected behavior.",
      "WARNING 01-02 23:15:56 [vllm.py:1427] Current vLLM config is not set.",
      "INFO 01-02 23:15:56 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=2048.",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'fp4', 'bnb_4bit_use_double_quant': False, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': [], 'llm_int8_threshold': 6.0}",
      "INFO 01-02 23:15:57 [core.py:95] Initializing a V1 LLM engine (v0.14.0rc1.dev201+gadcf682fc) with config: model='unsloth/Ministral-3-3B-Reasoning-2512', speculative_config=None, tokenizer='unsloth/Ministral-3-3B-Reasoning-2512', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False), seed=0, served_model_name=unsloth/Ministral-3-3B-Reasoning-2512, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': 3, 'mode': 3, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'epilogue_fusion': True, 'max_autotune': False, 'shape_padding': True, 'trace.enabled': False, 'triton.cudagraphs': False, 'debug': False, 'dce': True, 'memory_planning': True, 'coordinate_descent_tuning': False, 'trace.graph_diagram': False, 'compile_threads': 16, 'group_fusion': True, 'disable_progress': False, 'verbose_progress': True, 'triton.multi_kernel': 0, 'triton.use_block_ptr': True, 'triton.enable_persistent_tma_matmul': True, 'triton.autotune_at_compile_time': False, 'triton.cooperative_reductions': False, 'cuda.compile_opt_level': '-O2', 'cuda.enable_cuda_lto': True, 'combo_kernels': False, 'benchmark_combo_kernel': True, 'combo_kernel_foreach_dynamic_shapes': True, 'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}",
      "INFO 01-02 23:15:57 [parallel_state.py:1210] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.89.0.16:46177 backend=nccl",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0",
      "INFO 01-02 23:15:57 [parallel_state.py:1418] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/pixi/.pixi/envs/default/lib/python3.13/site-packages/pydantic/type_adapter.py:605: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `enum` - serialized value may not be as expected [field_name='mode', input_value=3, input_type=int])\n  return self.serializer.to_python("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-02 23:15:58 [topk_topp_sampler.py:47] Using FlashInfer for top-p & top-k sampling.",
      "INFO 01-02 23:15:58 [gpu_model_runner.py:3762] Starting to load model unsloth/Ministral-3-3B-Reasoning-2512...",
      "INFO 01-02 23:15:59 [cuda.py:315] Using AttentionBackendEnum.FLASHINFER backend.",
      "‚ö† fast_inference=True NOT SUPPORTED for Ministral vision models\n  Reason: vLLM's PixtralForConditionalGeneration lacks packed_modules_mapping\n  This is a known vLLM limitation, not an unsloth bug\n  Workaround: Use standard inference (fast_inference=False)\n‚úì Test completed - limitation documented\n\nüìä Result: fast_inference=NOT SUPPORTED for Ministral vision"
     ]
    }
   ],
   "source": [
    "# Test FastVisionModel with fast_inference=True (expected to fail)\n",
    "MODEL_NAME = \"unsloth/Ministral-3-3B-Reasoning-2512\"\n",
    "print(f\"\\nTesting FastVisionModel with fast_inference=True...\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import os\n",
    "\n",
    "fast_inference_supported = False\n",
    "elapsed = None\n",
    "\n",
    "# Suppress verbose model loading output by redirecting to /dev/null\n",
    "_stdout_fd = os.dup(1)\n",
    "_stderr_fd = os.dup(2)\n",
    "_devnull = os.open(os.devnull, os.O_WRONLY)\n",
    "os.dup2(_devnull, 1)\n",
    "os.dup2(_devnull, 2)\n",
    "\n",
    "try:\n",
    "    model, tokenizer = FastVisionModel.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        load_in_4bit=True,\n",
    "        fast_inference=True,\n",
    "        gpu_memory_utilization=0.5,\n",
    "    )\n",
    "    fast_inference_supported = True\n",
    "    \n",
    "    # If it loads, test generation\n",
    "    FastVisionModel.for_inference(model)\n",
    "    dataset = load_dataset(\"unsloth/LaTeX_OCR\", split=\"train[:1]\")\n",
    "    test_image = dataset[0][\"image\"]\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"Describe briefly.\"}]}]\n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = tokenizer(test_image, input_text, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    start = time.time()\n",
    "    output = model.generate(**inputs, max_new_tokens=32, temperature=0.1)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "except Exception as e:\n",
    "    pass  # Expected - fast_inference not supported\n",
    "\n",
    "finally:\n",
    "    os.dup2(_stdout_fd, 1)\n",
    "    os.dup2(_stderr_fd, 2)\n",
    "    os.close(_devnull)\n",
    "    os.close(_stdout_fd)\n",
    "    os.close(_stderr_fd)\n",
    "\n",
    "# Clear result\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Model: {MODEL_NAME} (Vision)\")\n",
    "if fast_inference_supported:\n",
    "    print(f\"FastInference: ‚úÖ SUPPORTED\")\n",
    "    if elapsed:\n",
    "        print(f\"Generation: {elapsed:.2f}s\")\n",
    "else:\n",
    "    print(f\"FastInference: ‚ùå NOT SUPPORTED\")\n",
    "    print(f\"Reason: vLLM vision models lack packed_modules_mapping\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea0c26af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fast Inference Capability Check ===\n‚úì fast_inference parameter available: True\n‚úì fast_inference in FastVisionModel: True\n\nCurrent versions:\n  vLLM: 0.14.0rc1.dev201+gadcf682fc\n  Unsloth: 2025.12.10\n\n‚úì fast_inference=True works with vLLM 0.14.0 (patched)"
     ]
    }
   ],
   "source": [
    "# Summary: FastInference support by model type\n",
    "import inspect\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FASTINFERENCE SUPPORT SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Llama, Qwen (text-only):     ‚úÖ SUPPORTED\")\n",
    "print(f\"Ministral (multimodal):      ‚ùå NOT SUPPORTED\")\n",
    "print(f\"Vision models:               ‚ùå NOT SUPPORTED\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nNote: Multimodal models use PixtralForConditionalGeneration\")\n",
    "print(f\"which lacks vLLM's packed_modules_mapping for fast inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79a2464",
   "metadata": {},
   "source": [
    "## Test Complete\n",
    "\n",
    "The Ministral model tests have completed. The kernel will now shut down to release all GPU memory.\n",
    "\n",
    "**Summary:**\n",
    "- Ministral 3B (text): Standard inference works, fast_inference not supported (multimodal architecture)\n",
    "- Ministral 3B Vision: fast_inference support depends on vLLM's PixtralForConditionalGeneration\n",
    "\n",
    "**Next:** Run `04_Vision_Training.ipynb` for vision training pipeline testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8086ce30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down kernel to release GPU memory..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shutdown kernel to release all GPU memory\n",
    "import IPython\n",
    "print(\"Shutting down kernel to release GPU memory...\")\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
