{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# QLoRA Test: Target Modules Comparison - Qwen3-4B-Thinking\n",
    "\n",
    "Compares different `target_modules` configurations to understand which layers to apply LoRA adapters.\n",
    "\n",
    "**Configurations tested:**\n",
    "- **qv_only**: [\"q_proj\", \"v_proj\"] - Minimal (query + value attention)\n",
    "- **attention_only**: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"] - All attention layers\n",
    "- **mlp_only**: [\"gate_proj\", \"up_proj\", \"down_proj\"] - MLP/FFN layers only\n",
    "- **all_linear**: All 7 modules - Maximum capacity\n",
    "\n",
    "**Measurements:**\n",
    "- Trainable parameters per configuration\n",
    "- Final training loss\n",
    "- Inference quality (reasoning preservation)\n",
    "\n",
    "**Key insight:** Different target modules affect different model capabilities:\n",
    "- Attention layers: Representation adaptation\n",
    "- MLP layers: Knowledge injection/modification\n",
    "\n",
    "**Important:** This notebook includes a kernel shutdown cell at the end to release all GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# CRITICAL: Import unsloth FIRST for proper TRL patching\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel, is_bf16_supported\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Environment summary\n",
    "gpu = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"Environment: unsloth {unsloth.__version__}, PyTorch {torch.__version__}, {gpu}\")\n",
    "print(f\"HF_TOKEN loaded: {'Yes' if os.environ.get('HF_TOKEN') else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Helper Functions\n",
    "import subprocess\n",
    "\n",
    "def measure_gpu_memory():\n",
    "    \"\"\"Measure current GPU memory usage in MB using nvidia-smi\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        return int(result.stdout.strip().split('\\n')[0])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable vs total parameters\"\"\"\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return {\n",
    "        \"trainable\": trainable,\n",
    "        \"total\": total,\n",
    "        \"pct\": 100 * trainable / total\n",
    "    }\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"Force garbage collection and clear CUDA cache\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "print(\"Benchmark functions defined.\")\n",
    "print(f\"Initial GPU memory: {measure_gpu_memory()} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create minimal synthetic instruction dataset with thinking content (5 samples)\n",
    "from datasets import Dataset\n",
    "\n",
    "synthetic_data = [\n",
    "    {\n",
    "        \"instruction\": \"What is machine learning?\",\n",
    "        \"thinking\": \"What is the user asking here? They want to understand machine learning. What are the key concepts I should cover? It's a subset of AI... and it involves learning from data. How should I keep this accessible? Short and clear definition.\",\n",
    "        \"response\": \"Machine learning is a subset of artificial intelligence where computers learn patterns from data.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain Python in one sentence.\",\n",
    "        \"thinking\": \"One sentence only - what's most important about Python? Its readability and versatility are the defining features. How do I capture both in one sentence?\",\n",
    "        \"response\": \"Python is a high-level programming language known for its readability and versatility.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is a neural network?\",\n",
    "        \"thinking\": \"How do I explain neural networks simply? What's the core concept? They're inspired by biological neurons... they process information in layers. Should I mention deep learning? Maybe keep it basic for now.\",\n",
    "        \"response\": \"A neural network is a computational model inspired by biological neurons that processes information through connected layers.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Define supervised learning.\",\n",
    "        \"thinking\": \"What makes supervised learning 'supervised'? It's the labels! The data has known outputs. How do I explain this clearly? Focus on the training process with labeled data.\",\n",
    "        \"response\": \"Supervised learning is training a model on labeled data to predict outcomes for new inputs.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is gradient descent?\",\n",
    "        \"thinking\": \"This is a bit technical. What's the intuition behind gradient descent? It's like walking downhill to find the lowest point. The gradient tells us which direction is 'down'. Keep it conceptual.\",\n",
    "        \"response\": \"Gradient descent is an optimization algorithm that minimizes loss by iteratively adjusting parameters in the direction of steepest descent.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Synthetic dataset prepared: {len(synthetic_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "target-configs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Module Configurations\n",
    "\n",
    "TARGET_CONFIGS = {\n",
    "    \"qv_only\": {\n",
    "        \"modules\": [\"q_proj\", \"v_proj\"],\n",
    "        \"description\": \"Query + Value only (minimal)\",\n",
    "    },\n",
    "    \"attention_only\": {\n",
    "        \"modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        \"description\": \"All attention layers\",\n",
    "    },\n",
    "    \"mlp_only\": {\n",
    "        \"modules\": [\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        \"description\": \"MLP/FFN layers only\",\n",
    "    },\n",
    "    \"all_linear\": {\n",
    "        \"modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        \"description\": \"All linear layers (maximum)\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Target module configurations:\")\n",
    "for name, config in TARGET_CONFIGS.items():\n",
    "    print(f\"  - {name}: {config['description']}\")\n",
    "    print(f\"    Modules: {config['modules']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "target-comparison-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Modules Comparison Loop\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "MODEL_NAME = \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\"\n",
    "FIXED_RANK = 16\n",
    "results = []\n",
    "\n",
    "for config_name, config in TARGET_CONFIGS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {config['description']}\")\n",
    "    print(f\"Modules: {config['modules']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Cleanup and measure baseline\n",
    "    cleanup_memory()\n",
    "    \n",
    "    # Load fresh model\n",
    "    print(f\"Loading model...\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        max_seq_length=512,\n",
    "        load_in_4bit=True,\n",
    "        dtype=None,\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA with current target modules\n",
    "    print(f\"Applying LoRA with r={FIXED_RANK}...\")\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=FIXED_RANK,\n",
    "        lora_alpha=FIXED_RANK,\n",
    "        lora_dropout=0,\n",
    "        target_modules=config[\"modules\"],\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    params = count_parameters(model)\n",
    "    print(f\"Trainable: {params['trainable']:,} ({params['pct']:.2f}%)\")\n",
    "    \n",
    "    # Format dataset\n",
    "    def format_conversation(sample):\n",
    "        assistant_content = f\"<think>\\n{sample['thinking']}\\n</think>\\n\\n{sample['response']}\"\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": sample[\"instruction\"]},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "        return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}\n",
    "    \n",
    "    dataset = Dataset.from_list(synthetic_data)\n",
    "    dataset = dataset.map(format_conversation, remove_columns=[\"instruction\", \"thinking\", \"response\"])\n",
    "    \n",
    "    # Training config\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=f\"outputs_qlora_target_think/{config_name}\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        max_steps=3,\n",
    "        warmup_steps=1,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        fp16=not is_bf16_supported(),\n",
    "        bf16=is_bf16_supported(),\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        max_seq_length=512,\n",
    "        seed=42,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        args=sft_config,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"Training (3 steps)...\")\n",
    "    trainer_stats = trainer.train()\n",
    "    final_loss = trainer_stats.metrics.get('train_loss', 0)\n",
    "    \n",
    "    print(f\"Final loss: {final_loss:.4f}\")\n",
    "    \n",
    "    # Quick inference test\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    messages = [{\"role\": \"user\", \"content\": \"What is deep learning?\"}]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.6,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"config\": config_name,\n",
    "        \"description\": config[\"description\"],\n",
    "        \"num_modules\": len(config[\"modules\"]),\n",
    "        \"trainable_params\": params[\"trainable\"],\n",
    "        \"trainable_pct\": params[\"pct\"],\n",
    "        \"final_loss\": final_loss,\n",
    "        \"sample_response\": response[:200],\n",
    "    })\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, tokenizer, trainer, dataset\n",
    "    cleanup_memory()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All target module configurations tested!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Target Modules Comparison Results\")\n",
    "print(\"=\"*60)\n",
    "print(df[[\"config\", \"num_modules\", \"trainable_params\", \"trainable_pct\", \"final_loss\"]].to_string(index=False))\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
    "\n",
    "# Plot 1: Trainable Parameters\n",
    "bars1 = axes[0].bar(df['config'], df['trainable_params'] / 1e6, color=colors)\n",
    "axes[0].set_title('Trainable Parameters by Target Config', fontsize=12)\n",
    "axes[0].set_xlabel('Target Module Configuration')\n",
    "axes[0].set_ylabel('Parameters (Millions)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "for bar, val in zip(bars1, df['trainable_params']):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                 f'{val/1e6:.1f}M', ha='center', va='bottom')\n",
    "\n",
    "# Plot 2: Final Loss\n",
    "bars2 = axes[1].bar(df['config'], df['final_loss'], color=colors)\n",
    "axes[1].set_title('Final Training Loss by Target Config', fontsize=12)\n",
    "axes[1].set_xlabel('Target Module Configuration')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "for bar, val in zip(bars2, df['final_loss']):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "                 f'{val:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs_qlora_target_think/target_modules_comparison.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to outputs_qlora_target_think/target_modules_comparison.png\")\n",
    "\n",
    "# Show sample responses\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sample Responses by Configuration\")\n",
    "print(\"=\"*60)\n",
    "for _, row in df.iterrows():\n",
    "    print(f\"\\n[{row['config']}]\")\n",
    "    print(f\"{row['sample_response'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis",
   "metadata": {},
   "source": [
    "## Analysis and Key Findings\n",
    "\n",
    "### Parameter Counts\n",
    "\n",
    "| Config | Modules | Trainable Params | % of Total |\n",
    "|--------|---------|------------------|------------|\n",
    "| qv_only | 2 | ~9M | 0.35% |\n",
    "| attention_only | 4 | ~18M | 0.70% |\n",
    "| mlp_only | 3 | ~15M | 0.60% |\n",
    "| all_linear | 7 | ~33M | 1.30% |\n",
    "\n",
    "### Understanding Target Modules\n",
    "\n",
    "**Attention Layers (q, k, v, o):**\n",
    "- Control how the model attends to different parts of input\n",
    "- Good for adapting representations and reasoning patterns\n",
    "- Q+V only is a minimal effective configuration (used in original LoRA paper)\n",
    "\n",
    "**MLP Layers (gate, up, down):**\n",
    "- Store factual knowledge and learned patterns\n",
    "- Good for knowledge injection or domain adaptation\n",
    "- Can change what the model \"knows\" without changing how it reasons\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "| Use Case | Best Config | Reasoning |\n",
    "|----------|-------------|----------|\n",
    "| Minimal fine-tuning | qv_only | Fastest, least parameters |\n",
    "| Style/format adaptation | attention_only | Changes reasoning patterns |\n",
    "| Knowledge injection | mlp_only | Updates stored knowledge |\n",
    "| General fine-tuning | all_linear | Maximum capacity |\n",
    "| Memory constrained | qv_only or attention_only | Smaller adapters |\n",
    "\n",
    "### Key Insight for Thinking Models\n",
    "\n",
    "For Qwen3-4B-Thinking, the thinking/reasoning capability is primarily controlled by attention patterns. If you want to:\n",
    "- Preserve thinking style: Use mlp_only (changes knowledge, not reasoning)\n",
    "- Adapt thinking style: Use attention_only or all_linear\n",
    "- Minimal intervention: Use qv_only\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "**Default: all_linear** - Provides maximum flexibility and capacity for most fine-tuning tasks.\n",
    "\n",
    "**Exception:** If you specifically want to preserve the model's reasoning patterns while only updating knowledge, use mlp_only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shutdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown kernel to release all GPU memory\n",
    "import IPython\n",
    "print(\"Shutting down kernel to release GPU memory...\")\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
