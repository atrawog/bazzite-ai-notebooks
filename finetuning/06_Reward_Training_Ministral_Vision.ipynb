{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dacbf28c",
   "metadata": {},
   "source": [
    "# Reward Model Training Test: Ministral (Vision)\n",
    "\n",
    "Tests Reward Model training with vision capabilities on Ministral-3B.\n",
    "\n",
    "**Model Variant:** Vision (FastVisionModel)\n",
    "**Expected Result:** NOT SUPPORTED - AutoModelForSequenceClassification incompatible with vision models\n",
    "\n",
    "**Reward Model + Vision Challenge:**\n",
    "Standard reward models use AutoModelForSequenceClassification which outputs scalar rewards. Vision models (Mistral3ForConditionalGeneration) have different architectures that don't support sequence classification.\n",
    "\n",
    "**Why Not Supported:**\n",
    "1. Vision models output generation logits, not classification scores\n",
    "2. No classification head available for scalar reward output\n",
    "3. Image processing pipeline incompatible with RewardTrainer\n",
    "\n",
    "**Alternatives for Vision RLHF:**\n",
    "1. Use GRPO with custom reward functions (04_GRPO_Training_Ministral_Vision.ipynb) - **WORKS!**\n",
    "2. Use generation loss as reward proxy (shown in this notebook)\n",
    "3. Use a separate text-only reward model alongside vision model\n",
    "\n",
    "**Important:** This notebook documents the limitations and shows custom reward function alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59e80894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "/opt/pixi/.pixi/envs/default/lib/python3.13/site-packages/trl/__init__.py:203: UserWarning: TRL currently supports vLLM versions: 0.10.2, 0.11.0, 0.11.1, 0.11.2. You have version 0.14.0rc1.dev201+gadcf682fc.cu130 installed. We recommend installing a supported version to avoid compatibility issues.\n",
       "  if is_vllm_available():\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Environment: unsloth 2025.12.10, PyTorch 2.9.1+cu130, NVIDIA GeForce RTX 4080 SUPER\n",
       "HF_TOKEN loaded: Yes\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# CRITICAL: Import unsloth FIRST for proper TRL patching\n",
    "import unsloth\n",
    "from unsloth import FastVisionModel, is_bf16_supported\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Environment summary\n",
    "gpu = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"Environment: unsloth {unsloth.__version__}, PyTorch {torch.__version__}, {gpu}\")\n",
    "print(f\"HF_TOKEN loaded: {'Yes' if os.environ.get('HF_TOKEN') else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1df02078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Attempting to load Ministral-3-3B-Reasoning-2512 for reward modeling...\n",
       "Note: Vision models use PixtralForConditionalGeneration, not SequenceClassification\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==((====))==  Unsloth 2025.12.10: Fast Ministral3 patching. Transformers: 5.0.0rc1. vLLM: 0.14.0rc1.dev201+gadcf682fc.cu130.\n",
       "   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.568 GB. Platform: Linux.\n",
       "O^O/ \\_/ \\    Torch: 2.9.1+cu130. CUDA: 8.9. CUDA Toolkit: 13.0. Triton: 3.5.1\n",
       "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
       " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
       "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/458 [00:00<?, ?it/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Model loaded: Mistral3ForConditionalGeneration\n",
       "Model type: Conditional Generation (NOT Sequence Classification)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt to load Vision Model for reward modeling\n",
    "# Note: This is experimental - vision models don't natively support sequence classification\n",
    "MODEL_NAME = \"unsloth/Ministral-3-3B-Reasoning-2512\"\n",
    "print(f\"\\nAttempting to load {MODEL_NAME.split('/')[-1]} for reward modeling...\")\n",
    "print(\"Note: Vision models use PixtralForConditionalGeneration, not SequenceClassification\")\n",
    "\n",
    "VISION_REWARD_SUPPORTED = False\n",
    "\n",
    "try:\n",
    "    # Load vision model (for generation, not classification)\n",
    "    model, tokenizer = FastVisionModel.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        load_in_4bit=True,\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "    )\n",
    "    print(f\"Model loaded: {type(model).__name__}\")\n",
    "    print(\"Model type: Conditional Generation (NOT Sequence Classification)\")\n",
    "    VISION_MODEL_LOADED = True\n",
    "except Exception as e:\n",
    "    print(f\"Model loading failed: {e}\")\n",
    "    VISION_MODEL_LOADED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cf9a10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Custom vision reward function defined\n",
       "Approach: Use generation loss as reward proxy\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternative: Custom reward function using vision model\n",
    "# This demonstrates how to create a reward function for vision content\n",
    "\n",
    "def vision_reward_function(model, tokenizer, image, prompt, response):\n",
    "    \"\"\"\n",
    "    Custom reward function using vision model.\n",
    "    Instead of training a separate reward model, we can use the vision model\n",
    "    to evaluate responses based on generation likelihood.\n",
    "    \"\"\"\n",
    "    # Format the full conversation\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt}]},\n",
    "        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": response}]}\n",
    "    ]\n",
    "    \n",
    "    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    inputs = tokenizer(image, input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move to device with correct dtype\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    # Convert pixel_values to bfloat16 to match model weights\n",
    "    if \"pixel_values\" in inputs:\n",
    "        inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(torch.bfloat16)\n",
    "    \n",
    "    # Compute loss as proxy for reward (lower loss = better response)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss.item()\n",
    "    \n",
    "    # Convert loss to reward (negative loss, normalized)\n",
    "    reward = -loss\n",
    "    return reward\n",
    "\n",
    "if VISION_MODEL_LOADED:\n",
    "    print(\"Custom vision reward function defined\")\n",
    "    print(\"Approach: Use generation loss as reward proxy\")\n",
    "else:\n",
    "    print(\"Skipping - model not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b34f44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Testing custom vision reward function...\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Good response reward: -5.2612\n",
       "Bad response reward:  -5.6585\n",
       "Preference correct:   Yes\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test custom vision reward function\n",
    "if VISION_MODEL_LOADED:\n",
    "    print(\"Testing custom vision reward function...\")\n",
    "    \n",
    "    # Load test image\n",
    "    vision_dataset = load_dataset(\"unsloth/LaTeX_OCR\", split=\"train[:1]\")\n",
    "    test_image = vision_dataset[0][\"image\"]\n",
    "    test_prompt = \"What mathematical expression is shown in this image?\"\n",
    "    \n",
    "    # Compare good vs bad responses\n",
    "    good_response = \"The image shows a mathematical expression in LaTeX notation with variables and operators.\"\n",
    "    bad_response = \"Image.\"\n",
    "    \n",
    "    try:\n",
    "        FastVisionModel.for_inference(model)\n",
    "        good_reward = vision_reward_function(model, tokenizer, test_image, test_prompt, good_response)\n",
    "        bad_reward = vision_reward_function(model, tokenizer, test_image, test_prompt, bad_response)\n",
    "        \n",
    "        print(f\"Good response reward: {good_reward:.4f}\")\n",
    "        print(f\"Bad response reward:  {bad_reward:.4f}\")\n",
    "        print(f\"Preference correct:   {'Yes' if good_reward > bad_reward else 'No'}\")\n",
    "        VISION_REWARD_SUPPORTED = True\n",
    "    except Exception as e:\n",
    "        print(f\"Reward function test failed: {e}\")\n",
    "        VISION_REWARD_SUPPORTED = False\n",
    "else:\n",
    "    print(\"Skipping test - model not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd8bdb28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================\n",
       "VISION REWARD MODEL SUMMARY\n",
       "============================================================\n",
       "\n",
       "Standard RewardTrainer with AutoModelForSequenceClassification:\n",
       "  Status: NOT SUPPORTED\n",
       "  Reason: Vision models use PixtralForConditionalGeneration\n",
       "          which doesn't support sequence classification output\n",
       "\n",
       "Alternative: Custom Reward Function (demonstrated above)\n",
       "  Status: WORKING\n",
       "  Method: Use generation loss as reward proxy\n",
       "\n",
       "Recommended Approach for Vision RLHF:\n",
       "  1. Use SFT for initial vision fine-tuning (03_SFT_Vision)\n",
       "  2. Use custom reward functions with GRPO/RLOO\n",
       "  3. Or use text-only reward model + vision generation model\n",
       "============================================================\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary: Vision Reward Model Support\n",
    "print(\"=\" * 60)\n",
    "print(\"VISION REWARD MODEL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Standard RewardTrainer with AutoModelForSequenceClassification:\")\n",
    "print(\"  Status: NOT SUPPORTED\")\n",
    "print(\"  Reason: Vision models use PixtralForConditionalGeneration\")\n",
    "print(\"          which doesn't support sequence classification output\")\n",
    "print()\n",
    "print(\"Alternative: Custom Reward Function (demonstrated above)\")\n",
    "if VISION_MODEL_LOADED and VISION_REWARD_SUPPORTED:\n",
    "    print(\"  Status: WORKING\")\n",
    "    print(\"  Method: Use generation loss as reward proxy\")\n",
    "else:\n",
    "    print(\"  Status: See test results above\")\n",
    "print()\n",
    "print(\"Recommended Approach for Vision RLHF:\")\n",
    "print(\"  1. Use SFT for initial vision fine-tuning (03_SFT_Vision)\")\n",
    "print(\"  2. Use custom reward functions with GRPO/RLOO\")\n",
    "print(\"  3. Or use text-only reward model + vision generation model\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e63714a",
   "metadata": {},
   "source": [
    "## Test Complete\n",
    "\n",
    "The Reward Model Training Pipeline test for Ministral (Vision) has completed. The kernel will now shut down to release all GPU memory.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "**Standard Reward Training: NOT SUPPORTED**\n",
    "- Vision models (PixtralForConditionalGeneration) don't support sequence classification\n",
    "- AutoModelForSequenceClassification requires text-only architectures\n",
    "\n",
    "**Alternative Approach: Custom Reward Function**\n",
    "- Use vision model's generation loss as reward proxy\n",
    "- Can be integrated with GRPO/RLOO trainers via `reward_funcs` parameter\n",
    "\n",
    "### Comparison with Text-Only\n",
    "| Aspect | Text-Only | Vision |\n",
    "|--------|-----------|--------|\n",
    "| RewardTrainer | SUPPORTED | NOT SUPPORTED |\n",
    "| Custom Reward | Possible | Demonstrated |\n",
    "| Architecture | SequenceClassification | ConditionalGeneration |\n",
    "\n",
    "### Recommended Vision RLHF Pipeline\n",
    "1. SFT training with vision data (03_SFT_Vision)\n",
    "2. Custom reward function (not trained reward model)\n",
    "3. GRPO/RLOO with custom reward_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac5327a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Shutting down kernel to release GPU memory...\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': False}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shutdown kernel to release all GPU memory\n",
    "import IPython\n",
    "print(\"Shutting down kernel to release GPU memory...\")\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
