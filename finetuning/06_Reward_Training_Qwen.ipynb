{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85aa1f8c",
   "metadata": {},
   "source": [
    "# Reward Model Training Test: Qwen3-4B\n",
    "\n",
    "Tests Reward Model training with Unsloth on Qwen3-4B.\n",
    "\n",
    "**Key features tested:**\n",
    "- AutoModelForSequenceClassification loading with 4-bit quantization\n",
    "- LoRA adapter configuration for reward modeling\n",
    "- RewardTrainer with synthetic preference pairs\n",
    "- Post-training reward scoring verification\n",
    "\n",
    "**Reward Model Overview:**\n",
    "Reward models learn to score responses based on human preferences. They output a scalar reward for each response, used in RLHF pipelines (GRPO, RLOO) for policy optimization.\n",
    "\n",
    "**Important:** This notebook includes a kernel shutdown cell at the end to release all GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c06f1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/pixi/.pixi/envs/default/lib/python3.13/site-packages/trl/__init__.py:203: UserWarning: TRL currently supports vLLM versions: 0.10.2, 0.11.0, 0.11.1, 0.11.2. You have version 0.14.0rc1.dev201+gadcf682fc.cu130 installed. We recommend installing a supported version to avoid compatibility issues.\n  if is_vllm_available():"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!",
      "Environment: unsloth 2025.12.10, PyTorch 2.9.1+cu130, NVIDIA GeForce RTX 4080 SUPER\nHF_TOKEN loaded: Yes"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Force text-based progress instead of HTML widgets\n",
    "os.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n",
    "\n",
    "# CRITICAL: Import unsloth FIRST for proper TRL patching\n",
    "import unsloth\n",
    "from unsloth import is_bf16_supported\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from trl import RewardConfig, RewardTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "\n",
    "# Environment summary\n",
    "gpu = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"Environment: unsloth {unsloth.__version__}, PyTorch {torch.__version__}, {gpu}\")\n",
    "print(f\"HF_TOKEN loaded: {'Yes' if os.environ.get('HF_TOKEN') else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c74f970a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nLoading Qwen3-4B as reward model..."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971e3584bb554754bdfcb6a001155ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d194b9d6234f4fb2bb7beead065aa56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c1f14001254fbf86de66e2da5185d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3df830cd0c46f1b646d396f32b42ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785769a8a63c411e84ea314554aab4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Qwen3ForSequenceClassification LOAD REPORT from: Qwen/Qwen3-4B\nKey          | Status  | \n-------------+---------+-\nscore.weight | MISSING | \n\nNotes:\n- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22eb9c24c93d430dab5e6c73d09e01c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570cbcc8de48422c8c96dac8eade926c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612dd6376fd64731980347b79174c53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a209069007477bbe482da2d472f38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Qwen3ForSequenceClassification"
     ]
    }
   ],
   "source": [
    "# Load Qwen3-4B as Reward Model (SequenceClassification head)\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-4B\"\n",
    "print(f\"\\nLoading {MODEL_NAME.split('/')[-1]} as reward model...\")\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if is_bf16_supported() else torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=1,  # Single scalar reward output\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# Ensure pad token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Model loaded: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25da06f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,032,704 || all params: 4,055,503,360 || trainable%: 0.8145"
     ]
    }
   ],
   "source": [
    "# Apply LoRA adapters for reward model training\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7198a9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created: 5 preference pairs"
     ]
    }
   ],
   "source": [
    "# Create minimal synthetic preference dataset (5 samples)\n",
    "# Reward models learn from preference pairs (chosen > rejected)\n",
    "\n",
    "preference_data = [\n",
    "    {\n",
    "        \"prompt\": \"Explain recursion in programming.\",\n",
    "        \"chosen\": \"Recursion is when a function calls itself with a simpler version of the problem, including a base case to stop infinite loops.\",\n",
    "        \"rejected\": \"Recursion is just loops.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is an API?\",\n",
    "        \"chosen\": \"An API (Application Programming Interface) is a set of protocols that allows different software applications to communicate with each other.\",\n",
    "        \"rejected\": \"API is code.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Describe version control.\",\n",
    "        \"chosen\": \"Version control is a system that records changes to files over time, allowing you to recall specific versions and collaborate with others.\",\n",
    "        \"rejected\": \"Version control saves files.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is a database?\",\n",
    "        \"chosen\": \"A database is an organized collection of structured data stored electronically, typically managed by a database management system (DBMS).\",\n",
    "        \"rejected\": \"A database stores stuff.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain object-oriented programming.\",\n",
    "        \"chosen\": \"Object-oriented programming (OOP) is a paradigm that organizes code into objects containing data (attributes) and behavior (methods).\",\n",
    "        \"rejected\": \"OOP uses objects.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_list(preference_data)\n",
    "print(f\"Dataset created: {len(dataset)} preference pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af815e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward Training Configuration (minimal steps for testing)\n",
    "reward_config = RewardConfig(\n",
    "    output_dir=\"outputs_reward_qwen_test\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    max_steps=3,  # Minimal steps for testing\n",
    "    warmup_steps=0,\n",
    "    learning_rate=1e-5,\n",
    "    logging_steps=1,\n",
    "    fp16=not is_bf16_supported(),\n",
    "    bf16=is_bf16_supported(),\n",
    "    optim=\"adamw_8bit\",\n",
    "    max_length=512,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Initialize Reward Trainer\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=reward_config,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Starting Reward training (3 steps)...\")\n",
    "trainer_stats = trainer.train()\n",
    "print(f\"Reward training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cd8036a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\nReward Model Training Pipeline Test PASSED\n============================================================\nGood response score: 1.4453\nBad response score:  1.0703\nPreference correct:  Yes"
     ]
    }
   ],
   "source": [
    "# Post-training reward scoring test\n",
    "model.eval()\n",
    "\n",
    "def get_reward(prompt, response):\n",
    "    \"\"\"Score a response using the trained reward model.\"\"\"\n",
    "    text = prompt + \" \" + response\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        reward = outputs.logits[0, 0].item()\n",
    "    \n",
    "    return reward\n",
    "\n",
    "# Test with a good and bad response\n",
    "test_prompt = \"What is machine learning?\"\n",
    "good_response = \"Machine learning is a subset of AI where computers learn patterns from data to make predictions.\"\n",
    "bad_response = \"ML is stuff.\"\n",
    "\n",
    "good_score = get_reward(test_prompt, good_response)\n",
    "bad_score = get_reward(test_prompt, bad_response)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Reward Model Training Pipeline Test PASSED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Good response score: {good_score:.4f}\")\n",
    "print(f\"Bad response score:  {bad_score:.4f}\")\n",
    "print(f\"Preference correct:  {'Yes' if good_score > bad_score else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c82f318",
   "metadata": {},
   "source": [
    "## Test Complete\n",
    "\n",
    "The Reward Model Training Pipeline test has completed successfully. The kernel will now shut down to release all GPU memory.\n",
    "\n",
    "### What Was Verified\n",
    "- AutoModelForSequenceClassification loading with 4-bit quantization (Qwen3-4B)\n",
    "- LoRA adapter configuration for reward modeling\n",
    "- Synthetic preference dataset creation\n",
    "- RewardTrainer training loop (3 steps)\n",
    "- Post-training reward scoring\n",
    "\n",
    "### Reward Model Concepts Demonstrated\n",
    "- **Sequence Classification**: Model outputs scalar reward score\n",
    "- **Preference Learning**: Trained on chosen vs rejected pairs\n",
    "- **RLHF Integration**: Can be used with GRPO/RLOO trainers\n",
    "\n",
    "### Ready for Production\n",
    "If this test passed, your environment is ready for:\n",
    "- Training reward models on real preference data\n",
    "- RLHF pipelines with learned rewards\n",
    "- Response quality scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b351c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown kernel to release all GPU memory\n",
    "import IPython\n",
    "print(\"Shutting down kernel to release GPU memory...\")\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}