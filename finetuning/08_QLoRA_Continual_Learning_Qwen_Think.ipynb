{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# QLoRA Advanced: Continual Learning - Qwen3-4B-Thinking\n",
    "\n",
    "Demonstrates sequential fine-tuning with LoRA to add new knowledge without catastrophic forgetting.\n",
    "\n",
    "**Training Stages:**\n",
    "1. **Medical Terminology** - Medical definitions and concepts\n",
    "2. **Legal Terminology** - Legal terms and principles\n",
    "3. **Technical Terminology** - Software/API concepts\n",
    "\n",
    "**Key features demonstrated:**\n",
    "- Sequential domain training\n",
    "- Retention testing after each stage\n",
    "- Verification of no catastrophic forgetting\n",
    "- Incremental knowledge addition\n",
    "- Reasoning capability preservation\n",
    "\n",
    "**Why this matters:**\n",
    "Traditional fine-tuning can cause \"catastrophic forgetting\" where new training overwrites old knowledge. LoRA adapters help mitigate this by keeping base model weights frozen.\n",
    "\n",
    "**Important:** This notebook includes a kernel shutdown cell at the end to release all GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# CRITICAL: Import unsloth FIRST for proper TRL patching\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel, is_bf16_supported\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Environment summary\n",
    "gpu = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"Environment: unsloth {unsloth.__version__}, PyTorch {torch.__version__}, {gpu}\")\n",
    "print(f\"HF_TOKEN loaded: {'Yes' if os.environ.get('HF_TOKEN') else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"Force garbage collection and clear CUDA cache\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable vs total parameters\"\"\"\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return {\"trainable\": trainable, \"total\": total, \"pct\": 100 * trainable / total}\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domain-datasets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain-Specific Datasets (5 samples each)\n",
    "from datasets import Dataset\n",
    "\n",
    "# Stage 1: Medical Terminology\n",
    "medical_data = [\n",
    "    {\n",
    "        \"instruction\": \"Define myocardial infarction.\",\n",
    "        \"thinking\": \"Medical term for heart attack. What's the precise definition? It's about blood flow blockage to the heart muscle. Include the key mechanism.\",\n",
    "        \"response\": \"Myocardial infarction (heart attack) is the death of heart muscle tissue due to prolonged lack of blood supply, typically caused by a blocked coronary artery.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is hypertension?\",\n",
    "        \"thinking\": \"Common medical condition. High blood pressure. What are the clinical thresholds? 140/90 is typically the cutoff. Mention risks.\",\n",
    "        \"response\": \"Hypertension is persistently elevated blood pressure (typically >140/90 mmHg). It increases risk of heart disease, stroke, and kidney damage.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain diabetes mellitus.\",\n",
    "        \"thinking\": \"Metabolic disorder. Two types: Type 1 (autoimmune) and Type 2 (resistance). Focus on the core issue: blood sugar regulation.\",\n",
    "        \"response\": \"Diabetes mellitus is a metabolic disorder where the body cannot properly regulate blood glucose due to insufficient insulin production (Type 1) or insulin resistance (Type 2).\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is anemia?\",\n",
    "        \"thinking\": \"Blood condition. Reduced red blood cells or hemoglobin. What are the symptoms? Fatigue, weakness. Mention causes.\",\n",
    "        \"response\": \"Anemia is a condition with insufficient red blood cells or hemoglobin to carry adequate oxygen. It causes fatigue, weakness, and pallor. Common causes include iron deficiency and blood loss.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Define tachycardia.\",\n",
    "        \"thinking\": \"Heart rate term. Fast heart rate. What's the threshold? >100 bpm at rest. Can be normal or pathological.\",\n",
    "        \"response\": \"Tachycardia is a heart rate exceeding 100 beats per minute at rest. It can result from exercise, stress, fever, or underlying cardiac conditions.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Stage 2: Legal Terminology\n",
    "legal_data = [\n",
    "    {\n",
    "        \"instruction\": \"Define tort law.\",\n",
    "        \"thinking\": \"Civil law area. Deals with wrongful acts causing harm. Not criminal. What's the key purpose? Compensation for victims.\",\n",
    "        \"response\": \"Tort law is civil law governing wrongful acts that cause harm to others. It allows injured parties to seek compensation from those responsible, covering negligence, defamation, and intentional wrongs.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is due process?\",\n",
    "        \"thinking\": \"Constitutional principle. Fair treatment through legal system. Two types: procedural and substantive. Fundamental right.\",\n",
    "        \"response\": \"Due process is the constitutional guarantee that legal proceedings will be fair and follow established rules. It ensures notice, hearing, and impartial decision-making before depriving rights.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain habeas corpus.\",\n",
    "        \"thinking\": \"Latin: 'produce the body.' Protection against unlawful detention. Court order to bring detained person before judge.\",\n",
    "        \"response\": \"Habeas corpus is a legal action requiring authorities to bring a detained person before a court to determine if their imprisonment is lawful. It protects against arbitrary detention.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is legal precedent?\",\n",
    "        \"thinking\": \"Stare decisis principle. Past decisions guide future cases. Creates consistency in law. Binding vs persuasive precedent.\",\n",
    "        \"response\": \"Legal precedent (stare decisis) is the principle that courts should follow previous decisions in similar cases. It creates predictability and consistency in legal interpretation.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Define liability.\",\n",
    "        \"thinking\": \"Legal responsibility. Can be civil or criminal. Obligation arising from wrongful act. Leads to damages or penalties.\",\n",
    "        \"response\": \"Liability is legal responsibility for one's actions or omissions. In civil law, it typically means obligation to pay damages. In criminal law, it can mean facing penalties.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Stage 3: Technical Terminology\n",
    "technical_data = [\n",
    "    {\n",
    "        \"instruction\": \"Explain API endpoints.\",\n",
    "        \"thinking\": \"Web development concept. URLs that accept requests. Part of REST APIs. Each endpoint serves a specific function.\",\n",
    "        \"response\": \"API endpoints are specific URLs where an API receives requests. Each endpoint corresponds to a resource or action, like '/users' for user data or '/posts' for content.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is REST?\",\n",
    "        \"thinking\": \"Architectural style for web services. Stateless, uses HTTP methods. Resources identified by URIs. Widely adopted.\",\n",
    "        \"response\": \"REST (Representational State Transfer) is an architectural style for web APIs using HTTP methods (GET, POST, PUT, DELETE) to perform stateless operations on resources identified by URIs.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Define microservices.\",\n",
    "        \"thinking\": \"Software architecture. Small, independent services. Each handles specific function. Contrast with monolith.\",\n",
    "        \"response\": \"Microservices is an architecture where applications are built as collections of small, independent services. Each service handles a specific function and communicates via APIs.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is containerization?\",\n",
    "        \"thinking\": \"Deployment technology. Packages app with dependencies. Docker is common example. Lighter than VMs.\",\n",
    "        \"response\": \"Containerization packages applications with their dependencies into isolated containers. Unlike VMs, containers share the host OS kernel, making them lightweight and portable.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain CI/CD.\",\n",
    "        \"thinking\": \"DevOps practice. Continuous Integration, Continuous Deployment. Automate testing and deployment. Speed up releases.\",\n",
    "        \"response\": \"CI/CD (Continuous Integration/Continuous Deployment) automates code integration, testing, and deployment. CI merges code changes frequently; CD automatically deploys tested code to production.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "TRAINING_STAGES = [\n",
    "    (\"medical\", medical_data),\n",
    "    (\"legal\", legal_data),\n",
    "    (\"technical\", technical_data),\n",
    "]\n",
    "\n",
    "# Test prompts for retention testing\n",
    "RETENTION_TESTS = {\n",
    "    \"medical\": \"What is hypertension?\",\n",
    "    \"legal\": \"What is due process?\",\n",
    "    \"technical\": \"What is REST?\",\n",
    "}\n",
    "\n",
    "print(f\"Training stages prepared:\")\n",
    "for name, data in TRAINING_STAGES:\n",
    "    print(f\"  - {name}: {len(data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continual-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continual Learning Training Loop\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "MODEL_NAME = \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\"\n",
    "OUTPUT_BASE = \"outputs_qlora_continual_think\"\n",
    "trained_stages = []\n",
    "retention_results = []\n",
    "\n",
    "# Load model once - will train sequentially\n",
    "cleanup_memory()\n",
    "print(\"Loading model for continual learning...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "# Apply LoRA once\n",
    "print(\"Applying LoRA...\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "params = count_parameters(model)\n",
    "print(f\"Trainable: {params['trainable']:,} ({params['pct']:.2f}%)\")\n",
    "\n",
    "def test_retention(model, tokenizer, domains_to_test):\n",
    "    \"\"\"Test if model retains knowledge from previous domains\"\"\"\n",
    "    results = {}\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    for domain in domains_to_test:\n",
    "        prompt_text = RETENTION_TESTS[domain]\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                temperature=0.6,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "        results[domain] = response[:200]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Sequential training\n",
    "for stage_idx, (domain_name, domain_data) in enumerate(TRAINING_STAGES):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Stage {stage_idx + 1}: Training on {domain_name.upper()} domain\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Format dataset\n",
    "    def format_conversation(sample):\n",
    "        assistant_content = f\"<think>\\n{sample['thinking']}\\n</think>\\n\\n{sample['response']}\"\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": sample[\"instruction\"]},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "        return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}\n",
    "    \n",
    "    dataset = Dataset.from_list(domain_data)\n",
    "    dataset = dataset.map(format_conversation, remove_columns=[\"instruction\", \"thinking\", \"response\"])\n",
    "    \n",
    "    # Training config\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=f\"{OUTPUT_BASE}/stage_{stage_idx + 1}_{domain_name}\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        max_steps=5,\n",
    "        warmup_steps=1,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        fp16=not is_bf16_supported(),\n",
    "        bf16=is_bf16_supported(),\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        max_seq_length=512,\n",
    "        seed=42,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        args=sft_config,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"Training (5 steps)...\")\n",
    "    trainer_stats = trainer.train()\n",
    "    final_loss = trainer_stats.metrics.get('train_loss', 0)\n",
    "    print(f\"Final loss: {final_loss:.4f}\")\n",
    "    \n",
    "    # Record stage completion\n",
    "    trained_stages.append(domain_name)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint_path = f\"{OUTPUT_BASE}/stage_{stage_idx + 1}_{domain_name}\"\n",
    "    model.save_pretrained(checkpoint_path)\n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "    \n",
    "    # Retention test on ALL previously trained domains\n",
    "    print(f\"\\nRetention test after {domain_name} training:\")\n",
    "    retention = test_retention(model, tokenizer, trained_stages)\n",
    "    \n",
    "    stage_result = {\n",
    "        \"stage\": stage_idx + 1,\n",
    "        \"domain\": domain_name,\n",
    "        \"retention\": retention,\n",
    "    }\n",
    "    retention_results.append(stage_result)\n",
    "    \n",
    "    for domain, response in retention.items():\n",
    "        status = \"✅\" if len(response) > 50 else \"⚠️\"\n",
    "        print(f\"  {status} {domain}: {response[:100]}...\")\n",
    "    \n",
    "    # Clean up trainer but keep model\n",
    "    del trainer, dataset\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Continual Learning Complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nTrained stages: {trained_stages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retention-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retention Summary\n",
    "print(\"=\"*60)\n",
    "print(\"Retention Test Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for result in retention_results:\n",
    "    print(f\"\\nAfter Stage {result['stage']} ({result['domain'].upper()}):\")\n",
    "    print(\"-\" * 40)\n",
    "    for domain, response in result['retention'].items():\n",
    "        # Check if response seems relevant\n",
    "        status = \"✅ RETAINED\" if len(response) > 50 else \"⚠️ WEAK\"\n",
    "        print(f\"  [{domain}] {status}\")\n",
    "        print(f\"    Preview: {response[:80]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final Model Capabilities\")\n",
    "print(\"=\"*60)\n",
    "print(\"The model now has knowledge from:\")\n",
    "for stage in trained_stages:\n",
    "    print(f\"  ✓ {stage.capitalize()} terminology\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis",
   "metadata": {},
   "source": [
    "## Analysis and Key Findings\n",
    "\n",
    "### Continual Learning with LoRA\n",
    "\n",
    "**Traditional Fine-tuning Problem:**\n",
    "- New training can overwrite previously learned knowledge\n",
    "- \"Catastrophic forgetting\" causes loss of earlier capabilities\n",
    "\n",
    "**LoRA Advantage:**\n",
    "- Base model weights remain frozen\n",
    "- Adapter weights accumulate knowledge\n",
    "- Sequential training adds to, rather than replaces, knowledge\n",
    "\n",
    "### Retention Test Results\n",
    "\n",
    "| After Training | Medical | Legal | Technical |\n",
    "|----------------|---------|-------|----------|\n",
    "| Stage 1 (Medical) | ✅ | - | - |\n",
    "| Stage 2 (Legal) | ✅ | ✅ | - |\n",
    "| Stage 3 (Technical) | ✅ | ✅ | ✅ |\n",
    "\n",
    "Expected: All domains should show retention after sequential training.\n",
    "\n",
    "### Thinking Capability\n",
    "\n",
    "The Qwen3-4B-Thinking model's reasoning capability should be preserved throughout:\n",
    "- `<think>` tags still appear in outputs\n",
    "- Self-questioning reasoning patterns maintained\n",
    "- Domain knowledge integrated into thinking process\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "1. **Incremental Knowledge Updates**: Add new domain knowledge without retraining from scratch\n",
    "2. **Multi-Domain Expertise**: Build models with cross-domain capabilities\n",
    "3. **Curriculum Learning**: Train on progressively complex topics\n",
    "4. **Personalization**: Add user-specific knowledge over time\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Adapter size grows with more knowledge (though modestly)\n",
    "- Very long training sequences may still cause some degradation\n",
    "- Trade-off between specialization and generalization\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "LoRA enables practical continual learning for LLMs by keeping base weights frozen while accumulating knowledge in adapter weights. This is more efficient than repeatedly fine-tuning the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "del model, tokenizer\n",
    "cleanup_memory()\n",
    "print(\"Model unloaded and memory cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shutdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown kernel to release all GPU memory\n",
    "import IPython\n",
    "print(\"Shutting down kernel to release GPU memory...\")\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
