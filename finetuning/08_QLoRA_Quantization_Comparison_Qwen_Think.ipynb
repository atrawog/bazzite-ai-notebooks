{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# QLoRA Test: Quantization Method Comparison - Qwen3-4B-Thinking\n",
    "\n",
    "Compares different quantization methods for QLoRA training to understand memory/quality tradeoffs.\n",
    "\n",
    "**Quantization methods tested:**\n",
    "- **4-bit NF4**: QLoRA standard - Normal Float 4-bit with double quantization\n",
    "- **8-bit INT8**: BitsAndBytes 8-bit quantization\n",
    "- **FP16**: Full precision baseline (16-bit floating point)\n",
    "\n",
    "**Measurements:**\n",
    "- Peak GPU memory usage (MB)\n",
    "- Model loading time\n",
    "- Training time\n",
    "- Final loss\n",
    "- Inference quality with thinking traces\n",
    "\n",
    "**Expected outcomes:**\n",
    "- 4-bit: ~70% memory reduction vs FP16\n",
    "- 8-bit: ~50% memory reduction vs FP16\n",
    "- Quality degradation: minimal (<2% loss difference)\n",
    "\n",
    "**Important:** This notebook includes a kernel shutdown cell at the end to release all GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# CRITICAL: Import unsloth FIRST for proper TRL patching\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel, is_bf16_supported\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Environment summary\n",
    "gpu = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"Environment: unsloth {unsloth.__version__}, PyTorch {torch.__version__}, {gpu}\")\n",
    "print(f\"HF_TOKEN loaded: {'Yes' if os.environ.get('HF_TOKEN') else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Helper Functions\n",
    "import subprocess\n",
    "\n",
    "def measure_gpu_memory():\n",
    "    \"\"\"Measure current GPU memory usage in MB using nvidia-smi\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        return int(result.stdout.strip().split('\\n')[0])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable vs total parameters\"\"\"\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return {\n",
    "        \"trainable\": trainable,\n",
    "        \"total\": total,\n",
    "        \"pct\": 100 * trainable / total\n",
    "    }\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"Force garbage collection and clear CUDA cache\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "print(\"Benchmark functions defined.\")\n",
    "print(f\"Initial GPU memory: {measure_gpu_memory()} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create minimal synthetic instruction dataset with thinking content (5 samples)\n",
    "from datasets import Dataset\n",
    "\n",
    "# Same dataset across all quantization methods for fair comparison\n",
    "synthetic_data = [\n",
    "    {\n",
    "        \"instruction\": \"What is machine learning?\",\n",
    "        \"thinking\": \"What is the user asking here? They want to understand machine learning. What are the key concepts I should cover? It's a subset of AI... and it involves learning from data. How should I keep this accessible? Short and clear definition.\",\n",
    "        \"response\": \"Machine learning is a subset of artificial intelligence where computers learn patterns from data.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain Python in one sentence.\",\n",
    "        \"thinking\": \"One sentence only - what's most important about Python? Its readability and versatility are the defining features. How do I capture both in one sentence?\",\n",
    "        \"response\": \"Python is a high-level programming language known for its readability and versatility.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is a neural network?\",\n",
    "        \"thinking\": \"How do I explain neural networks simply? What's the core concept? They're inspired by biological neurons... they process information in layers. Should I mention deep learning? Maybe keep it basic for now.\",\n",
    "        \"response\": \"A neural network is a computational model inspired by biological neurons that processes information through connected layers.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Define supervised learning.\",\n",
    "        \"thinking\": \"What makes supervised learning 'supervised'? It's the labels! The data has known outputs. How do I explain this clearly? Focus on the training process with labeled data.\",\n",
    "        \"response\": \"Supervised learning is training a model on labeled data to predict outcomes for new inputs.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is gradient descent?\",\n",
    "        \"thinking\": \"This is a bit technical. What's the intuition behind gradient descent? It's like walking downhill to find the lowest point. The gradient tells us which direction is 'down'. Keep it conceptual.\",\n",
    "        \"response\": \"Gradient descent is an optimization algorithm that minimizes loss by iteratively adjusting parameters in the direction of steepest descent.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Synthetic dataset prepared: {len(synthetic_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantization-configs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization Configuration\n",
    "\n",
    "# Note: For Unsloth, we use different model variants for different quantizations\n",
    "QUANT_CONFIGS = {\n",
    "    \"4bit_nf4\": {\n",
    "        \"model_name\": \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n",
    "        \"load_in_4bit\": True,\n",
    "        \"description\": \"4-bit NF4 (QLoRA standard)\",\n",
    "    },\n",
    "    \"8bit\": {\n",
    "        \"model_name\": \"unsloth/Qwen3-4B-Thinking-2507\",\n",
    "        \"load_in_4bit\": False,\n",
    "        \"load_in_8bit\": True,\n",
    "        \"description\": \"8-bit INT8 (BitsAndBytes)\",\n",
    "    },\n",
    "    \"fp16\": {\n",
    "        \"model_name\": \"unsloth/Qwen3-4B-Thinking-2507\",\n",
    "        \"load_in_4bit\": False,\n",
    "        \"load_in_8bit\": False,\n",
    "        \"description\": \"FP16 (full precision baseline)\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Quantization configurations:\")\n",
    "for name, config in QUANT_CONFIGS.items():\n",
    "    print(f\"  - {name}: {config['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantization-comparison-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization Comparison Loop\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "results = []\n",
    "\n",
    "for quant_name, quant_config in QUANT_CONFIGS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {quant_config['description']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Cleanup and measure baseline\n",
    "    cleanup_memory()\n",
    "    mem_baseline = measure_gpu_memory()\n",
    "    \n",
    "    # Load model with timing\n",
    "    print(f\"Loading model: {quant_config['model_name'].split('/')[-1]}...\")\n",
    "    load_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        load_kwargs = {\n",
    "            \"max_seq_length\": 512,\n",
    "            \"dtype\": None,\n",
    "        }\n",
    "        \n",
    "        if quant_config.get(\"load_in_4bit\"):\n",
    "            load_kwargs[\"load_in_4bit\"] = True\n",
    "        elif quant_config.get(\"load_in_8bit\"):\n",
    "            load_kwargs[\"load_in_8bit\"] = True\n",
    "        else:\n",
    "            # FP16 - no quantization\n",
    "            load_kwargs[\"dtype\"] = torch.float16\n",
    "        \n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            quant_config[\"model_name\"],\n",
    "            **load_kwargs\n",
    "        )\n",
    "        \n",
    "        load_time = time.time() - load_start\n",
    "        mem_after_load = measure_gpu_memory()\n",
    "        \n",
    "        print(f\"Load time: {load_time:.1f}s, Memory: {mem_after_load} MB\")\n",
    "        \n",
    "        # Apply LoRA\n",
    "        print(f\"Applying LoRA (r=16, alpha=16)...\")\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=16,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                            \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            bias=\"none\",\n",
    "            use_gradient_checkpointing=\"unsloth\",\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        mem_after_lora = measure_gpu_memory()\n",
    "        params = count_parameters(model)\n",
    "        print(f\"Trainable: {params['trainable']:,} ({params['pct']:.2f}%)\")\n",
    "        \n",
    "        # Format dataset\n",
    "        def format_conversation(sample):\n",
    "            assistant_content = f\"<think>\\n{sample['thinking']}\\n</think>\\n\\n{sample['response']}\"\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": sample[\"instruction\"]},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "            ]\n",
    "            return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}\n",
    "        \n",
    "        dataset = Dataset.from_list(synthetic_data)\n",
    "        dataset = dataset.map(format_conversation, remove_columns=[\"instruction\", \"thinking\", \"response\"])\n",
    "        \n",
    "        # Training config\n",
    "        sft_config = SFTConfig(\n",
    "            output_dir=f\"outputs_qlora_quant_think/{quant_name}\",\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=1,\n",
    "            max_steps=3,\n",
    "            warmup_steps=1,\n",
    "            learning_rate=2e-4,\n",
    "            logging_steps=1,\n",
    "            fp16=not is_bf16_supported(),\n",
    "            bf16=is_bf16_supported(),\n",
    "            optim=\"adamw_8bit\",\n",
    "            weight_decay=0.01,\n",
    "            max_seq_length=512,\n",
    "            seed=42,\n",
    "            report_to=\"none\",\n",
    "        )\n",
    "        \n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            train_dataset=dataset,\n",
    "            dataset_text_field=\"text\",\n",
    "            args=sft_config,\n",
    "        )\n",
    "        \n",
    "        # Train with timing\n",
    "        print(f\"Training (3 steps)...\")\n",
    "        train_start = time.time()\n",
    "        trainer_stats = trainer.train()\n",
    "        train_time = time.time() - train_start\n",
    "        \n",
    "        final_loss = trainer_stats.metrics.get('train_loss', 0)\n",
    "        mem_peak = measure_gpu_memory()\n",
    "        \n",
    "        print(f\"Train time: {train_time:.1f}s, Final loss: {final_loss:.4f}\")\n",
    "        print(f\"Peak memory: {mem_peak} MB\")\n",
    "        \n",
    "        # Quick inference test\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        messages = [{\"role\": \"user\", \"content\": \"What is deep learning?\"}]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                temperature=0.6,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"quantization\": quant_name,\n",
    "            \"description\": quant_config[\"description\"],\n",
    "            \"load_time_sec\": load_time,\n",
    "            \"train_time_sec\": train_time,\n",
    "            \"model_memory_mb\": mem_after_load - mem_baseline,\n",
    "            \"peak_memory_mb\": mem_peak,\n",
    "            \"final_loss\": final_loss,\n",
    "            \"sample_response\": response[:150],\n",
    "            \"success\": True,\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        results.append({\n",
    "            \"quantization\": quant_name,\n",
    "            \"description\": quant_config[\"description\"],\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "        })\n",
    "    \n",
    "    finally:\n",
    "        # Cleanup\n",
    "        try:\n",
    "            del model, tokenizer, trainer, dataset\n",
    "        except:\n",
    "            pass\n",
    "        cleanup_memory()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All quantization methods tested!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter successful results\n",
    "successful_results = [r for r in results if r.get('success', False)]\n",
    "\n",
    "if successful_results:\n",
    "    df = pd.DataFrame(successful_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Quantization Comparison Results\")\n",
    "    print(\"=\"*60)\n",
    "    print(df[[\"quantization\", \"peak_memory_mb\", \"train_time_sec\", \"final_loss\"]].to_string(index=False))\n",
    "    \n",
    "    # Calculate memory savings vs FP16\n",
    "    if \"fp16\" in df[\"quantization\"].values:\n",
    "        fp16_mem = df[df[\"quantization\"] == \"fp16\"][\"peak_memory_mb\"].values[0]\n",
    "        print(f\"\\nMemory savings vs FP16 ({fp16_mem} MB):\")\n",
    "        for _, row in df.iterrows():\n",
    "            if row[\"quantization\"] != \"fp16\":\n",
    "                savings = 100 * (1 - row[\"peak_memory_mb\"] / fp16_mem)\n",
    "                print(f\"  {row['quantization']}: {savings:.1f}% reduction\")\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    colors = ['#2ecc71', '#3498db', '#e74c3c']  # green, blue, red\n",
    "    \n",
    "    # Plot 1: Peak GPU Memory\n",
    "    bars1 = axes[0].bar(df['quantization'], df['peak_memory_mb'], color=colors[:len(df)])\n",
    "    axes[0].set_title('Peak GPU Memory', fontsize=12)\n",
    "    axes[0].set_xlabel('Quantization Method')\n",
    "    axes[0].set_ylabel('Memory (MB)')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    for bar, val in zip(bars1, df['peak_memory_mb']):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
    "                     f'{val:.0f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot 2: Training Time\n",
    "    bars2 = axes[1].bar(df['quantization'], df['train_time_sec'], color=colors[:len(df)])\n",
    "    axes[1].set_title('Training Time (3 steps)', fontsize=12)\n",
    "    axes[1].set_xlabel('Quantization Method')\n",
    "    axes[1].set_ylabel('Seconds')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    for bar, val in zip(bars2, df['train_time_sec']):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                     f'{val:.1f}s', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot 3: Final Loss\n",
    "    bars3 = axes[2].bar(df['quantization'], df['final_loss'], color=colors[:len(df)])\n",
    "    axes[2].set_title('Final Training Loss', fontsize=12)\n",
    "    axes[2].set_xlabel('Quantization Method')\n",
    "    axes[2].set_ylabel('Loss')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    for bar, val in zip(bars3, df['final_loss']):\n",
    "        axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "                     f'{val:.4f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs_qlora_quant_think/quantization_comparison.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nVisualization saved to outputs_qlora_quant_think/quantization_comparison.png\")\n",
    "else:\n",
    "    print(\"No successful results to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis",
   "metadata": {},
   "source": [
    "## Analysis and Key Findings\n",
    "\n",
    "### Memory Usage\n",
    "\n",
    "| Method | Typical Memory | Savings vs FP16 |\n",
    "|--------|---------------|------------------|\n",
    "| 4-bit NF4 | ~3-4 GB | ~70-75% |\n",
    "| 8-bit INT8 | ~5-6 GB | ~50% |\n",
    "| FP16 | ~10-12 GB | baseline |\n",
    "\n",
    "### Training Performance\n",
    "\n",
    "**4-bit NF4:**\n",
    "- Fastest loading (pre-quantized model)\n",
    "- May have slightly slower per-step training due to dequantization\n",
    "- Enables training on smaller GPUs\n",
    "\n",
    "**8-bit INT8:**\n",
    "- Middle ground memory usage\n",
    "- Better precision than 4-bit\n",
    "- Good balance for quality-constrained applications\n",
    "\n",
    "**FP16:**\n",
    "- Highest memory usage\n",
    "- Fastest per-step training (no dequantization overhead)\n",
    "- Best for maximum quality when GPU memory allows\n",
    "\n",
    "### Quality Comparison\n",
    "\n",
    "Final loss differences are typically minimal (<2%) between quantization methods for most tasks. The thinking capability of Qwen3-4B-Thinking is preserved across all quantization levels.\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "| GPU Memory | Recommended | Notes |\n",
    "|------------|-------------|-------|\n",
    "| <8 GB | 4-bit NF4 | Only viable option |\n",
    "| 8-16 GB | 4-bit NF4 or 8-bit | Choose based on quality needs |\n",
    "| >16 GB | FP16 or 8-bit | Maximum quality possible |\n",
    "\n",
    "### Key Insight\n",
    "4-bit NF4 quantization (QLoRA) provides an excellent trade-off: ~70% memory reduction with minimal quality loss, making it the go-to choice for most fine-tuning scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shutdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown kernel to release all GPU memory\n",
    "import IPython\n",
    "print(\"Shutting down kernel to release GPU memory...\")\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
