{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsloth Environment Verification\n",
    "\n",
    "This notebook verifies that all components are correctly installed for running Unsloth notebooks:\n",
    "- GRPO (Reinforcement Learning)\n",
    "- Vision fine-tuning (Ministral VL)\n",
    "- fast_inference support\n",
    "\n",
    "**Run this after rebuilding the jupyter pod to verify the environment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ HF_TOKEN loaded: Yes",
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/pixi/.pixi/envs/default/lib/python3.13/site-packages/trl/__init__.py:203: UserWarning: TRL currently supports vLLM versions: 0.10.2, 0.11.0, 0.11.1, 0.11.2. You have version 0.14.0rc1.dev201+gadcf682fc.cu130 installed. We recommend installing a supported version to avoid compatibility issues.\n  if is_vllm_available():"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!",
      "âœ“ unsloth: 2025.12.10\nâœ“ transformers: 5.0.0rc1\nâœ“ vLLM: 0.14.0rc1.dev201+gadcf682fc\nâœ“ TRL: 0.26.2\nâœ“ PyTorch: 2.9.1+cu130\nâœ“ CUDA available: True\nâœ“ GPU: NVIDIA GeForce RTX 4080 SUPER"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env from notebook directory\n",
    "load_dotenv()\n",
    "print(f\"âœ“ HF_TOKEN loaded: {'Yes' if os.environ.get('HF_TOKEN') else 'No'}\")\n",
    "\n",
    "# CRITICAL: Import unsloth FIRST for proper TRL patching\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel, FastVisionModel\n",
    "print(f\"âœ“ unsloth: {unsloth.__version__}\")\n",
    "\n",
    "import transformers\n",
    "print(f\"âœ“ transformers: {transformers.__version__}\")\n",
    "\n",
    "import vllm\n",
    "print(f\"âœ“ vLLM: {vllm.__version__}\")\n",
    "\n",
    "import trl\n",
    "print(f\"âœ“ TRL: {trl.__version__}\")\n",
    "\n",
    "import torch\n",
    "print(f\"âœ“ PyTorch: {torch.__version__}\")\n",
    "print(f\"âœ“ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GRPO/RL Imports ===\nâœ“ All GRPO imports successful"
     ]
    }
   ],
   "source": [
    "# Test imports for Reinforcement Learning notebook\n",
    "print(\"=== GRPO/RL Imports ===\")\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from datasets import Dataset\n",
    "print(\"\\u2713 All GRPO imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vision/SFT Imports ===\nâœ“ All Vision imports successful"
     ]
    }
   ],
   "source": [
    "# Test imports for Vision notebook\n",
    "print(\"=== Vision/SFT Imports ===\")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from unsloth import is_bf16_supported\n",
    "from transformers import TextStreamer\n",
    "print(\"\\u2713 All Vision imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Model Loading ===",
      "==((====))==  Unsloth 2025.12.10: Fast Ministral3 patching. Transformers: 5.0.0rc1. vLLM: 0.14.0rc1.dev201+gadcf682fc.cu130.\n   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.568 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.9.1+cu130. CUDA: 8.9. CUDA Toolkit: 13.0. Triton: 3.5.1\n\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42489f8fca214303959e34bd94ff97d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/458 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded: Mistral3ForConditionalGeneration\nâœ“ Tokenizer: PixtralProcessor\nâœ“ Cleanup complete"
     ]
    }
   ],
   "source": [
    "# Test model loading with FastLanguageModel\n",
    "print(\"=== Testing Model Loading ===\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"unsloth/Ministral-3-3B-Reasoning-2512\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "print(f\"âœ“ Model loaded: {type(model).__name__}\")\n",
    "print(f\"âœ“ Tokenizer: {type(tokenizer).__name__}\")\n",
    "\n",
    "# Clean up\n",
    "del model, tokenizer\n",
    "import gc; gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"âœ“ Cleanup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fast Inference Check ===\nfast_inference=True uses vLLM as backend for 2x faster inference\nvLLM version: 0.14.0rc1.dev201+gadcf682fc\nUnsloth version: 2025.12.10\nâœ“ fast_inference parameter available"
     ]
    }
   ],
   "source": [
    "# Test fast_inference capability\n",
    "print(\"=== Fast Inference Check ===\")\n",
    "print(\"fast_inference=True uses vLLM as backend for 2x faster inference\")\n",
    "print(f\"vLLM version: {vllm.__version__}\")\n",
    "print(f\"Unsloth version: {unsloth.__version__}\")\n",
    "\n",
    "# Check if fast_inference is supported\n",
    "import inspect\n",
    "sig = inspect.signature(FastLanguageModel.from_pretrained)\n",
    "if 'fast_inference' in sig.parameters:\n",
    "    print(\"\\u2713 fast_inference parameter available\")\n",
    "else:\n",
    "    print(\"\\u26a0 fast_inference parameter not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Inference Testing (vLLM Backend)\n",
    "\n",
    "**Note:** `fast_inference=True` requires compatible vLLM/Unsloth versions. \n",
    "Current vLLM 0.14.0 has API changes that cause compatibility issues with Unsloth's LoRA manager.\n",
    "\n",
    "The test below verifies the parameter is available. Full fast_inference testing requires:\n",
    "- vLLM 0.10.2 - 0.11.2 (per TRL warning) or waiting for Unsloth update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fast Inference Capability Check ===\nâœ“ fast_inference parameter available: True\nâœ“ fast_inference in FastVisionModel: True\n\nCurrent versions:\n  vLLM: 0.14.0rc1.dev201+gadcf682fc\n  Unsloth: 2025.12.10\n\nâš  Note: fast_inference=True has vLLM 0.14.0 compatibility issues\n  Error: LoRA manager API mismatch - waiting for Unsloth update"
     ]
    }
   ],
   "source": [
    "# Verify fast_inference parameter exists and document current limitation\n",
    "print(\"=== Fast Inference Capability Check ===\")\n",
    "import inspect\n",
    "\n",
    "# Check FastLanguageModel\n",
    "sig = inspect.signature(FastLanguageModel.from_pretrained)\n",
    "has_fast_inference = 'fast_inference' in sig.parameters\n",
    "print(f\"âœ“ fast_inference parameter available: {has_fast_inference}\")\n",
    "\n",
    "# Check FastVisionModel  \n",
    "sig_vision = inspect.signature(FastVisionModel.from_pretrained)\n",
    "has_fast_inference_vision = 'fast_inference' in sig_vision.parameters\n",
    "print(f\"âœ“ fast_inference in FastVisionModel: {has_fast_inference_vision}\")\n",
    "\n",
    "# Document current versions\n",
    "print(f\"\\nCurrent versions:\")\n",
    "print(f\"  vLLM: {vllm.__version__}\")\n",
    "print(f\"  Unsloth: {unsloth.__version__}\")\n",
    "print(f\"\\nâš  Note: fast_inference=True has vLLM 0.14.0 compatibility issues\")\n",
    "print(f\"  Error: LoRA manager API mismatch - waiting for Unsloth update\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ministral VL (Vision) Training Verification\n",
    "\n",
    "This section tests the complete vision model fine-tuning pipeline:\n",
    "- FastVisionModel loading\n",
    "- LoRA adapter configuration\n",
    "- Dataset loading and formatting\n",
    "- SFTTrainer training loop\n",
    "- Inference after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ministral VL Model Loading ===",
      "==((====))==  Unsloth 2025.12.10: Fast Ministral3 patching. Transformers: 5.0.0rc1. vLLM: 0.14.0rc1.dev201+gadcf682fc.cu130.\n   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.568 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.9.1+cu130. CUDA: 8.9. CUDA Toolkit: 13.0. Triton: 3.5.1\n\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9674a42ae02245a7a146347c6721f01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/458 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ FastVisionModel loaded: Mistral3ForConditionalGeneration\nâœ“ Tokenizer: PixtralProcessor"
     ]
    }
   ],
   "source": [
    "# Test FastVisionModel loading with Ministral 3 VL\n",
    "print(\"=== Ministral VL Model Loading ===\")\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Ministral-3-3B-Instruct-2512\",\n",
    "    load_in_4bit=True,  # Use 4bit for memory efficiency in testing\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")\n",
    "print(f\"âœ“ FastVisionModel loaded: {type(model).__name__}\")\n",
    "print(f\"âœ“ Tokenizer: {type(tokenizer).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LoRA Configuration ===",
      "Unsloth: Making `model.base_model.model.model.vision_tower.transformer` require gradients",
      "âœ“ LoRA adapters applied successfully\nâœ“ Trainable parameters: 33,751,040"
     ]
    }
   ],
   "source": [
    "# Apply LoRA adapters for parameter-efficient fine-tuning\n",
    "print(\"=== LoRA Configuration ===\")\n",
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers=True,\n",
    "    finetune_language_layers=True,\n",
    "    finetune_attention_modules=True,\n",
    "    finetune_mlp_modules=True,\n",
    "    r=16,  # Reduced rank for testing\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    random_state=3407,\n",
    ")\n",
    "print(\"âœ“ LoRA adapters applied successfully\")\n",
    "print(f\"âœ“ Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Loading ==="
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7cf4f25446496390af04db92595daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/519 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf402fdbb5c444e87f41a96cb3d4a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/344M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f4512faed4a47fa8c54494d20667bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/38.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea03a2127494120a5377b024d4d017f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/68686 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56bffcb83e35464890f9c779274dd08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7632 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 5 samples from LaTeX_OCR dataset\nâœ“ Converted 5 samples to conversation format"
     ]
    }
   ],
   "source": [
    "# Load sample from real LaTeX OCR dataset\n",
    "print(\"=== Dataset Loading ===\")\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"unsloth/LaTeX_OCR\", split=\"train[:5]\")\n",
    "print(f\"âœ“ Loaded {len(dataset)} samples from LaTeX_OCR dataset\")\n",
    "\n",
    "# Format for vision training\n",
    "instruction = \"Write the LaTeX representation for this image.\"\n",
    "\n",
    "def convert_to_conversation(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": instruction},\n",
    "                {\"type\": \"image\", \"image\": sample[\"image\"]}\n",
    "            ]},\n",
    "            {\"role\": \"assistant\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": sample[\"text\"]}\n",
    "            ]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "converted_dataset = [convert_to_conversation(s) for s in dataset]\n",
    "print(f\"âœ“ Converted {len(converted_dataset)} samples to conversation format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Test (2 steps) ==="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 5 | Num Epochs = 1 | Total steps = 2\nO^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 2\n\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 2 x 1) = 2\n \"-____-\"     Trainable parameters = 33,751,040 of 3,882,841,088 (0.87% trained)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 : < :, Epoch 0.40/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Training completed successfully\n  Final loss: 3.0692602396011353"
     ]
    }
   ],
   "source": [
    "# Run 2 training steps to verify pipeline\n",
    "print(\"=== Training Test (2 steps) ===\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=UnslothVisionDataCollator(model, tokenizer),\n",
    "    train_dataset=converted_dataset,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=1,\n",
    "        max_steps=2,\n",
    "        warmup_steps=0,  # Use warmup_steps instead of deprecated warmup_ratio\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        fp16=not is_bf16_supported(),\n",
    "        bf16=is_bf16_supported(),\n",
    "        output_dir=\"outputs_ministral_vl_test\",\n",
    "        remove_unused_columns=False,\n",
    "        dataset_text_field=\"\",\n",
    "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "        max_seq_length=1024,\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()\n",
    "print(f\"âœ“ Training completed successfully\")\n",
    "print(f\"  Final loss: {trainer_stats.metrics.get('train_loss', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inference Test ===",
      "âœ“ Inference test passed\n  Sample output (last 100 chars): ...nÄ \\mathbb{Z},Ä \\quadÄ \\frac{M}{P}Ä \\inÄ \\mathbb{Z},Ä \\quadÄ \\frac{P}{Q}Ä \\inÄ \\mathbb{Z}ÄŠ\\end{equation*}ÄŠ```"
     ]
    }
   ],
   "source": [
    "# Test inference after training\n",
    "print(\"=== Inference Test ===\")\n",
    "FastVisionModel.for_inference(model)\n",
    "\n",
    "test_image = dataset[0][\"image\"]\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": instruction}\n",
    "    ]}\n",
    "]\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = tokenizer(test_image, input_text, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=64, temperature=1.5, min_p=0.1)\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"âœ“ Inference test passed\")\n",
    "print(f\"  Sample output (last 100 chars): ...{response[-100:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cleanup ===\nâœ“ Ministral VL verification complete - GPU memory released"
     ]
    }
   ],
   "source": [
    "# Cleanup GPU memory\n",
    "print(\"=== Cleanup ===\")\n",
    "del model, tokenizer, trainer, dataset, converted_dataset\n",
    "import gc; gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"âœ“ Ministral VL verification complete - GPU memory released\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification Summary\n",
    "\n",
    "If all cells above ran without errors, your environment is ready for:\n",
    "\n",
    "1. **Ministral_3_(3B)_Reinforcement_Learning_Sudoku_Game.ipynb**\n",
    "   - Uses: GRPOConfig, GRPOTrainer, FastLanguageModel\n",
    "   - Status: Import verification only\n",
    "\n",
    "2. **Ministral_3_VL_(3B)_Vision.ipynb**\n",
    "   - Uses: SFTTrainer, SFTConfig, FastVisionModel, UnslothVisionDataCollator\n",
    "   - Status: **Full pipeline tested** (model loading, LoRA, training, inference)\n",
    "\n",
    "### What Was Verified\n",
    "- Core imports (unsloth, transformers, vLLM, TRL, torch)\n",
    "- FastLanguageModel loading (Ministral-3-3B-Reasoning)\n",
    "- **fast_inference parameter available** (vLLM 0.14.0 compatibility issue noted)\n",
    "- FastVisionModel loading (Ministral-3-3B-Instruct)\n",
    "- LoRA adapter configuration\n",
    "- Dataset loading (LaTeX_OCR from HuggingFace)\n",
    "- SFTTrainer training loop (2 steps)\n",
    "- Post-training inference\n",
    "\n",
    "### Known Limitations\n",
    "- `fast_inference=True` has compatibility issues with vLLM 0.14.0\n",
    "- Waiting for Unsloth update to fix LoRA manager API mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown kernel\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
