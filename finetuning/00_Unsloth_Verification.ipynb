{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsloth Environment Verification\n",
    "\n",
    "This notebook verifies that all components are correctly installed for running Unsloth notebooks:\n",
    "- GRPO (Reinforcement Learning)\n",
    "- Vision fine-tuning (Ministral VL)\n",
    "- fast_inference support\n",
    "\n",
    "**Run this after rebuilding the jupyter pod to verify the environment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/pixi/.pixi/envs/default/lib/python3.13/site-packages/trl/__init__.py:203: UserWarning: TRL currently supports vLLM versions: 0.10.2, 0.11.0, 0.11.1, 0.11.2. You have version 0.14.0rc1.dev201+gadcf682fc.cu130 installed. We recommend installing a supported version to avoid compatibility issues.\n  if is_vllm_available():"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!",
      "âœ“ unsloth: 2025.12.10\nâœ“ transformers: 5.0.0rc1\nâœ“ vLLM: 0.14.0rc1.dev201+gadcf682fc\nâœ“ TRL: 0.26.2\nâœ“ PyTorch: 2.9.1+cu130\nâœ“ CUDA available: True\nâœ“ GPU: NVIDIA GeForce RTX 4080 SUPER"
     ]
    }
   ],
   "source": [
    "# CRITICAL: Import unsloth FIRST for proper TRL patching\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel, FastVisionModel\n",
    "print(f\"\\u2713 unsloth: {unsloth.__version__}\")\n",
    "\n",
    "import transformers\n",
    "print(f\"\\u2713 transformers: {transformers.__version__}\")\n",
    "\n",
    "import vllm\n",
    "print(f\"\\u2713 vLLM: {vllm.__version__}\")\n",
    "\n",
    "import trl\n",
    "print(f\"\\u2713 TRL: {trl.__version__}\")\n",
    "\n",
    "import torch\n",
    "print(f\"\\u2713 PyTorch: {torch.__version__}\")\n",
    "print(f\"\\u2713 CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\u2713 GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GRPO/RL Imports ===\nâœ“ All GRPO imports successful"
     ]
    }
   ],
   "source": [
    "# Test imports for Reinforcement Learning notebook\n",
    "print(\"=== GRPO/RL Imports ===\")\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from datasets import Dataset\n",
    "print(\"\\u2713 All GRPO imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vision/SFT Imports ===\nâœ“ All Vision imports successful"
     ]
    }
   ],
   "source": [
    "# Test imports for Vision notebook\n",
    "print(\"=== Vision/SFT Imports ===\")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from unsloth import is_bf16_supported\n",
    "from transformers import TextStreamer\n",
    "print(\"\\u2713 All Vision imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Model Loading ===",
      "==((====))==  Unsloth 2025.12.10: Fast Qwen2 patching. Transformers: 5.0.0.1. vLLM: 0.14.0rc1.dev201+gadcf682fc.cu130.\n   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.568 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.9.1+cu130. CUDA: 8.9. CUDA Toolkit: 13.0. Triton: 3.5.1\n\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a16775206d4d38bdf5e705d007eab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded: Qwen2ForCausalLM\nâœ“ Tokenizer: Qwen2Tokenizer\nâœ“ Cleanup complete"
     ]
    }
   ],
   "source": [
    "# Test model loading with FastLanguageModel\n",
    "print(\"=== Testing Model Loading ===\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"unsloth/Qwen2.5-0.5B\",  # Small model for quick test\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "print(f\"âœ“ Model loaded: {type(model).__name__}\")\n",
    "print(f\"âœ“ Tokenizer: {type(tokenizer).__name__}\")\n",
    "\n",
    "# Clean up\n",
    "del model, tokenizer\n",
    "import gc; gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"âœ“ Cleanup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fast Inference Check ===\nfast_inference=True uses vLLM as backend for 2x faster inference\nvLLM version: 0.14.0rc1.dev201+gadcf682fc\nUnsloth version: 2025.12.10\nâœ“ fast_inference parameter available"
     ]
    }
   ],
   "source": [
    "# Test fast_inference capability\n",
    "print(\"=== Fast Inference Check ===\")\n",
    "print(\"fast_inference=True uses vLLM as backend for 2x faster inference\")\n",
    "print(f\"vLLM version: {vllm.__version__}\")\n",
    "print(f\"Unsloth version: {unsloth.__version__}\")\n",
    "\n",
    "# Check if fast_inference is supported\n",
    "import inspect\n",
    "sig = inspect.signature(FastLanguageModel.from_pretrained)\n",
    "if 'fast_inference' in sig.parameters:\n",
    "    print(\"\\u2713 fast_inference parameter available\")\n",
    "else:\n",
    "    print(\"\\u26a0 fast_inference parameter not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification Summary\n",
    "\n",
    "If all cells above ran without errors, your environment is ready for:\n",
    "\n",
    "1. **Ministral_3_(3B)_Reinforcement_Learning_Sudoku_Game.ipynb**\n",
    "   - Uses: GRPOConfig, GRPOTrainer, FastLanguageModel\n",
    "\n",
    "2. **Ministral_3_VL_(3B)_Vision.ipynb**\n",
    "   - Uses: SFTTrainer, SFTConfig, FastVisionModel, UnslothVisionDataCollator\n",
    "\n",
    "### Verified Versions\n",
    "- unsloth: 2025.12.10\n",
    "- transformers: 5.0.0rc1\n",
    "- vLLM: 0.14.0\n",
    "- TRL: 0.26.2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
