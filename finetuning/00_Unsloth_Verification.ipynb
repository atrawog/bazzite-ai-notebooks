{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsloth Environment Verification\n",
    "\n",
    "This notebook verifies that all components are correctly installed for running Unsloth notebooks:\n",
    "- GRPO (Reinforcement Learning)\n",
    "- Vision fine-tuning (Ministral VL)\n",
    "- fast_inference support\n",
    "\n",
    "**Run this after rebuilding the jupyter pod to verify the environment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "âœ“ HF_TOKEN loaded: Yes\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "/opt/pixi/.pixi/envs/default/lib/python3.13/site-packages/trl/__init__.py:203: UserWarning: TRL currently supports vLLM versions: 0.10.2, 0.11.0, 0.11.1, 0.11.2. You have version 0.14.0rc1.dev201+gadcf682fc.cu130 installed. We recommend installing a supported version to avoid compatibility issues.\n",
       "  if is_vllm_available():\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "âœ“ unsloth: 2025.12.10\n",
       "âœ“ transformers: 5.0.0rc1\n",
       "âœ“ vLLM: 0.14.0rc1.dev201+gadcf682fc\n",
       "âœ“ TRL: 0.26.2\n",
       "âœ“ PyTorch: 2.9.1+cu130\n",
       "âœ“ CUDA available: True\n",
       "âœ“ GPU: NVIDIA GeForce RTX 4080 SUPER\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env from notebook directory\n",
    "load_dotenv()\n",
    "print(f\"âœ“ HF_TOKEN loaded: {'Yes' if os.environ.get('HF_TOKEN') else 'No'}\")\n",
    "\n",
    "# CRITICAL: Import unsloth FIRST for proper TRL patching\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel, FastVisionModel\n",
    "print(f\"âœ“ unsloth: {unsloth.__version__}\")\n",
    "\n",
    "import transformers\n",
    "print(f\"âœ“ transformers: {transformers.__version__}\")\n",
    "\n",
    "import vllm\n",
    "print(f\"âœ“ vLLM: {vllm.__version__}\")\n",
    "\n",
    "import trl\n",
    "print(f\"âœ“ TRL: {trl.__version__}\")\n",
    "\n",
    "import torch\n",
    "print(f\"âœ“ PyTorch: {torch.__version__}\")\n",
    "print(f\"âœ“ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== GRPO/RL Imports ===\n",
       "âœ“ All GRPO imports successful\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test imports for Reinforcement Learning notebook\n",
    "print(\"=== GRPO/RL Imports ===\")\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from datasets import Dataset\n",
    "print(\"\\u2713 All GRPO imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Vision/SFT Imports ===\n",
       "âœ“ All Vision imports successful\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test imports for Vision notebook\n",
    "print(\"=== Vision/SFT Imports ===\")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from unsloth import is_bf16_supported\n",
    "from transformers import TextStreamer\n",
    "print(\"\\u2713 All Vision imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Testing Model Loading ===\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==((====))==  Unsloth 2025.12.10: Fast Ministral3 patching. Transformers: 5.0.0rc1. vLLM: 0.14.0rc1.dev201+gadcf682fc.cu130.\n",
       "   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.568 GB. Platform: Linux.\n",
       "O^O/ \\_/ \\    Torch: 2.9.1+cu130. CUDA: 8.9. CUDA Toolkit: 13.0. Triton: 3.5.1\n",
       "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
       " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
       "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/458 [00:00<?, ?it/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "âœ“ Model loaded: Mistral3ForConditionalGeneration\n",
       "âœ“ Tokenizer: PixtralProcessor\n",
       "âœ“ Cleanup complete\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test model loading with FastLanguageModel\n",
    "print(\"=== Testing Model Loading ===\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"unsloth/Ministral-3-3B-Reasoning-2512\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "print(f\"âœ“ Model loaded: {type(model).__name__}\")\n",
    "print(f\"âœ“ Tokenizer: {type(tokenizer).__name__}\")\n",
    "\n",
    "# Clean up\n",
    "del model, tokenizer\n",
    "import gc; gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"âœ“ Cleanup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Fast Inference Check ===\n",
       "fast_inference=True uses vLLM as backend for 2x faster inference\n",
       "vLLM version: 0.14.0rc1.dev201+gadcf682fc\n",
       "Unsloth version: 2025.12.10\n",
       "âœ“ fast_inference parameter available\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test fast_inference capability\n",
    "print(\"=== Fast Inference Check ===\")\n",
    "print(\"fast_inference=True uses vLLM as backend for 2x faster inference\")\n",
    "print(f\"vLLM version: {vllm.__version__}\")\n",
    "print(f\"Unsloth version: {unsloth.__version__}\")\n",
    "\n",
    "# Check if fast_inference is supported\n",
    "import inspect\n",
    "sig = inspect.signature(FastLanguageModel.from_pretrained)\n",
    "if 'fast_inference' in sig.parameters:\n",
    "    print(\"\\u2713 fast_inference parameter available\")\n",
    "else:\n",
    "    print(\"\\u26a0 fast_inference parameter not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Inference Testing (vLLM Backend)\n",
    "\n",
    "**Note:** `fast_inference=True` requires compatible vLLM/Unsloth versions. \n",
    "Current vLLM 0.14.0 has API changes that cause compatibility issues with Unsloth's LoRA manager.\n",
    "\n",
    "The test below verifies the parameter is available. Full fast_inference testing requires:\n",
    "- vLLM 0.10.2 - 0.11.2 (per TRL warning) or waiting for Unsloth update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Fast Inference Test (vLLM Backend) ===\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "WARNING 01-02 19:42:36 [vllm.py:1427] Current vLLM config is not set.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:36 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=2048.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:36 [vllm.py:609] Disabling NCCL for DP synchronization when using async scheduling.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:36 [vllm.py:614] Asynchronous scheduling is enabled.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:36 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==((====))==  Unsloth 2025.12.10: Fast Llama patching. Transformers: 5.0.0.1. vLLM: 0.14.0rc1.dev201+gadcf682fc.cu130.\n",
       "   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.568 GB. Platform: Linux.\n",
       "O^O/ \\_/ \\    Torch: 2.9.1+cu130. CUDA: 8.9. CUDA Toolkit: 13.0. Triton: 3.5.1\n",
       "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
       " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
       "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Unsloth: vLLM loading unsloth/llama-3.2-1b-instruct-unsloth-bnb-4bit with actual GPU utilization = 34.46%\n",
       "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 15.57 GB.\n",
       "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 512. Num Sequences = 32.\n",
       "Unsloth: vLLM's KV Cache can use up to 4.22 GB. Also swap space = 6 GB.\n",
       "Unsloth: Not an error, but `use_cudagraph` is not supported in vLLM.config.CompilationConfig. Skipping.\n",
       "Unsloth: Not an error, but `use_inductor` is not supported in vLLM.config.CompilationConfig. Skipping.\n",
       "WARNING 01-02 19:42:42 [compilation.py:739] Level is deprecated and will be removed in the next release,either 0.12.0 or 0.11.2 whichever is soonest.Use mode instead.If both level and mode are given,only mode will be used.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
       "WARNING 01-02 19:42:42 [attention.py:82] Using VLLM_ATTENTION_BACKEND environment variable is deprecated and will be removed in v0.14.0 or v1.0.0, whichever is soonest. Please use --attention-config.backend command line argument or AttentionConfig(backend=...) config field instead.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:42 [utils.py:253] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'max_model_len': 512, 'enable_prefix_caching': True, 'swap_space': 6, 'gpu_memory_utilization': 0.34456733540082884, 'max_num_batched_tokens': 4096, 'max_num_seqs': 32, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {'level': 3, 'mode': 3, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'epilogue_fusion': True, 'max_autotune': False, 'shape_padding': True, 'trace.enabled': False, 'triton.cudagraphs': False, 'debug': False, 'dce': True, 'memory_planning': True, 'coordinate_descent_tuning': False, 'trace.graph_diagram': False, 'compile_threads': 16, 'group_fusion': True, 'disable_progress': False, 'verbose_progress': True, 'triton.multi_kernel': 0, 'triton.use_block_ptr': True, 'triton.enable_persistent_tma_matmul': True, 'triton.autotune_at_compile_time': False, 'triton.cooperative_reductions': False, 'cuda.compile_opt_level': '-O2', 'cuda.enable_cuda_lto': True, 'combo_kernels': False, 'benchmark_combo_kernel': True, 'combo_kernel_foreach_dynamic_shapes': True, 'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}, 'model': 'unsloth/llama-3.2-1b-instruct-unsloth-bnb-4bit'}\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "/opt/pixi/.pixi/envs/default/lib/python3.13/site-packages/pydantic/type_adapter.py:605: UserWarning: Pydantic serializer warnings:\n",
       "  PydanticSerializationUnexpectedValue(Expected `enum` - serialized value may not be as expected [field_name='mode', input_value=3, input_type=int])\n",
       "  return self.serializer.to_python(\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "WARNING 01-02 19:42:42 [arg_utils.py:1196] The global random seed is set to 0. Since VLLM_ENABLE_V1_MULTIPROCESSING is set to False, this may affect the random state of the Python process that launched vLLM.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:44 [model.py:517] Resolved architecture: LlamaForCausalLM\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:44 [model.py:1688] Using max model len 512\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "WARNING 01-02 19:42:44 [vllm.py:1427] Current vLLM config is not set.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:44 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=2048.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:44 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=4096.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.1.mlp'], 'llm_int8_threshold': 6.0}\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:46 [core.py:95] Initializing a V1 LLM engine (v0.14.0rc1.dev201+gadcf682fc) with config: model='unsloth/llama-3.2-1b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/llama-3.2-1b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=512, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False), seed=0, served_model_name=unsloth/llama-3.2-1b-instruct-unsloth-bnb-4bit, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': 3, 'mode': 3, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'epilogue_fusion': True, 'max_autotune': False, 'shape_padding': True, 'trace.enabled': False, 'triton.cudagraphs': False, 'debug': False, 'dce': True, 'memory_planning': True, 'coordinate_descent_tuning': False, 'trace.graph_diagram': False, 'compile_threads': 16, 'group_fusion': True, 'disable_progress': False, 'verbose_progress': True, 'triton.multi_kernel': 0, 'triton.use_block_ptr': True, 'triton.enable_persistent_tma_matmul': True, 'triton.autotune_at_compile_time': False, 'triton.cooperative_reductions': False, 'cuda.compile_opt_level': '-O2', 'cuda.enable_cuda_lto': True, 'combo_kernels': False, 'benchmark_combo_kernel': True, 'combo_kernel_foreach_dynamic_shapes': True, 'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 64, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:46 [parallel_state.py:1210] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.89.0.13:46379 backend=nccl\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:46 [parallel_state.py:1418] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "/opt/pixi/.pixi/envs/default/lib/python3.13/site-packages/pydantic/type_adapter.py:605: UserWarning: Pydantic serializer warnings:\n",
       "  PydanticSerializationUnexpectedValue(Expected `enum` - serialized value may not be as expected [field_name='mode', input_value=3, input_type=int])\n",
       "  return self.serializer.to_python(\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:47 [topk_topp_sampler.py:47] Using FlashInfer for top-p & top-k sampling.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:47 [gpu_model_runner.py:3762] Starting to load model unsloth/llama-3.2-1b-instruct-unsloth-bnb-4bit...\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:47 [cuda.py:315] Using AttentionBackendEnum.FLASHINFER backend.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:47 [bitsandbytes_loader.py:790] Loading weights with BitsAndBytes quantization. May take a while ...\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:49 [weight_utils.py:550] No model.safetensors.index.json found in remote.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:49 [punica_selector.py:20] Using PunicaWrapperGPU.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:50 [gpu_model_runner.py:3859] Model loading took 1.1606 GiB memory and 1.784368 seconds\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:52 [backends.py:644] Using cache directory: /workspace/.cache/vllm/torch_compile_cache/13a88f2397/rank_0_0/backbone for vLLM's torch.compile\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:52 [backends.py:704] Dynamo bytecode transform time: 2.29 s\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:53 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 4096) from the cache, took 0.197 s\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:53 [monitor.py:34] torch.compile takes 2.49 s in total\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:54 [gpu_worker.py:363] Available KV cache memory: 3.86 GiB\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:55 [kv_cache_utils.py:1305] GPU KV cache size: 126,592 tokens\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:55 [kv_cache_utils.py:1310] Maximum concurrency for 512 tokens per request: 247.25x\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:55 [kernel_warmup.py:64] Warming up FlashInfer attention.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:55 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "WARNING 01-02 19:42:55 [utils.py:256] Using default LoRA kernel configs\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|â–‰         | 2/22 [00:00<00:01, 19.19it/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 6/22 [00:00<00:00, 29.01it/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 11/22 [00:00<00:00, 34.05it/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 15/22 [00:00<00:00, 34.02it/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 19/22 [00:00<00:00, 35.02it/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:00<00:00, 33.69it/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "Capturing CUDA graphs (decode, FULL):   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "Capturing CUDA graphs (decode, FULL):  21%|â–ˆâ–ˆâ–       | 3/14 [00:00<00:00, 29.28it/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "Capturing CUDA graphs (decode, FULL):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 7/14 [00:00<00:00, 32.68it/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "Capturing CUDA graphs (decode, FULL):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 11/14 [00:00<00:00, 33.68it/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\r",
       "Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:00<00:00, 35.05it/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:56 [gpu_model_runner.py:4810] Graph capturing finished in 1 secs, took 0.31 GiB\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:56 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 1 secs.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:57 [core.py:272] init engine (profile, create kv cache, warmup model) took 7.36 seconds\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:57 [core.py:184] Batch queue is enabled with size 2\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "INFO 01-02 19:42:57 [llm.py:344] Supported tasks: ('generate',)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Unsloth: Just some info: will skip parsing ['post_layernorm', 'norm', 'post_attention_layernorm', 'layer_norm2', 'norm1', 'layer_norm1', 'norm2', 'input_layernorm', 'q_norm', 'ffn_norm', 'attention_norm', 'post_feedforward_layernorm', 'k_norm', 'pre_feedforward_layernorm']\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Performing substitution for additional_keys=set()\n",
       "Unsloth: Just some info: will skip parsing ['post_layernorm', 'cross_attn_input_layernorm', 'norm', 'post_attention_layernorm', 'layer_norm2', 'norm1', 'layer_norm1', 'norm2', 'input_layernorm', 'q_norm', 'cross_attn_post_attention_layernorm', 'ffn_norm', 'attention_norm', 'post_feedforward_layernorm', 'k_norm', 'pre_feedforward_layernorm']\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Unsloth: Will load unsloth/llama-3.2-1b-instruct-unsloth-bnb-4bit as a legacy tokenizer.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "âœ“ Model loaded with fast_inference=True\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "âœ“ vLLM generation completed in 0.02s\n",
       "  Response: Hello.\n",
       "âœ“ Fast inference test PASSED\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test fast_inference=True with vLLM backend (after patch)\n",
    "print(\"=== Fast Inference Test (vLLM Backend) ===\")\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "from vllm import SamplingParams\n",
    "import torch\n",
    "import time\n",
    "\n",
    "try:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        \"unsloth/Llama-3.2-1B-Instruct\",\n",
    "        max_seq_length=512,\n",
    "        load_in_4bit=True,\n",
    "        fast_inference=True,\n",
    "        gpu_memory_utilization=0.5,\n",
    "    )\n",
    "    print(\"âœ“ Model loaded with fast_inference=True\")\n",
    "\n",
    "    # Test generation using vLLM's API\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    messages = [{\"role\": \"user\", \"content\": \"Say hello in one word.\"}]\n",
    "    \n",
    "    # Format prompt as text for vLLM backend\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Create SamplingParams object (required by vLLM 0.14)\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.1,\n",
    "        max_tokens=10,\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    outputs = model.fast_generate([prompt], sampling_params=sampling_params)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    response = outputs[0].outputs[0].text\n",
    "    print(f\"âœ“ vLLM generation completed in {elapsed:.2f}s\")\n",
    "    print(f\"  Response: {response}\")\n",
    "\n",
    "    # Cleanup\n",
    "    del model, tokenizer\n",
    "    import gc; gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"âœ“ Fast inference test PASSED\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"âŒ Fast inference test FAILED: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Fast Inference Capability Check ===\n",
       "âœ“ fast_inference parameter available: True\n",
       "âœ“ fast_inference in FastVisionModel: True\n",
       "\n",
       "Current versions:\n",
       "  vLLM: 0.14.0rc1.dev201+gadcf682fc\n",
       "  Unsloth: 2025.12.10\n",
       "\n",
       "âš  Note: fast_inference=True has vLLM 0.14.0 compatibility issues\n",
       "  Error: LoRA manager API mismatch - waiting for Unsloth update\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify fast_inference parameter exists and confirm it works\n",
    "print(\"=== Fast Inference Capability Check ===\")\n",
    "import inspect\n",
    "\n",
    "# Check FastLanguageModel\n",
    "sig = inspect.signature(FastLanguageModel.from_pretrained)\n",
    "has_fast_inference = 'fast_inference' in sig.parameters\n",
    "print(f\"âœ“ fast_inference parameter available: {has_fast_inference}\")\n",
    "\n",
    "# Check FastVisionModel  \n",
    "sig_vision = inspect.signature(FastVisionModel.from_pretrained)\n",
    "has_fast_inference_vision = 'fast_inference' in sig_vision.parameters\n",
    "print(f\"âœ“ fast_inference in FastVisionModel: {has_fast_inference_vision}\")\n",
    "\n",
    "# Document current versions\n",
    "print(f\"\\nCurrent versions:\")\n",
    "print(f\"  vLLM: {vllm.__version__}\")\n",
    "print(f\"  Unsloth: {unsloth.__version__}\")\n",
    "print(f\"\\nâœ“ fast_inference=True works with vLLM 0.14.0 (patched)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ministral VL (Vision) Training Verification\n",
    "\n",
    "This section tests the complete vision model fine-tuning pipeline:\n",
    "- FastVisionModel loading\n",
    "- LoRA adapter configuration\n",
    "- Dataset loading and formatting\n",
    "- SFTTrainer training loop\n",
    "- Inference after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Ministral VL Model Loading ===\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==((====))==  Unsloth 2025.12.10: Fast Ministral3 patching. Transformers: 5.0.0rc1. vLLM: 0.14.0rc1.dev201+gadcf682fc.cu130.\n",
       "   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.568 GB. Platform: Linux.\n",
       "O^O/ \\_/ \\    Torch: 2.9.1+cu130. CUDA: 8.9. CUDA Toolkit: 13.0. Triton: 3.5.1\n",
       "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
       " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
       "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/458 [00:00<?, ?it/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "âœ“ FastVisionModel loaded: Mistral3ForConditionalGeneration\n",
       "âœ“ Tokenizer: PixtralProcessor\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test FastVisionModel loading with Ministral 3 VL\n",
    "print(\"=== Ministral VL Model Loading ===\")\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Ministral-3-3B-Instruct-2512\",\n",
    "    load_in_4bit=True,  # Use 4bit for memory efficiency in testing\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")\n",
    "print(f\"âœ“ FastVisionModel loaded: {type(model).__name__}\")\n",
    "print(f\"âœ“ Tokenizer: {type(tokenizer).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== LoRA Configuration ===\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Unsloth: Making `model.base_model.model.model.vision_tower.transformer` require gradients\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "âœ“ LoRA adapters applied successfully\n",
       "âœ“ Trainable parameters: 33,751,040\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply LoRA adapters for parameter-efficient fine-tuning\n",
    "print(\"=== LoRA Configuration ===\")\n",
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers=True,\n",
    "    finetune_language_layers=True,\n",
    "    finetune_attention_modules=True,\n",
    "    finetune_mlp_modules=True,\n",
    "    r=16,  # Reduced rank for testing\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    random_state=3407,\n",
    ")\n",
    "print(\"âœ“ LoRA adapters applied successfully\")\n",
    "print(f\"âœ“ Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Dataset Loading ===\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "âœ“ Loaded 5 samples from LaTeX_OCR dataset\n",
       "âœ“ Converted 5 samples to conversation format\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load sample from real LaTeX OCR dataset\n",
    "print(\"=== Dataset Loading ===\")\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"unsloth/LaTeX_OCR\", split=\"train[:5]\")\n",
    "print(f\"âœ“ Loaded {len(dataset)} samples from LaTeX_OCR dataset\")\n",
    "\n",
    "# Format for vision training\n",
    "instruction = \"Write the LaTeX representation for this image.\"\n",
    "\n",
    "def convert_to_conversation(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": instruction},\n",
    "                {\"type\": \"image\", \"image\": sample[\"image\"]}\n",
    "            ]},\n",
    "            {\"role\": \"assistant\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": sample[\"text\"]}\n",
    "            ]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "converted_dataset = [convert_to_conversation(s) for s in dataset]\n",
    "print(f\"âœ“ Converted {len(converted_dataset)} samples to conversation format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "=== Training Test (2 steps) ===\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
       "   \\\\   /|    Num examples = 5 | Num Epochs = 1 | Total steps = 2\n",
       "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 2\n",
       "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 2 x 1) = 2\n",
       " \"-____-\"     Trainable parameters = 33,751,040 of 3,882,841,088 (0.87% trained)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Unsloth: Will smartly offload gradients to save VRAM!\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "âœ“ Training completed successfully\n",
       "  Final loss: 3.0692602396011353\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run 2 training steps to verify pipeline\n",
    "print(\"=== Training Test (2 steps) ===\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=UnslothVisionDataCollator(model, tokenizer),\n",
    "    train_dataset=converted_dataset,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=1,\n",
    "        max_steps=2,\n",
    "        warmup_steps=0,  # Use warmup_steps instead of deprecated warmup_ratio\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        fp16=not is_bf16_supported(),\n",
    "        bf16=is_bf16_supported(),\n",
    "        output_dir=\"outputs_ministral_vl_test\",\n",
    "        remove_unused_columns=False,\n",
    "        dataset_text_field=\"\",\n",
    "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "        max_seq_length=1024,\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()\n",
    "print(f\"âœ“ Training completed successfully\")\n",
    "print(f\"  Final loss: {trainer_stats.metrics.get('train_loss', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Inference Test ===\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "âœ“ Inference test passed\n",
       "  Sample output (last 100 chars): ...nÄ \\mathbb{Z},Ä \\quadÄ \\frac{M}{P}Ä \\inÄ \\mathbb{Z},Ä \\quadÄ \\frac{P}{Q}Ä \\inÄ \\mathbb{Z}ÄŠ\\end{equation*}ÄŠ```\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test inference after training\n",
    "print(\"=== Inference Test ===\")\n",
    "FastVisionModel.for_inference(model)\n",
    "\n",
    "test_image = dataset[0][\"image\"]\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": instruction}\n",
    "    ]}\n",
    "]\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = tokenizer(test_image, input_text, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=64, temperature=1.5, min_p=0.1)\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"âœ“ Inference test passed\")\n",
    "print(f\"  Sample output (last 100 chars): ...{response[-100:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Cleanup ===\n",
       "âœ“ Ministral VL verification complete - GPU memory released\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleanup GPU memory\n",
    "print(\"=== Cleanup ===\")\n",
    "del model, tokenizer, trainer, dataset, converted_dataset\n",
    "import gc; gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"âœ“ Ministral VL verification complete - GPU memory released\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification Summary\n",
    "\n",
    "If all cells above ran without errors, your environment is ready for:\n",
    "\n",
    "1. **Ministral_3_(3B)_Reinforcement_Learning_Sudoku_Game.ipynb**\n",
    "   - Uses: GRPOConfig, GRPOTrainer, FastLanguageModel\n",
    "   - Status: Import verification only\n",
    "\n",
    "2. **Ministral_3_VL_(3B)_Vision.ipynb**\n",
    "   - Uses: SFTTrainer, SFTConfig, FastVisionModel, UnslothVisionDataCollator\n",
    "   - Status: **Full pipeline tested** (model loading, LoRA, training, inference)\n",
    "\n",
    "### What Was Verified\n",
    "- Core imports (unsloth, transformers, vLLM, TRL, torch)\n",
    "- FastLanguageModel loading (Ministral-3-3B-Reasoning)\n",
    "- **fast_inference parameter available** (vLLM 0.14.0 compatibility issue noted)\n",
    "- FastVisionModel loading (Ministral-3-3B-Instruct)\n",
    "- LoRA adapter configuration\n",
    "- Dataset loading (LaTeX_OCR from HuggingFace)\n",
    "- SFTTrainer training loop (2 steps)\n",
    "- Post-training inference\n",
    "\n",
    "### Known Limitations\n",
    "- `fast_inference=True` has compatibility issues with vLLM 0.14.0\n",
    "- Waiting for Unsloth update to fix LoRA manager API mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown kernel\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
