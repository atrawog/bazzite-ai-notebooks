{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fbd520f",
   "metadata": {},
   "source": [
    "# QLoRA Parameter Test: LoRA Rank Comparison - Ministral-3B-Reasoning\n",
    "\n",
    "Tests different LoRA rank values (r=4, 8, 16, 32, 64) to demonstrate parameter efficiency tradeoffs on Ministral-3B-Reasoning.\n",
    "\n",
    "**Key features tested:**\n",
    "- Side-by-side rank comparison\n",
    "- Trainable parameter counting\n",
    "- GPU memory usage measurement\n",
    "- Training loss comparison\n",
    "- Inference quality with `[THINK]` reasoning output\n",
    "\n",
    "**Expected outcomes:**\n",
    "- r=4: Minimal parameters (~8M) but may underfit\n",
    "- r=16: Sweet spot (~33M, ~1.5% trainable)\n",
    "- r=64: Highest capacity (~132M) but diminishing returns\n",
    "\n",
    "**Model:** Ministral-3B-Reasoning with `[THINK]...[/THINK]` reasoning format\n",
    "\n",
    "**Important:** This notebook includes a kernel shutdown cell at the end to release all GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d432b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# CRITICAL: Import unsloth FIRST for proper TRL patching\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel, is_bf16_supported\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Environment summary\n",
    "gpu = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"Environment: unsloth {unsloth.__version__}, PyTorch {torch.__version__}, {gpu}\")\n",
    "print(f\"HF_TOKEN loaded: {'Yes' if os.environ.get('HF_TOKEN') else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd20ba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Helper Functions\n",
    "import subprocess\n",
    "\n",
    "def measure_gpu_memory():\n",
    "    \"\"\"Measure current GPU memory usage in MB using nvidia-smi\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        return int(result.stdout.strip().split('\\n')[0])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable vs total parameters\"\"\"\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return {\n",
    "        \"trainable\": trainable,\n",
    "        \"total\": total,\n",
    "        \"pct\": 100 * trainable / total\n",
    "    }\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"Force garbage collection and clear CUDA cache\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "print(\"Benchmark functions defined.\")\n",
    "print(f\"Initial GPU memory: {measure_gpu_memory()} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef92b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create minimal synthetic instruction dataset with thinking content (5 samples)\n",
    "from datasets import Dataset\n",
    "\n",
    "# Same dataset across all ranks for fair comparison\n",
    "synthetic_data = [\n",
    "    {\n",
    "        \"instruction\": \"What is machine learning?\",\n",
    "        \"thinking\": \"What is the user asking here? They want to understand machine learning. What are the key concepts I should cover? It's a subset of AI... and it involves learning from data. How should I keep this accessible? Short and clear definition.\",\n",
    "        \"response\": \"Machine learning is a subset of artificial intelligence where computers learn patterns from data.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain Python in one sentence.\",\n",
    "        \"thinking\": \"One sentence only - what's most important about Python? Its readability and versatility are the defining features. How do I capture both in one sentence?\",\n",
    "        \"response\": \"Python is a high-level programming language known for its readability and versatility.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is a neural network?\",\n",
    "        \"thinking\": \"How do I explain neural networks simply? What's the core concept? They're inspired by biological neurons... they process information in layers. Should I mention deep learning? Maybe keep it basic for now.\",\n",
    "        \"response\": \"A neural network is a computational model inspired by biological neurons that processes information through connected layers.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Define supervised learning.\",\n",
    "        \"thinking\": \"What makes supervised learning 'supervised'? It's the labels! The data has known outputs. How do I explain this clearly? Focus on the training process with labeled data.\",\n",
    "        \"response\": \"Supervised learning is training a model on labeled data to predict outcomes for new inputs.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is gradient descent?\",\n",
    "        \"thinking\": \"This is a bit technical. What's the intuition behind gradient descent? It's like walking downhill to find the lowest point. The gradient tells us which direction is 'down'. Keep it conceptual.\",\n",
    "        \"response\": \"Gradient descent is an optimization algorithm that minimizes loss by iteratively adjusting parameters in the direction of steepest descent.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Synthetic dataset prepared: {len(synthetic_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910922de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Rank Comparison Loop\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "MODEL_NAME = \"unsloth/Ministral-3-3B-Reasoning-2512\"\n",
    "RANKS = [4, 8, 16, 32, 64]\n",
    "results = []\n",
    "\n",
    "for rank in RANKS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing LoRA rank r={rank}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Measure baseline memory\n",
    "    cleanup_memory()\n",
    "    mem_before = measure_gpu_memory()\n",
    "    \n",
    "    # Load fresh model\n",
    "    print(f\"Loading model...\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        max_seq_length=512,\n",
    "        load_in_4bit=True,\n",
    "        dtype=None,\n",
    "    )\n",
    "    \n",
    "    mem_after_load = measure_gpu_memory()\n",
    "    \n",
    "    # Apply LoRA with current rank (alpha = rank for fair comparison)\n",
    "    print(f\"Applying LoRA with r={rank}, alpha={rank}...\")\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=rank,\n",
    "        lora_alpha=rank,  # Keep alpha=r for fair comparison\n",
    "        lora_dropout=0,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    mem_after_lora = measure_gpu_memory()\n",
    "    params = count_parameters(model)\n",
    "    \n",
    "    print(f\"Trainable: {params['trainable']:,} ({params['pct']:.2f}%)\")\n",
    "    \n",
    "    # Format dataset with tokenizer - using Ministral's [THINK] format\n",
    "    def format_conversation(sample):\n",
    "        assistant_content = f\"[THINK]\\n{sample['thinking']}\\n[/THINK]\\n\\n{sample['response']}\"\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": sample[\"instruction\"]}]},\n",
    "            {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": assistant_content}]}\n",
    "        ]\n",
    "        return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}\n",
    "    \n",
    "    dataset = Dataset.from_list(synthetic_data)\n",
    "    dataset = dataset.map(format_conversation, remove_columns=[\"instruction\", \"thinking\", \"response\"])\n",
    "    \n",
    "    # Training config\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=f\"outputs_qlora_rank_ministral/rank_{rank}\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        max_steps=3,  # Minimal steps for testing\n",
    "        warmup_steps=1,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        fp16=not is_bf16_supported(),\n",
    "        bf16=is_bf16_supported(),\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        max_seq_length=512,\n",
    "        seed=42,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        args=sft_config,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"Training (3 steps)...\")\n",
    "    trainer_stats = trainer.train()\n",
    "    final_loss = trainer_stats.metrics.get('train_loss', 0)\n",
    "    \n",
    "    mem_after_train = measure_gpu_memory()\n",
    "    \n",
    "    # Quick inference test\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is deep learning?\"}]}]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.6,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"rank\": rank,\n",
    "        \"trainable_params\": params[\"trainable\"],\n",
    "        \"trainable_pct\": params[\"pct\"],\n",
    "        \"gpu_load_mb\": mem_after_load - mem_before,\n",
    "        \"gpu_lora_mb\": mem_after_lora - mem_after_load,\n",
    "        \"gpu_peak_mb\": mem_after_train,\n",
    "        \"final_loss\": final_loss,\n",
    "        \"sample_response\": response[:150],\n",
    "    })\n",
    "    \n",
    "    print(f\"Final loss: {final_loss:.4f}\")\n",
    "    print(f\"GPU peak: {mem_after_train} MB\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, tokenizer, trainer, dataset\n",
    "    cleanup_memory()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All ranks tested!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00986b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LoRA Rank Comparison Results\")\n",
    "print(\"=\"*60)\n",
    "print(df[[\"rank\", \"trainable_params\", \"trainable_pct\", \"gpu_peak_mb\", \"final_loss\"]].to_string(index=False))\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Trainable Parameters vs Rank\n",
    "axes[0, 0].plot(df['rank'], df['trainable_params'] / 1e6, 'b-o', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_title('Trainable Parameters vs LoRA Rank', fontsize=12)\n",
    "axes[0, 0].set_xlabel('LoRA Rank (r)')\n",
    "axes[0, 0].set_ylabel('Parameters (Millions)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_xticks(df['rank'])\n",
    "\n",
    "# Plot 2: Trainable Percentage vs Rank\n",
    "axes[0, 1].plot(df['rank'], df['trainable_pct'], 'g-o', linewidth=2, markersize=8)\n",
    "axes[0, 1].set_title('Trainable % vs LoRA Rank', fontsize=12)\n",
    "axes[0, 1].set_xlabel('LoRA Rank (r)')\n",
    "axes[0, 1].set_ylabel('% of Total Parameters')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_xticks(df['rank'])\n",
    "\n",
    "# Plot 3: GPU Memory vs Rank\n",
    "axes[1, 0].plot(df['rank'], df['gpu_peak_mb'], 'r-o', linewidth=2, markersize=8)\n",
    "axes[1, 0].set_title('Peak GPU Memory vs LoRA Rank', fontsize=12)\n",
    "axes[1, 0].set_xlabel('LoRA Rank (r)')\n",
    "axes[1, 0].set_ylabel('Memory (MB)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_xticks(df['rank'])\n",
    "\n",
    "# Plot 4: Final Loss vs Rank\n",
    "axes[1, 1].plot(df['rank'], df['final_loss'], 'm-o', linewidth=2, markersize=8)\n",
    "axes[1, 1].set_title('Final Training Loss vs LoRA Rank', fontsize=12)\n",
    "axes[1, 1].set_xlabel('LoRA Rank (r)')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_xticks(df['rank'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs_qlora_rank_ministral/rank_comparison.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to outputs_qlora_rank_ministral/rank_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeee0ec",
   "metadata": {},
   "source": [
    "## Analysis and Key Findings\n",
    "\n",
    "### Parameter Efficiency\n",
    "- **r=4**: ~8M trainable parameters (0.3%) - Minimal but may underfit on complex tasks\n",
    "- **r=8**: ~16M trainable parameters (0.6%) - Good for simple adaptations\n",
    "- **r=16**: ~33M trainable parameters (1.3%) - **Sweet spot** for most tasks\n",
    "- **r=32**: ~66M trainable parameters (2.6%) - Higher capacity\n",
    "- **r=64**: ~132M trainable parameters (5.2%) - Maximum capacity but diminishing returns\n",
    "\n",
    "### Memory Usage\n",
    "- Memory scales linearly with rank\n",
    "- GPU memory increase is proportional to adapter size\n",
    "- Even r=64 remains memory-efficient compared to full fine-tuning\n",
    "\n",
    "### Training Dynamics\n",
    "- Higher rank = more capacity but may need more data\n",
    "- Loss curves show convergence patterns vary by rank\n",
    "- r=16 often achieves good loss with reasonable parameters\n",
    "\n",
    "### Recommendation\n",
    "For Ministral-3B-Reasoning with QLoRA:\n",
    "- **General use**: r=16 (balanced efficiency and capacity)\n",
    "- **Memory-constrained**: r=8 (half the parameters of r=16)\n",
    "- **Maximum performance**: r=32 (if enough training data)\n",
    "\n",
    "### Model Notes\n",
    "- **Model:** Ministral-3B-Reasoning (3B parameters)\n",
    "- **Reasoning Format:** `[THINK]...[/THINK]` tags (native Ministral format)\n",
    "- Smaller model than Qwen-4B, so may see different scaling behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823f6b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown kernel to release all GPU memory\n",
    "import IPython\n",
    "print(\"Shutting down kernel to release GPU memory...\")\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
