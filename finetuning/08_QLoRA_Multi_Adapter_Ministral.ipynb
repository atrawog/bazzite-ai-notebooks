{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21bc2827",
   "metadata": {},
   "source": [
    "# QLoRA Advanced: Multi-Adapter Training - Ministral-3B-Reasoning\n",
    "\n",
    "Demonstrates training multiple task-specific LoRA adapters and switching between them at inference.\n",
    "\n",
    "**Tasks/Adapters:**\n",
    "1. **Technical Q&A** - Precise technical explanations\n",
    "2. **Creative Writing** - Imaginative, expressive responses\n",
    "3. **Code Explanation** - Code-focused analysis\n",
    "\n",
    "**Key features demonstrated:**\n",
    "- Training independent adapters on different datasets\n",
    "- Saving lightweight adapter weights (~33M params each)\n",
    "- Switching adapters at inference time\n",
    "- Same prompt â†’ different responses per adapter\n",
    "- `[THINK]` reasoning capability preserved across all adapters\n",
    "\n",
    "**Benefits of multi-adapter approach:**\n",
    "- Modular specialization without retraining base model\n",
    "- Hot-swap capabilities at runtime\n",
    "- Storage efficient (adapters are ~130MB each vs ~6GB base model)\n",
    "\n",
    "**Model:** Ministral-3B-Reasoning with `[THINK]...[/THINK]` reasoning format\n",
    "\n",
    "**Important:** This notebook includes a kernel shutdown cell at the end to release all GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05f6023b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "/opt/pixi/.pixi/envs/default/lib/python3.13/site-packages/trl/__init__.py:203: UserWarning: TRL currently supports vLLM versions: 0.10.2, 0.11.0, 0.11.1, 0.11.2. You have version 0.14.0rc1.dev201+gadcf682fc.cu130 installed. We recommend installing a supported version to avoid compatibility issues.\n",
       "  if is_vllm_available():\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Environment: unsloth 2025.12.10, PyTorch 2.9.1+cu130, NVIDIA GeForce RTX 4080 SUPER\n",
       "HF_TOKEN loaded: Yes\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Force text-based progress instead of HTML widgets\n",
    "os.environ[\"TQDM_NOTEBOOK\"] = \"false\"\n",
    "\n",
    "# CRITICAL: Import unsloth FIRST for proper TRL patching\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel, is_bf16_supported\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Environment summary\n",
    "gpu = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"Environment: unsloth {unsloth.__version__}, PyTorch {torch.__version__}, {gpu}\")\n",
    "print(f\"HF_TOKEN loaded: {'Yes' if os.environ.get('HF_TOKEN') else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eca8f4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Helper functions defined.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"Force garbage collection and clear CUDA cache\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable vs total parameters\"\"\"\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return {\"trainable\": trainable, \"total\": total, \"pct\": 100 * trainable / total}\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe589010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task datasets prepared:\n",
       "  - technical: 5 samples\n",
       "  - creative: 5 samples\n",
       "  - code: 5 samples\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task-Specific Datasets (5 samples each)\n",
    "from datasets import Dataset\n",
    "\n",
    "# Task 1: Technical Q&A - precise, factual responses\n",
    "technical_data = [\n",
    "    {\n",
    "        \"instruction\": \"What is a hash table?\",\n",
    "        \"thinking\": \"The user wants a technical explanation. What are the key aspects? It's a data structure... uses hashing for O(1) lookups... handles collisions. Keep it precise but accessible.\",\n",
    "        \"response\": \"A hash table is a data structure that maps keys to values using a hash function. It provides O(1) average-case time complexity for insertions, deletions, and lookups by computing an index from the key.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain TCP vs UDP.\",\n",
    "        \"thinking\": \"Compare two network protocols. TCP is reliable, connection-oriented. UDP is fast, connectionless. What are the key differences to highlight?\",\n",
    "        \"response\": \"TCP provides reliable, ordered delivery with connection establishment and error checking. UDP is connectionless and faster but unreliable. TCP is used for web traffic; UDP for streaming and gaming.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is Big O notation?\",\n",
    "        \"thinking\": \"Fundamental CS concept. It describes algorithmic complexity. How do I explain it simply? Focus on growth rate and worst-case analysis.\",\n",
    "        \"response\": \"Big O notation describes the upper bound of an algorithm's time or space complexity as input size grows. O(n) means linear growth, O(nÂ²) means quadratic, and O(1) means constant time.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is recursion?\",\n",
    "        \"thinking\": \"Classic programming concept. A function calling itself with a base case. Should I mention stack frames? Keep it conceptual first.\",\n",
    "        \"response\": \"Recursion is when a function calls itself to solve smaller instances of the same problem. It requires a base case to terminate and recursive cases that reduce toward the base case.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain database indexing.\",\n",
    "        \"thinking\": \"Database performance topic. Indexes speed up queries... B-trees are common. Trade-off with insert performance. Focus on the core concept.\",\n",
    "        \"response\": \"Database indexing creates data structures that speed up query operations by allowing the database to find rows without scanning entire tables. Indexes trade storage space and insert speed for faster reads.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Task 2: Creative Writing - imaginative, expressive responses\n",
    "creative_data = [\n",
    "    {\n",
    "        \"instruction\": \"Write a haiku about coding.\",\n",
    "        \"thinking\": \"Haiku: 5-7-5 syllables. Theme is coding. What imagery captures the experience? Bugs, late nights, the flow state... Let me craft something evocative.\",\n",
    "        \"response\": \"Lines of logic flow\\nBugs dance in the midnight glow\\nFixedâ€”the dawn says so\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Describe a sunset poetically.\",\n",
    "        \"thinking\": \"Poetry mode. What makes sunsets beautiful? Colors, transition, emotion. Use metaphor and sensory language.\",\n",
    "        \"response\": \"The sun dips low, painting the sky in watercolors of amber and rose. Day surrenders to twilight's embrace as the horizon glows like embers of a dying fire, beautiful in its farewell.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Create a metaphor for learning.\",\n",
    "        \"thinking\": \"What is learning like? A journey? Growing? Building? Something that captures the gradual, sometimes difficult process...\",\n",
    "        \"response\": \"Learning is climbing a mountain in fogâ€”each step reveals new terrain, the path unclear until you've walked it. The summit exists, but the journey shapes you more than arrival.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a short opening line for a mystery novel.\",\n",
    "        \"thinking\": \"Mystery opening needs intrigue. Something that raises questions. A hook that makes readers want more. Tension from the first sentence.\",\n",
    "        \"response\": \"The letter arrived three years after her death, written in handwriting that was unmistakably hers.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Describe rain in an unusual way.\",\n",
    "        \"thinking\": \"Avoid clichÃ©s. What's a fresh perspective on rain? Sound, feeling, unusual comparisons... Make it memorable.\",\n",
    "        \"response\": \"The sky plays percussion on the rooftopsâ€”a million tiny drummers performing chaos into rhythm, washing the world in liquid applause.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Task 3: Code Explanation - code-focused analysis\n",
    "code_data = [\n",
    "    {\n",
    "        \"instruction\": \"Explain: def fib(n): return n if n<2 else fib(n-1)+fib(n-2)\",\n",
    "        \"thinking\": \"Classic recursive Fibonacci. Base case n<2, recursive case adds previous two. What should I highlight? The elegance but also the O(2^n) complexity.\",\n",
    "        \"response\": \"This is a recursive Fibonacci implementation. Base case: returns n for 0 or 1. Recursive case: sums the two preceding values. Elegant but O(2^n) timeâ€”use memoization for efficiency.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What does [x*2 for x in range(5)] do?\",\n",
    "        \"thinking\": \"Python list comprehension. Creates a list of doubled values. range(5) gives 0-4. Result is [0,2,4,6,8]. Explain the syntax pattern.\",\n",
    "        \"response\": \"This list comprehension creates [0, 2, 4, 6, 8]. It iterates x through range(5) (0-4) and multiplies each by 2. Syntax: [expression for item in iterable].\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain Python decorators.\",\n",
    "        \"thinking\": \"Decorators wrap functions. They're higher-order functions. @syntax is syntactic sugar. Use cases: logging, timing, auth. Start with the concept.\",\n",
    "        \"response\": \"Decorators are functions that modify other functions. The @decorator syntax wraps a function, adding behavior before/after execution. Common uses: logging, authentication, caching, timing.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is async/await in Python?\",\n",
    "        \"thinking\": \"Asynchronous programming. Coroutines, event loop, non-blocking I/O. How do I explain this concisely? Focus on the why and basic usage.\",\n",
    "        \"response\": \"async/await enables non-blocking I/O. 'async def' creates coroutines; 'await' pauses execution until a task completes. The event loop manages concurrent operations without threads.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain: lambda x,y: x if x>y else y\",\n",
    "        \"thinking\": \"Lambda function returning the max of two values. Anonymous function, single expression. Equivalent to max() for two values.\",\n",
    "        \"response\": \"This lambda returns the larger of x and y using a ternary expression. It's equivalent to max(x,y). Lambda creates anonymous, single-expression functions.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "TASK_DATASETS = {\n",
    "    \"technical\": technical_data,\n",
    "    \"creative\": creative_data,\n",
    "    \"code\": code_data,\n",
    "}\n",
    "\n",
    "print(f\"Task datasets prepared:\")\n",
    "for name, data in TASK_DATASETS.items():\n",
    "    print(f\"  - {name}: {len(data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0172c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Multiple Adapters\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "MODEL_NAME = \"unsloth/Ministral-3-3B-Reasoning-2512\"\n",
    "OUTPUT_BASE = \"outputs_qlora_multi_adapter_ministral\"\n",
    "adapter_paths = {}\n",
    "\n",
    "for task_name, task_data in TASK_DATASETS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training adapter: {task_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    cleanup_memory()\n",
    "    \n",
    "    # Load fresh model\n",
    "    print(f\"Loading model...\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        max_seq_length=512,\n",
    "        load_in_4bit=True,\n",
    "        dtype=None,\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA\n",
    "    print(f\"Applying LoRA...\")\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    params = count_parameters(model)\n",
    "    print(f\"Trainable: {params['trainable']:,} ({params['pct']:.2f}%)\")\n",
    "    \n",
    "    # Format dataset - using Ministral's [THINK] format\n",
    "    def format_conversation(sample):\n",
    "        assistant_content = f\"[THINK]\\n{sample['thinking']}\\n[/THINK]\\n\\n{sample['response']}\"\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": sample[\"instruction\"]}]},\n",
    "            {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": assistant_content}]}\n",
    "        ]\n",
    "        return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}\n",
    "    \n",
    "    dataset = Dataset.from_list(task_data)\n",
    "    dataset = dataset.map(format_conversation, remove_columns=[\"instruction\", \"thinking\", \"response\"])\n",
    "    \n",
    "    # Training config\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=f\"{OUTPUT_BASE}/{task_name}_adapter\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        max_steps=5,  # Few steps for each task\n",
    "        warmup_steps=1,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        fp16=not is_bf16_supported(),\n",
    "        bf16=is_bf16_supported(),\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        max_seq_length=512,\n",
    "        seed=42,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        args=sft_config,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"Training (5 steps)...\")\n",
    "    trainer_stats = trainer.train()\n",
    "    final_loss = trainer_stats.metrics.get('train_loss', 0)\n",
    "    print(f\"Final loss: {final_loss:.4f}\")\n",
    "    \n",
    "    # Save adapter\n",
    "    adapter_path = f\"{OUTPUT_BASE}/{task_name}_adapter\"\n",
    "    model.save_pretrained(adapter_path)\n",
    "    tokenizer.save_pretrained(adapter_path)\n",
    "    adapter_paths[task_name] = adapter_path\n",
    "    print(f\"Adapter saved to: {adapter_path}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, tokenizer, trainer, dataset\n",
    "    cleanup_memory()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All adapters trained and saved!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"\\nAdapter paths:\")\n",
    "for name, path in adapter_paths.items():\n",
    "    print(f\"  - {name}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4943de0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================\n",
       "Adapter Switching Demonstration\n",
       "============================================================\n",
       "\n",
       "Same prompt with different adapters:\n",
       "Prompt: 'Explain what a variable is.'\n",
       "============================================================\n",
       "\n",
       "Loading base model...\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==((====))==  Unsloth 2025.12.10: Fast Ministral3 patching. Transformers: 5.0.0rc1. vLLM: 0.14.0rc1.dev201+gadcf682fc.cu130.\n",
       "   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.568 GB. Platform: Linux.\n",
       "O^O/ \\_/ \\    Torch: 2.9.1+cu130. CUDA: 8.9. CUDA Toolkit: 13.0. Triton: 3.5.1\n",
       "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
       " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
       "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/458 [00:00<?, ?it/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "--- Loading technical adapter ---\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Response preview: Alright,Ä IÄ needÄ toÄ explainÄ whatÄ aÄ variableÄ is.Ä LetÄ meÄ startÄ byÄ recallingÄ theÄ basicÄ concept.Ä AÄ variableÄ isÄ aÄ pieceÄ ofÄ dataÄ thatÄ canÄ changeÄ orÄ beÄ modified.Ä ButÄ howÄ shouldÄ IÄ defineÄ it?ÄŠÄŠFirst,Ä perhapsÄ IÄ ...\n",
       "\n",
       "--- Loading creative adapter ---\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "/opt/pixi/.pixi/envs/default/lib/python3.13/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
       "  warnings.warn(\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Response preview: Okay,Ä IÄ needÄ toÄ explainÄ whatÄ aÄ variableÄ is.Ä IÄ recallÄ thatÄ inÄ programming,Ä aÄ variableÄ isÄ aÄ storageÄ locationÄ inÄ aÄ computer'sÄ memoryÄ thatÄ canÄ holdÄ data.Ä ButÄ maybeÄ IÄ shouldÄ makeÄ sureÄ toÄ explainÄ thisÄ inÄ aÄ ...\n",
       "\n",
       "--- Loading code adapter ---\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Response preview: Okay,Ä soÄ theÄ userÄ isÄ askingÄ aboutÄ whatÄ aÄ variableÄ is.Ä IÄ needÄ toÄ explainÄ theÄ conceptÄ ofÄ aÄ variableÄ inÄ aÄ simpleÄ way.Ä LetÄ meÄ thinkÄ aboutÄ howÄ toÄ describeÄ it.ÄŠÄŠAÄ variableÄ isÄ aÄ nameÄ forÄ aÄ storageÄ locationÄ i...\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "============================================================\n",
       "Demonstration complete!\n",
       "============================================================\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adapter Switching Demonstration\n",
    "from peft import PeftModel\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Adapter Switching Demonstration\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nSame prompt with different adapters:\")\n",
    "print(\"Prompt: 'Explain what a variable is.'\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "TEST_PROMPT = \"Explain what a variable is.\"\n",
    "results = {}\n",
    "\n",
    "# Load base model once\n",
    "cleanup_memory()\n",
    "print(\"\\nLoading base model...\")\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "# Use text tokenizer directly for text-only inference (Pixtral uses a processor)\n",
    "text_tokenizer = tokenizer.tokenizer if hasattr(tokenizer, 'tokenizer') else tokenizer\n",
    "\n",
    "for task_name, adapter_path in adapter_paths.items():\n",
    "    print(f\"\\n--- Loading {task_name} adapter ---\")\n",
    "    \n",
    "    # Load adapter on base model\n",
    "    adapted_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    FastLanguageModel.for_inference(adapted_model)\n",
    "    \n",
    "    # Generate response - using Ministral multimodal format\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": TEST_PROMPT}]}]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = text_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = adapted_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.6,\n",
    "            do_sample=True,\n",
    "            pad_token_id=text_tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    response = text_tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    results[task_name] = response\n",
    "    \n",
    "    print(f\"Response preview: {response[:200]}...\")\n",
    "    \n",
    "    # Unload adapter\n",
    "    del adapted_model\n",
    "    cleanup_memory()\n",
    "\n",
    "# Cleanup base model\n",
    "del base_model, tokenizer\n",
    "cleanup_memory()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Demonstration complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44b96fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================\n",
       "Response Comparison: Same Prompt, Different Adapters\n",
       "============================================================\n",
       "\n",
       "Prompt: 'Explain what a variable is.'\n",
       "\n",
       "\n",
       "[TECHNICAL ADAPTER]\n",
       "----------------------------------------\n",
       "Alright,Ä IÄ needÄ toÄ explainÄ whatÄ aÄ variableÄ is.Ä LetÄ meÄ startÄ byÄ recallingÄ theÄ basicÄ concept.Ä AÄ variableÄ isÄ aÄ pieceÄ ofÄ dataÄ thatÄ canÄ changeÄ orÄ beÄ modified.Ä ButÄ howÄ shouldÄ IÄ defineÄ it?ÄŠÄŠFirst,Ä perhapsÄ IÄ shouldÄ defineÄ itÄ inÄ termsÄ ofÄ programming,Ä sinceÄ theÄ questionÄ seemsÄ toÄ beÄ aboutÄ thatÄ context.ÄŠÄŠInÄ programming,Ä aÄ variableÄ isÄ aÄ namedÄ storageÄ locationÄ inÄ memoryÄ thatÄ canÄ holdÄ data.Ä ItÄ hasÄ aÄ name,Ä aÄ dataÄ type,Ä andÄ canÄ beÄ assignedÄ aÄ value.Ä ForÄ example,Ä inÄ manyÄ programmingÄ languages,Ä youÄ canÄ haveÄ anÄ integer,Ä float,Ä orÄ stringÄ variable.ÄŠÄŠButÄ theÄ questionÄ isÄ general,Ä soÄ maybeÄ IÄ shouldÄ explainÄ itÄ inÄ aÄ moreÄ fundamentalÄ way.ÄŠÄŠInÄ mathematics,Ä aÄ variableÄ isÄ aÄ symbolÄ thatÄ representsÄ anÄ unknownÄ or\n",
       "\n",
       "\n",
       "[CREATIVE ADAPTER]\n",
       "----------------------------------------\n",
       "Okay,Ä IÄ needÄ toÄ explainÄ whatÄ aÄ variableÄ is.Ä IÄ recallÄ thatÄ inÄ programming,Ä aÄ variableÄ isÄ aÄ storageÄ locationÄ inÄ aÄ computer'sÄ memoryÄ thatÄ canÄ holdÄ data.Ä ButÄ maybeÄ IÄ shouldÄ makeÄ sureÄ toÄ explainÄ thisÄ inÄ aÄ wayÄ thatÄ isÄ clearÄ andÄ concise.ÄŠÄŠLetÄ meÄ thinkÄ ofÄ aÄ simpleÄ analogy.Ä MaybeÄ IÄ canÄ sayÄ thatÄ aÄ variableÄ isÄ likeÄ aÄ labeledÄ boxÄ inÄ aÄ drawerÄ thatÄ canÄ holdÄ aÄ specificÄ typeÄ ofÄ item,Ä likeÄ aÄ penÄ orÄ aÄ book,Ä dependingÄ onÄ whatÄ youÄ putÄ inÄ it.ÄŠÄŠSo,Ä howÄ wouldÄ IÄ phraseÄ this?AÄ variableÄ isÄ aÄ namedÄ storageÄ locationÄ inÄ aÄ computer'sÄ memoryÄ thatÄ holdsÄ dataÄ ofÄ aÄ specificÄ type.Ä ForÄ example,Ä inÄ programming,Ä youÄ mightÄ haveÄ aÄ variableÄ namedÄ \"age\"Ä thatÄ canÄ storeÄ anÄ integerÄ representingÄ a\n",
       "\n",
       "\n",
       "[CODE ADAPTER]\n",
       "----------------------------------------\n",
       "Okay,Ä soÄ theÄ userÄ isÄ askingÄ aboutÄ whatÄ aÄ variableÄ is.Ä IÄ needÄ toÄ explainÄ theÄ conceptÄ ofÄ aÄ variableÄ inÄ aÄ simpleÄ way.Ä LetÄ meÄ thinkÄ aboutÄ howÄ toÄ describeÄ it.ÄŠÄŠAÄ variableÄ isÄ aÄ nameÄ forÄ aÄ storageÄ locationÄ inÄ aÄ computer'sÄ memoryÄ thatÄ holdsÄ data.Ä ItÄ canÄ beÄ assignedÄ aÄ valueÄ andÄ canÄ beÄ changedÄ asÄ needed.Ä ForÄ example,Ä inÄ programming,Ä aÄ variableÄ likeÄ 'x'Ä canÄ storeÄ aÄ numberÄ andÄ weÄ canÄ changeÄ itsÄ valueÄ toÄ somethingÄ else.ÄŠÄŠLetÄ meÄ draftÄ theÄ response.AÄ variableÄ isÄ aÄ nameÄ forÄ aÄ storageÄ locationÄ inÄ aÄ computer'sÄ memoryÄ thatÄ holdsÄ data.Ä ItÄ canÄ beÄ assignedÄ aÄ valueÄ andÄ canÄ beÄ changedÄ asÄ needed.Ä ForÄ example,Ä inÄ programming,Ä aÄ variableÄ likeÄ 'x'Ä canÄ storeÄ aÄ numberÄ andÄ we\n",
       "\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare Responses Side by Side\n",
    "print(\"=\"*60)\n",
    "print(\"Response Comparison: Same Prompt, Different Adapters\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPrompt: '{TEST_PROMPT}'\\n\")\n",
    "\n",
    "for task_name, response in results.items():\n",
    "    print(f\"\\n[{task_name.upper()} ADAPTER]\")\n",
    "    print(\"-\" * 40)\n",
    "    # Show full response\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394bf15a",
   "metadata": {},
   "source": [
    "## Analysis and Key Findings\n",
    "\n",
    "### Multi-Adapter Benefits\n",
    "\n",
    "1. **Modularity**: Each adapter specializes in a specific task\n",
    "2. **Efficiency**: Adapters are ~130MB each vs ~6GB base model\n",
    "3. **Flexibility**: Hot-swap adapters at runtime\n",
    "4. **Preservation**: Base model capabilities remain intact\n",
    "\n",
    "### Response Style Differences\n",
    "\n",
    "| Adapter | Expected Style |\n",
    "|---------|---------------|\n",
    "| Technical | Precise, factual, structured |\n",
    "| Creative | Expressive, metaphorical, imaginative |\n",
    "| Code | Code-focused, syntax-aware, practical |\n",
    "\n",
    "### Reasoning Preservation\n",
    "\n",
    "All adapters should maintain the `[THINK]...[/THINK]` reasoning pattern from the base Ministral-3B-Reasoning model, but the thinking content may reflect the adapter's specialization.\n",
    "\n",
    "### Production Usage\n",
    "\n",
    "```python\n",
    "# Load base model once\n",
    "base_model = load_model()\n",
    "\n",
    "# Switch adapters based on task\n",
    "if user_task == \"technical\":\n",
    "    model = PeftModel.from_pretrained(base_model, \"technical_adapter\")\n",
    "elif user_task == \"creative\":\n",
    "    model = PeftModel.from_pretrained(base_model, \"creative_adapter\")\n",
    "# etc.\n",
    "```\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "Multi-adapter training enables creating specialized \"modes\" for an LLM without the overhead of multiple full models. The same base model can serve different use cases by loading the appropriate adapter.\n",
    "\n",
    "### Model Notes\n",
    "- **Model:** Ministral-3B-Reasoning (3B parameters)\n",
    "- **Reasoning Format:** `[THINK]...[/THINK]` tags (native Ministral format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f30a0aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Shutting down kernel to release GPU memory...\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': False}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shutdown kernel to release all GPU memory\n",
    "import IPython\n",
    "print(\"Shutting down kernel to release GPU memory...\")\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}