{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21bc2827",
   "metadata": {},
   "source": [
    "# QLoRA Advanced: Multi-Adapter Training - Ministral-3B-Reasoning\n",
    "\n",
    "Demonstrates training multiple task-specific LoRA adapters and switching between them at inference.\n",
    "\n",
    "**Tasks/Adapters:**\n",
    "1. **Technical Q&A** - Precise technical explanations\n",
    "2. **Creative Writing** - Imaginative, expressive responses\n",
    "3. **Code Explanation** - Code-focused analysis\n",
    "\n",
    "**Key features demonstrated:**\n",
    "- Training independent adapters on different datasets\n",
    "- Saving lightweight adapter weights (~33M params each)\n",
    "- Switching adapters at inference time\n",
    "- Same prompt → different responses per adapter\n",
    "- `[THINK]` reasoning capability preserved across all adapters\n",
    "\n",
    "**Benefits of multi-adapter approach:**\n",
    "- Modular specialization without retraining base model\n",
    "- Hot-swap capabilities at runtime\n",
    "- Storage efficient (adapters are ~130MB each vs ~6GB base model)\n",
    "\n",
    "**Model:** Ministral-3B-Reasoning with `[THINK]...[/THINK]` reasoning format\n",
    "\n",
    "**Important:** This notebook includes a kernel shutdown cell at the end to release all GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# CRITICAL: Import unsloth FIRST for proper TRL patching\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel, is_bf16_supported\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Environment summary\n",
    "gpu = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"Environment: unsloth {unsloth.__version__}, PyTorch {torch.__version__}, {gpu}\")\n",
    "print(f\"HF_TOKEN loaded: {'Yes' if os.environ.get('HF_TOKEN') else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca8f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"Force garbage collection and clear CUDA cache\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable vs total parameters\"\"\"\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return {\"trainable\": trainable, \"total\": total, \"pct\": 100 * trainable / total}\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe589010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-Specific Datasets (5 samples each)\n",
    "from datasets import Dataset\n",
    "\n",
    "# Task 1: Technical Q&A - precise, factual responses\n",
    "technical_data = [\n",
    "    {\n",
    "        \"instruction\": \"What is a hash table?\",\n",
    "        \"thinking\": \"The user wants a technical explanation. What are the key aspects? It's a data structure... uses hashing for O(1) lookups... handles collisions. Keep it precise but accessible.\",\n",
    "        \"response\": \"A hash table is a data structure that maps keys to values using a hash function. It provides O(1) average-case time complexity for insertions, deletions, and lookups by computing an index from the key.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain TCP vs UDP.\",\n",
    "        \"thinking\": \"Compare two network protocols. TCP is reliable, connection-oriented. UDP is fast, connectionless. What are the key differences to highlight?\",\n",
    "        \"response\": \"TCP provides reliable, ordered delivery with connection establishment and error checking. UDP is connectionless and faster but unreliable. TCP is used for web traffic; UDP for streaming and gaming.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is Big O notation?\",\n",
    "        \"thinking\": \"Fundamental CS concept. It describes algorithmic complexity. How do I explain it simply? Focus on growth rate and worst-case analysis.\",\n",
    "        \"response\": \"Big O notation describes the upper bound of an algorithm's time or space complexity as input size grows. O(n) means linear growth, O(n²) means quadratic, and O(1) means constant time.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is recursion?\",\n",
    "        \"thinking\": \"Classic programming concept. A function calling itself with a base case. Should I mention stack frames? Keep it conceptual first.\",\n",
    "        \"response\": \"Recursion is when a function calls itself to solve smaller instances of the same problem. It requires a base case to terminate and recursive cases that reduce toward the base case.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain database indexing.\",\n",
    "        \"thinking\": \"Database performance topic. Indexes speed up queries... B-trees are common. Trade-off with insert performance. Focus on the core concept.\",\n",
    "        \"response\": \"Database indexing creates data structures that speed up query operations by allowing the database to find rows without scanning entire tables. Indexes trade storage space and insert speed for faster reads.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Task 2: Creative Writing - imaginative, expressive responses\n",
    "creative_data = [\n",
    "    {\n",
    "        \"instruction\": \"Write a haiku about coding.\",\n",
    "        \"thinking\": \"Haiku: 5-7-5 syllables. Theme is coding. What imagery captures the experience? Bugs, late nights, the flow state... Let me craft something evocative.\",\n",
    "        \"response\": \"Lines of logic flow\\nBugs dance in the midnight glow\\nFixed—the dawn says so\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Describe a sunset poetically.\",\n",
    "        \"thinking\": \"Poetry mode. What makes sunsets beautiful? Colors, transition, emotion. Use metaphor and sensory language.\",\n",
    "        \"response\": \"The sun dips low, painting the sky in watercolors of amber and rose. Day surrenders to twilight's embrace as the horizon glows like embers of a dying fire, beautiful in its farewell.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Create a metaphor for learning.\",\n",
    "        \"thinking\": \"What is learning like? A journey? Growing? Building? Something that captures the gradual, sometimes difficult process...\",\n",
    "        \"response\": \"Learning is climbing a mountain in fog—each step reveals new terrain, the path unclear until you've walked it. The summit exists, but the journey shapes you more than arrival.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a short opening line for a mystery novel.\",\n",
    "        \"thinking\": \"Mystery opening needs intrigue. Something that raises questions. A hook that makes readers want more. Tension from the first sentence.\",\n",
    "        \"response\": \"The letter arrived three years after her death, written in handwriting that was unmistakably hers.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Describe rain in an unusual way.\",\n",
    "        \"thinking\": \"Avoid clichés. What's a fresh perspective on rain? Sound, feeling, unusual comparisons... Make it memorable.\",\n",
    "        \"response\": \"The sky plays percussion on the rooftops—a million tiny drummers performing chaos into rhythm, washing the world in liquid applause.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Task 3: Code Explanation - code-focused analysis\n",
    "code_data = [\n",
    "    {\n",
    "        \"instruction\": \"Explain: def fib(n): return n if n<2 else fib(n-1)+fib(n-2)\",\n",
    "        \"thinking\": \"Classic recursive Fibonacci. Base case n<2, recursive case adds previous two. What should I highlight? The elegance but also the O(2^n) complexity.\",\n",
    "        \"response\": \"This is a recursive Fibonacci implementation. Base case: returns n for 0 or 1. Recursive case: sums the two preceding values. Elegant but O(2^n) time—use memoization for efficiency.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What does [x*2 for x in range(5)] do?\",\n",
    "        \"thinking\": \"Python list comprehension. Creates a list of doubled values. range(5) gives 0-4. Result is [0,2,4,6,8]. Explain the syntax pattern.\",\n",
    "        \"response\": \"This list comprehension creates [0, 2, 4, 6, 8]. It iterates x through range(5) (0-4) and multiplies each by 2. Syntax: [expression for item in iterable].\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain Python decorators.\",\n",
    "        \"thinking\": \"Decorators wrap functions. They're higher-order functions. @syntax is syntactic sugar. Use cases: logging, timing, auth. Start with the concept.\",\n",
    "        \"response\": \"Decorators are functions that modify other functions. The @decorator syntax wraps a function, adding behavior before/after execution. Common uses: logging, authentication, caching, timing.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is async/await in Python?\",\n",
    "        \"thinking\": \"Asynchronous programming. Coroutines, event loop, non-blocking I/O. How do I explain this concisely? Focus on the why and basic usage.\",\n",
    "        \"response\": \"async/await enables non-blocking I/O. 'async def' creates coroutines; 'await' pauses execution until a task completes. The event loop manages concurrent operations without threads.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain: lambda x,y: x if x>y else y\",\n",
    "        \"thinking\": \"Lambda function returning the max of two values. Anonymous function, single expression. Equivalent to max() for two values.\",\n",
    "        \"response\": \"This lambda returns the larger of x and y using a ternary expression. It's equivalent to max(x,y). Lambda creates anonymous, single-expression functions.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "TASK_DATASETS = {\n",
    "    \"technical\": technical_data,\n",
    "    \"creative\": creative_data,\n",
    "    \"code\": code_data,\n",
    "}\n",
    "\n",
    "print(f\"Task datasets prepared:\")\n",
    "for name, data in TASK_DATASETS.items():\n",
    "    print(f\"  - {name}: {len(data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0172c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Multiple Adapters\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "MODEL_NAME = \"unsloth/Ministral-3-3B-Reasoning-2512\"\n",
    "OUTPUT_BASE = \"outputs_qlora_multi_adapter_ministral\"\n",
    "adapter_paths = {}\n",
    "\n",
    "for task_name, task_data in TASK_DATASETS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training adapter: {task_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    cleanup_memory()\n",
    "    \n",
    "    # Load fresh model\n",
    "    print(f\"Loading model...\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        max_seq_length=512,\n",
    "        load_in_4bit=True,\n",
    "        dtype=None,\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA\n",
    "    print(f\"Applying LoRA...\")\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    params = count_parameters(model)\n",
    "    print(f\"Trainable: {params['trainable']:,} ({params['pct']:.2f}%)\")\n",
    "    \n",
    "    # Format dataset - using Ministral's [THINK] format\n",
    "    def format_conversation(sample):\n",
    "        assistant_content = f\"[THINK]\\n{sample['thinking']}\\n[/THINK]\\n\\n{sample['response']}\"\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": sample[\"instruction\"]}]},\n",
    "            {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": assistant_content}]}\n",
    "        ]\n",
    "        return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}\n",
    "    \n",
    "    dataset = Dataset.from_list(task_data)\n",
    "    dataset = dataset.map(format_conversation, remove_columns=[\"instruction\", \"thinking\", \"response\"])\n",
    "    \n",
    "    # Training config\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=f\"{OUTPUT_BASE}/{task_name}_adapter\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        max_steps=5,  # Few steps for each task\n",
    "        warmup_steps=1,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        fp16=not is_bf16_supported(),\n",
    "        bf16=is_bf16_supported(),\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        max_seq_length=512,\n",
    "        seed=42,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        args=sft_config,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"Training (5 steps)...\")\n",
    "    trainer_stats = trainer.train()\n",
    "    final_loss = trainer_stats.metrics.get('train_loss', 0)\n",
    "    print(f\"Final loss: {final_loss:.4f}\")\n",
    "    \n",
    "    # Save adapter\n",
    "    adapter_path = f\"{OUTPUT_BASE}/{task_name}_adapter\"\n",
    "    model.save_pretrained(adapter_path)\n",
    "    tokenizer.save_pretrained(adapter_path)\n",
    "    adapter_paths[task_name] = adapter_path\n",
    "    print(f\"Adapter saved to: {adapter_path}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, tokenizer, trainer, dataset\n",
    "    cleanup_memory()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All adapters trained and saved!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"\\nAdapter paths:\")\n",
    "for name, path in adapter_paths.items():\n",
    "    print(f\"  - {name}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4943de0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapter Switching Demonstration\n",
    "from peft import PeftModel\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Adapter Switching Demonstration\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nSame prompt with different adapters:\")\n",
    "print(\"Prompt: 'Explain what a variable is.'\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "TEST_PROMPT = \"Explain what a variable is.\"\n",
    "results = {}\n",
    "\n",
    "# Load base model once\n",
    "cleanup_memory()\n",
    "print(\"\\nLoading base model...\")\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "for task_name, adapter_path in adapter_paths.items():\n",
    "    print(f\"\\n--- Loading {task_name} adapter ---\")\n",
    "    \n",
    "    # Load adapter on base model\n",
    "    adapted_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    FastLanguageModel.for_inference(adapted_model)\n",
    "    \n",
    "    # Generate response - using Ministral multimodal format\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": TEST_PROMPT}]}]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = adapted_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.6,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    results[task_name] = response\n",
    "    \n",
    "    print(f\"Response preview: {response[:200]}...\")\n",
    "    \n",
    "    # Unload adapter\n",
    "    del adapted_model\n",
    "    cleanup_memory()\n",
    "\n",
    "# Cleanup base model\n",
    "del base_model, tokenizer\n",
    "cleanup_memory()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Demonstration complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b96fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Responses Side by Side\n",
    "print(\"=\"*60)\n",
    "print(\"Response Comparison: Same Prompt, Different Adapters\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPrompt: '{TEST_PROMPT}'\\n\")\n",
    "\n",
    "for task_name, response in results.items():\n",
    "    print(f\"\\n[{task_name.upper()} ADAPTER]\")\n",
    "    print(\"-\" * 40)\n",
    "    # Show full response\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394bf15a",
   "metadata": {},
   "source": [
    "## Analysis and Key Findings\n",
    "\n",
    "### Multi-Adapter Benefits\n",
    "\n",
    "1. **Modularity**: Each adapter specializes in a specific task\n",
    "2. **Efficiency**: Adapters are ~130MB each vs ~6GB base model\n",
    "3. **Flexibility**: Hot-swap adapters at runtime\n",
    "4. **Preservation**: Base model capabilities remain intact\n",
    "\n",
    "### Response Style Differences\n",
    "\n",
    "| Adapter | Expected Style |\n",
    "|---------|---------------|\n",
    "| Technical | Precise, factual, structured |\n",
    "| Creative | Expressive, metaphorical, imaginative |\n",
    "| Code | Code-focused, syntax-aware, practical |\n",
    "\n",
    "### Reasoning Preservation\n",
    "\n",
    "All adapters should maintain the `[THINK]...[/THINK]` reasoning pattern from the base Ministral-3B-Reasoning model, but the thinking content may reflect the adapter's specialization.\n",
    "\n",
    "### Production Usage\n",
    "\n",
    "```python\n",
    "# Load base model once\n",
    "base_model = load_model()\n",
    "\n",
    "# Switch adapters based on task\n",
    "if user_task == \"technical\":\n",
    "    model = PeftModel.from_pretrained(base_model, \"technical_adapter\")\n",
    "elif user_task == \"creative\":\n",
    "    model = PeftModel.from_pretrained(base_model, \"creative_adapter\")\n",
    "# etc.\n",
    "```\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "Multi-adapter training enables creating specialized \"modes\" for an LLM without the overhead of multiple full models. The same base model can serve different use cases by loading the appropriate adapter.\n",
    "\n",
    "### Model Notes\n",
    "- **Model:** Ministral-3B-Reasoning (3B parameters)\n",
    "- **Reasoning Format:** `[THINK]...[/THINK]` tags (native Ministral format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30a0aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown kernel to release all GPU memory\n",
    "import IPython\n",
    "print(\"Shutting down kernel to release GPU memory...\")\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
