{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e525c82",
   "metadata": {},
   "source": [
    "## Prompt Templates, Few-Shot Learning & Output Parsing\n",
    "\n",
    "This notebook demonstrates how to use LangChain's Prompt Templates, few-shot prompting techniques, and structured output parsing with a local open-source language model.\n",
    "\n",
    "We will use the instruction-tuned model `\"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\"` throughout, and explore:\n",
    "- Prompt templates for reusability and clarity\n",
    "- Few-shot prompting to guide the model with examples\n",
    "- Structured output parsing using Pydantic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72092751",
   "metadata": {},
   "source": [
    "> **Bazzite-AI Setup Required**  \n",
    "> Run `D0_00_Bazzite_AI_Setup.ipynb` first to configure Ollama and verify GPU access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2898a88",
   "metadata": {},
   "source": [
    "### Prompt Templates in LangChain\n",
    "\n",
    "LangChain’s `PromptTemplate` lets you define reusable prompt structures with placeholders for dynamic input.\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain the following topic in simple terms:\n",
    "\n",
    "{topic}\"\n",
    ")\n",
    "\n",
    "print(prompt.format(topic=\"What is machine learning?\"))\n",
    "```\n",
    "\n",
    "This is helpful for keeping prompts clean and consistent across inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c536fc1d-12aa-4840-a76a-3c3b3c1985aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain_huggingface import ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, AIMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import json\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3f9421c4-ffbc-48dc-9a3a-743d4d5bccfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download model from HuggingFace (same base model as D1_01)\n",
    "HF_LLM_MODEL = \"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "897f3e73-dbf1-4432-bc20-cbe0747e81fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Device set to use cuda:0\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4-bit quantization config for efficient loading\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_LLM_MODEL)\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_LLM_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "# Create text generation pipeline\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    return_full_text=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d14290b0-57bd-4e1c-8574-88ac922aa886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Explain the following topic in simple terms:\n",
       "\n",
       "What is machine learning?\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain the following topic in simple terms:\\n\\n{topic}\"\n",
    ")\n",
    "\n",
    "print(prompt_template.format(topic=\"What is machine learning?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7f4e2ec8-cd92-48fe-98b2-a5f09b1476cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Machine learning is a branch of artificial intelligence (AI) that uses algorithms to learn from and make decisions based on data, without being explicitly programmed.\n",
       "\n",
       "In simple terms, machine learning is a way for computers to learn from experience and improve their performance over time.\n",
       "\n",
       "How does machine learning work?\n",
       "\n",
       "Machine learning works by analyzing large amounts of data and identifying patterns and relationships within the data. This analysis is used to create a model that can be used to make predictions or decisions based on new data.\n",
       "\n",
       "Machine learning algorithms can be broken down into two main categories: supervised and unsupervised learning.\n",
       "\n",
       "In supervised learning, the algorithm is given a set of input data along with the correct output for that data. The algorithm then learns to map the input to the output and can use that mapping to make predictions on new data.\n",
       "\n",
       "In unsupervised learning, the algorithm is given a set of input data but no corresponding output. The algorithm then tries to find patterns and relationships within the data on its own.\n",
       "\n",
       "What are some examples of machine learning in action?\n",
       "\n",
       "Machine learning is used in a wide variety of applications and industries. Some examples of machine learning in action include:\n",
       "\n",
       "- Personalized recommendations on platforms like Netflix and Amazon, which use machine learning algorithms to analyze user data and suggest items they might be interested in.\n",
       "- Fraud detection in banking, where machine learning algorithms can analyze transaction data to identify suspicious activity and flag potential fraud.\n",
       "- Medical diagnosis, where machine learning algorithms can analyze patient data to identify patterns that may indicate certain medical conditions.\n",
       "- Self-driving cars, where machine learning algorithms are used to analyze sensor data and make decisions about how to safely navigate the road.\n",
       "\n",
       "What are the benefits of machine learning?\n",
       "\n",
       "Machine learning has a number of benefits, including:\n",
       "\n",
       "- Improved accuracy: Machine learning algorithms can analyze large amounts of data and identify patterns that may be difficult for humans to detect, leading to more accurate predictions and decisions.\n",
       "- Increased efficiency: Machine learning algorithms can automate repetitive tasks, freeing up time and resources for other activities.\n",
       "- Better insights: By analyzing large amounts of data, machine learning algorithms can provide insights that may not be immediately apparent to humans.\n",
       "- Adaptability: Machine learning algorithms can learn from new data and improve their performance over time, making them adaptable to changing circumstances.\n",
       "\n",
       "What are the challenges of machine learning?\n",
       "\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(prompt_template.format(topic=\"What is machine learning?\"))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a88950-cbb6-45bd-b61f-1486d56ecddf",
   "metadata": {},
   "source": [
    "<br>\n",
    "Let's have a look at another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5803b574-debc-4b74-b33b-496680a81d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplify_prompt = PromptTemplate(\n",
    "    input_variables=[\"clause\"],\n",
    "    template=\"\"\"\n",
    "You are a legal assistant that simplifies complex legal clauses into plain, understandable English.\n",
    "\n",
    "Clause:\n",
    "{clause}\n",
    "\n",
    "Simplified Explanation:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dd9ffdbc-cbd6-49c4-82cd-33fe8b266187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Simplified:\n",
       " The person renting the property agrees to protect the property owner from any legal responsibility, losses, or claims that happen because of how the property is used, but only if the problem isn't because of serious negligence.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legal_clause = (\n",
    "    \"The lessee shall indemnify and hold harmless the lessor from any liabilities, damages, \"\n",
    "    \"or claims arising out of the use of the premises, except in cases of gross negligence.\"\n",
    ")\n",
    "\n",
    "formatted_prompt = simplify_prompt.format(clause=legal_clause)\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "print(\"Simplified:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "21496ae9-638e-422f-9819-d1c4058e59b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama host: http://ollama:11434\n",
       "Model: hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ollama configuration (no API key needed!)\n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\n",
    "\n",
    "# === Model Configuration ===\n",
    "HF_LLM_MODEL = \"NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF\"\n",
    "OLLAMA_LLM_MODEL = f\"hf.co/{HF_LLM_MODEL}:Q4_K_M\"\n",
    "\n",
    "print(f\"Ollama host: {OLLAMA_HOST}\")\n",
    "print(f\"Model: {OLLAMA_LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3962e891-f109-4579-b470-ca1fe4536c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Use Ollama as OpenAI-compatible endpoint (no API key required)\n",
    "llm_ollama = ChatOpenAI(\n",
    "    base_url=f\"{OLLAMA_HOST}/v1\",\n",
    "    api_key=\"ollama\",  # Ollama ignores this but LangChain requires it\n",
    "    model=OLLAMA_LLM_MODEL,\n",
    "    temperature=0.7,\n",
    "    max_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1b80cddd-fca7-4236-9f17-61e59f3254a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Simplified:\n",
       " content='The person renting the property (lessee) promises to protect and not hold responsible the property owner (lessor) for any problems or costs that happen because the place is being used, unless those issues are due to very serious mistakes.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 95, 'total_tokens': 144, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-188', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b616e-2eaf-7ef1-b94d-8f2bf4f30593-0' usage_metadata={'input_tokens': 95, 'output_tokens': 49, 'total_tokens': 144, 'input_token_details': {}, 'output_token_details': {}}\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm_ollama.invoke(formatted_prompt)\n",
    "\n",
    "print(\"Simplified:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a117a35-c3a6-4bfd-b46e-fc144a4683ac",
   "metadata": {},
   "source": [
    "### Few Shot Prompt Template\n",
    "\n",
    "Let's go over an example where you want a historical conversation to show the LLM Chat Bot a few examples, known as \"Few Shot Prompts\". We essentially provide some examples *before* sending the message history to the LLM. Be careful not to make the entire message too long, as you may hit context limits (but the latest models have quite large contexsts.\n",
    "\n",
    "LangChain distinguishes between:\n",
    "\n",
    " - PromptTemplates for simple string prompts\n",
    " - MessagePromptTemplates for structured chat-style prompts using roles like system, user, assistant\n",
    "\n",
    "So SystemMessagePromptTemplate helps build structured prompts that work with ChatModels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5810213-483b-44fe-8d0a-55d20b7e06e9",
   "metadata": {},
   "source": [
    "**Creating Example Inputs and Outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "be9ac832-93de-4653-9912-1cd585c86063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"You are a helpful assistant that translates complex legal terms into plain and understandable language.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b3c0aace-77d1-4ca4-9f51-7c1145a37fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legal_text_1 = \"Notwithstanding any provision to the contrary herein, the indemnitor agrees to indemnify, defend, and hold harmless the indemnitee from and against any and all claims, liabilities, damages, or expenses (including, without limitation, reasonable attorney’s fees) arising out of or related to the indemnitor’s acts or omissions, except to the extent that such claims, liabilities, damages, or expenses result from the gross negligence or willful misconduct of the indemnitee.\"\n",
    "example_input_1 = HumanMessagePromptTemplate.from_template(legal_text_1)\n",
    "\n",
    "plain_text_1 = \"One party agrees to cover any costs, claims, or damages that happen because of their actions, including legal fees. However, they do not have to pay if the other party was extremely careless or acted intentionally wrong.\"\n",
    "example_output_1 = AIMessagePromptTemplate.from_template(plain_text_1)\n",
    "\n",
    "legal_text_2 = \"This agreement shall be binding upon and inure to the benefit of the parties hereto and their respective heirs, executors, administrators, successors, and assigns, and shall not be assignable by either party without the prior written consent of the other, except that either party may assign its rights and obligations hereunder in connection with a merger, consolidation, or sale of substantially all of its assets.\"\n",
    "example_input_2 = HumanMessagePromptTemplate.from_template(legal_text_2)\n",
    "\n",
    "plain_text_2 = \"This agreement applies to both parties and their future representatives, such as heirs or business successors. Neither party can transfer their rights under this agreement to someone else unless they get written permission. However, if one party merges with another company or sells most of its assets, they can transfer their rights without permission.\"\n",
    "example_output_2 = AIMessagePromptTemplate.from_template(plain_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7c651577-c2d3-4a43-9edb-125119579da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_template = \"{legal_text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4842eb22-7da0-4293-93af-f41f60bde6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, example_input_1, example_output_1, example_input_2, example_output_2, human_message_prompt]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d3430799-dd3f-4dad-a739-253778b5b316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_example_text = \"Any waiver of any term or condition of this agreement shall not be deemed a continuing waiver of such term or condition, nor shall it be considered a waiver of any other term or condition hereof. No failure or delay by either party in exercising any right, power, or privilege under this agreement shall operate as a waiver thereof, nor shall any single or partial exercise preclude any other or further exercise thereof or the exercise of any other right, power, or privilege.\"\n",
    "request = chat_prompt.format_prompt(legal_text=some_example_text).to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4521c9dd-cb8d-4257-9165-da15048534f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm_ollama.invoke(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1368bca7-f36e-4e2d-b752-28db75b94b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content=\"A waiver of a part of this agreement does not mean that the same thing won't be waived again in the future, and it doesn't apply to other parts of the agreement. If one party doesn't use their rights, it doesn't mean they can't use them later, and they can still use other rights too.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 479, 'total_tokens': 553, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-657', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b616e-9b07-7e83-a666-97d91e6da042-0' usage_metadata={'input_tokens': 479, 'output_tokens': 74, 'total_tokens': 553, 'input_token_details': {}, 'output_token_details': {}}\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41256428-52a2-4def-9282-2ebeffcdf2ad",
   "metadata": {},
   "source": [
    "### Parsing output\n",
    "\n",
    "Large language models (LLMs) typically generate free-form text, which is great for human conversation — but not ideal when we want to **extract specific information** or **automate downstream tasks**.\n",
    "\n",
    "---\n",
    "\n",
    "**The Problem**\n",
    "Imagine asking an LLM:\n",
    "\n",
    "> \"Summarize this contract and give me the parties involved, the start date, and any penalties.\"\n",
    "\n",
    "If the model responds with a long paragraph, it becomes difficult to:\n",
    "- Reliably extract the pieces you need\n",
    "- Validate whether the answer is complete\n",
    "- Feed the output into another system\n",
    "\n",
    "---\n",
    "\n",
    "**The Solution**\n",
    "Structured Output (e.g. JSON)\n",
    "By instructing the LLM to return data in a structured format like JSON, we can:\n",
    "- Parse the output automatically, although this does not always work\n",
    "- Validate that required fields are present\n",
    "- Integrate with other tools and code seamlessly\n",
    "\n",
    "---\n",
    "\n",
    "Structured output turns the LLM into a more reliable component of your application.  \n",
    "Parsing with tools like Pydantic ensures your data is clean, complete, and ready for automation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4d10e4-903a-4725-a968-c4c6b91e1a92",
   "metadata": {},
   "source": [
    "**Define format**\n",
    "Let's first see if we can get the output in form of a JSON object, by adding that request to the system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "61b0df47-38b6-405b-aa43-428dfd4f83e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"You are a helpful assistant that translates complex legal terms into plain and understandable language.  Respond only with a JSON object containing a single key 'translation' and its corresponding value.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cde1bbce-46b8-4b9a-84be-06691593e0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, example_input_1, example_output_1, example_input_2, example_output_2, human_message_prompt]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0481b8f3-7fdc-489e-abda-50c594cde391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_example_text = \"Any waiver of any term or condition of this agreement shall not be deemed a continuing waiver of such term or condition, nor shall it be considered a waiver of any other term or condition hereof. No failure or delay by either party in exercising any right, power, or privilege under this agreement shall operate as a waiver thereof, nor shall any single or partial exercise preclude any other or further exercise thereof or the exercise of any other right, power, or privilege.\"\n",
    "request = chat_prompt.format_prompt(legal_text=some_example_text).to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f3da3153-3911-4c0e-9201-7f4b8503b98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm_ollama.invoke(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7c484399-9980-44a0-9fe4-229d7b71f6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Any change to an agreement's terms is not considered a permanent waiver and doesn't affect other conditions. Also, not using a right or delaying its use does not mean it is waived. This doesn't prevent the use of other rights or powers within the agreement.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 60, 'prompt_tokens': 499, 'total_tokens': 559, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-53', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b616f-0ec6-7f52-a224-4e05ca9e4ad9-0', usage_metadata={'input_tokens': 499, 'output_tokens': 60, 'total_tokens': 559, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c68b1-4d80-44bc-b30c-1970bfd38b50",
   "metadata": {},
   "source": [
    "That clearly didn't do the trick.\n",
    "\n",
    "#### Pydantic\n",
    "\n",
    "[Pydantic](https://docs.pydantic.dev/) is a Python library for defining data models with validation. With LangChain, it allows you to:\n",
    "- Define the structure you expect from the model\n",
    "- Automatically parse the raw LLM output\n",
    "- Catch errors if fields are missing or malformed\n",
    "\n",
    "---\n",
    "\n",
    "**Example**\n",
    "\n",
    "1. Define a Pydantic model\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class ClauseSummary(BaseModel):\n",
    "    parties: List[str]\n",
    "    start_date: str\n",
    "    penalty_clause: str\n",
    "```\n",
    "\n",
    "This defines the structure we want the LLM to return — a JSON object with:\n",
    "- A list of `parties`\n",
    "- A `start_date`\n",
    "- A `penalty_clause` string\n",
    "\n",
    "---\n",
    "\n",
    "2. Set up a parser using LangChain\n",
    "\n",
    "```python\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=ClauseSummary)\n",
    "```\n",
    "\n",
    "This parser will take a raw string (from the LLM) and try to convert it into a `ClauseSummary` object.\n",
    "\n",
    "---\n",
    "\n",
    "3. Include the schema in the system prompt\n",
    "\n",
    "```python\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"clause\", \"format_instructions\"],\n",
    "    template=\"\"\"\n",
    "Extract the following fields from the contract clause below and return them in **valid JSON format ONLY**, with no extra text or explanation.\n",
    "\n",
    "Clause:\n",
    "{clause}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    ")\n",
    "```\n",
    "\n",
    "> The `format_instructions` tells the LLM exactly what JSON structure to return, based on your Pydantic model.\n",
    "\n",
    "---\n",
    "\n",
    "4. Run the LLM and parse the output\n",
    "\n",
    "```python\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "try:\n",
    "    parsed = parser.parse(response.content)\n",
    "    print(parsed.dict())\n",
    "except Exception as e:\n",
    "    print(\"Could not parse output.\")\n",
    "    print(\"Raw response:\", response.content)\n",
    "    print(e)\n",
    "```\n",
    "\n",
    "If the model returns a correctly structured JSON string, you now get a real Python object with attributes you can use:\n",
    "```python\n",
    "parsed.parties\n",
    "parsed.start_date\n",
    "parsed.penalty_clause\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "With this model, you can ensure the LLM responds in a way that fits your expected format — or fail gracefully when it doesn't.<br>\n",
    "Let's try out this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1236a684-3400-4633-8113-adb00f23bba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define output structure\n",
    "class ClauseSummary(BaseModel):\n",
    "    parties: List[str]\n",
    "    start_date: str\n",
    "    penalty_clause: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "54ab9521-9fa6-4867-9827-7dc530172660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up parser\n",
    "parser = PydanticOutputParser(pydantic_object=ClauseSummary)\n",
    "format_instructions = parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "49bb724a-2656-4e01-b5aa-3416c9ecaae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create prompt with correct input variables\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"clause\", \"format_instructions\"],\n",
    "    template=\"\"\"\n",
    "Extract the following fields from the contract clause below and return them in **valid JSON format ONLY**, with no extra text or explanation.\n",
    "\n",
    "Clause:\n",
    "{clause}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ebf4027a-ddc3-45d7-b3b4-f20f78813133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clause to parse\n",
    "clause_text = (\n",
    "    \"The agreement between Acme Corp and Beta LLC begins on January 1, 2025. \"\n",
    "    \"If either party breaks the agreement, a €5,000 penalty applies.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e09afd49-512e-450e-8802-567171dd4ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format the full prompt\n",
    "full_prompt = prompt.format(clause=clause_text, format_instructions=format_instructions)\n",
    "\n",
    "# Run the model\n",
    "response = llm_ollama.invoke(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c8db3a38-a04b-4ed6-aa4b-4cff153e670d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'parties': ['Acme Corp', 'Beta LLC'], 'start_date': 'January 1, 2025', 'penalty_clause': '€5,000 penalty'}\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse the response\n",
    "try:\n",
    "    parsed = parser.parse(response.content)\n",
    "    print(parsed.model_dump())\n",
    "except Exception as e:\n",
    "    print(\"Could not parse output.\")\n",
    "    print(\"Raw response:\", response)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "36fb97fa-5db3-43fe-97ec-f8fa1efa70da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Acme Corp', 'Beta LLC']\n",
       "January 1, 2025\n",
       "€5,000 penalty\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(parsed.parties)\n",
    "print(parsed.start_date)\n",
    "print(parsed.penalty_clause)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a9bc0f-527c-47c1-a958-31d0544e1343",
   "metadata": {},
   "source": [
    "<br>\n",
    "Let's try that on the example which tried to simplify legal clauses and output them in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "35952ebc-c6dc-4616-86af-649a2360baa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define output schema with Pydantic \n",
    "class LegalSimplification(BaseModel):\n",
    "    translation: str\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=LegalSimplification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5b075cd6-5d7b-4369-88bf-cef23a78deb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the system prompt\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "system_message = SystemMessage(content=f\"\"\"You are a helpful assistant that translates complex legal terms into plain and understandable language.\n",
    "Respond only in this format: {format_instructions}\n",
    "Do not ask for clarification. Always use the given legal input.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3072eaa7-fd00-4284-b737-ef6f1a16df63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define few-shot examples\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Notwithstanding any provision to the contrary herein, the indemnitor agrees to indemnify, defend, and hold harmless the indemnitee...\",\n",
    "        \"output\": \"One party agrees to cover any costs, claims, or damages that happen because of their actions...\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"This agreement shall be binding upon and inure to the benefit of the parties...\",\n",
    "        \"output\": \"This agreement applies to both parties and their future representatives...\"\n",
    "    }\n",
    "]\n",
    "\n",
    "few_shot_messages = []\n",
    "for ex in examples:\n",
    "    few_shot_messages.append(HumanMessage(content=ex[\"input\"]))\n",
    "    few_shot_messages.append(AIMessage(content=f'{{\"translation\": \"{ex[\"output\"]}\"}}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c78e204f-1955-4655-af6f-7504f1370ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the legal input text\n",
    "legal_text = (\n",
    "    \"Any waiver of any term or condition of this agreement shall not be deemed a continuing waiver of such term \"\n",
    "    \"or condition, nor shall it be considered a waiver of any other term or condition hereof. No failure or delay \"\n",
    "    \"by either party in exercising any right, power, or privilege under this agreement shall operate as a waiver \"\n",
    "    \"thereof, nor shall any single or partial exercise preclude any other or further exercise thereof or the \"\n",
    "    \"exercise of any other right, power, or privilege.\"\n",
    ")\n",
    "\n",
    "user_message = HumanMessage(content=legal_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2ba67b1b-35c1-423d-be14-16a553fea991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "===== Prompt Sent to Model =====\n",
       "SYSTEM: You are a helpful assistant that translates complex legal terms into plain and understandable language.\n",
       "Respond only in this format: The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
       "\n",
       "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
       "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
       "\n",
       "Here is the output schema:\n",
       "```\n",
       "{\"properties\": {\"translation\": {\"title\": \"Translation\", \"type\": \"string\"}}, \"required\": [\"translation\"]}\n",
       "```\n",
       "Do not ask for clarification. Always use the given legal input.\n",
       "\n",
       "HUMAN: Notwithstanding any provision to the contrary herein, the indemnitor agrees to indemnify, defend, and hold harmless the indemnitee...\n",
       "\n",
       "AI: {\"translation\": \"One party agrees to cover any costs, claims, or damages that happen because of their actions...\"}\n",
       "\n",
       "HUMAN: This agreement shall be binding upon and inure to the benefit of the parties...\n",
       "\n",
       "AI: {\"translation\": \"This agreement applies to both parties and their future representatives...\"}\n",
       "\n",
       "HUMAN: Any waiver of any term or condition of this agreement shall not be deemed a continuing waiver of such term or condition, nor shall it be considered a waiver of any other term or condition hereof. No failure or delay by either party in exercising any right, power, or privilege under this agreement shall operate as a waiver thereof, nor shall any single or partial exercise preclude any other or further exercise thereof or the exercise of any other right, power, or privilege.\n",
       "\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build full message list\n",
    "messages = [system_message] + few_shot_messages + [user_message]\n",
    "\n",
    "# Sanity check the prompt\n",
    "print(\"\\n\\n===== Prompt Sent to Model =====\")\n",
    "for m in messages:\n",
    "    print(f\"{m.type.upper()}: {m.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "efc091d7-a383-47e3-aadb-13330bf88d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Device set to use cuda:0\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create HF text-generation pipeline with lower temperature for parsing accuracy\n",
    "# (temperature=0.6 here vs 0.7 in initial setup for more deterministic JSON output)\n",
    "hf_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    return_full_text=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    skip_special_tokens=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "75736be7-dc37-4c3c-9360-d97fce3912b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrap in LangChain-compatible Chat LLM\n",
    "wrapped_llm = HuggingFacePipeline(pipeline=hf_pipe)\n",
    "llm_chat = ChatHuggingFace(llm=wrapped_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "52cff459-192f-4dda-b7ad-e7b2eb335131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate model output\n",
    "raw_output = llm_chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ccb194c7-39ff-4f85-9526-cc59a276adcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"translation\": \"If one side gives up a part of this agreement, it doesn't mean they give up the whole thing or other parts. Also, not using a right or waiting to use it doesn't mean they can't use it later or use other rights.\"}\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw_output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "30226364-933a-445e-943a-2a3b153b6958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Simplified translation:\n",
       "If one side gives up a part of this agreement, it doesn't mean they give up the whole thing or other parts. Also, not using a right or waiting to use it doesn't mean they can't use it later or use other rights.\n",
       "\n",
       "Appended to simplified_output.json\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract valid JSON from output\n",
    "\n",
    "def extract_first_json(text):\n",
    "    match = re.search(r'\\{.*?\\}', text, re.DOTALL)\n",
    "    return match.group(0) if match else text.strip()\n",
    "\n",
    "try:\n",
    "    output_text = raw_output.content if isinstance(raw_output, AIMessage) else raw_output\n",
    "    clean_output = extract_first_json(output_text)\n",
    "    result = parser.parse(clean_output)\n",
    "\n",
    "    print(\"\\nSimplified translation:\")\n",
    "    print(result.translation)\n",
    "\n",
    "    # Combine original and simplified\n",
    "    entry = {\n",
    "        \"legal_text\": legal_text,\n",
    "        \"translation\": result.translation\n",
    "    }\n",
    "\n",
    "    # Load existing data if file exists\n",
    "    data = []\n",
    "    output_file = \"simplified_output.json\"\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, \"r\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                if not isinstance(data, list):\n",
    "                    print(\"Warning: existing file is not a list. Overwriting.\")\n",
    "                    data = []\n",
    "            except json.JSONDecodeError:\n",
    "                data = []\n",
    "\n",
    "    # Append new entry\n",
    "    data.append(entry)\n",
    "\n",
    "    # Write back to file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"\\nAppended to {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\nCould not parse output.\")\n",
    "    print(\"Raw output:\", raw_output)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3814bbb-8f47-434d-acd1-b1cd3d0064b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d7d595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Unload Ollama Model & Shutdown Kernel ===\n",
    "# Unloads the model from GPU memory before shutting down\n",
    "\n",
    "try:\n",
    "    import ollama\n",
    "    print(f\"Unloading Ollama model: {OLLAMA_LLM_MODEL}\")\n",
    "    ollama.generate(model=OLLAMA_LLM_MODEL, prompt=\"\", keep_alive=0)\n",
    "    print(\"Model unloaded from GPU memory\")\n",
    "except Exception as e:\n",
    "    print(f\"Model unload skipped: {e}\")\n",
    "\n",
    "# Shut down the kernel to fully release resources\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  },
  "title": "Few-Shot Prompting and Output Parsing with Prompt Templates"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
