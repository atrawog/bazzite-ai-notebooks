{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c272574-6a76-40c2-b7fd-9cadce7ef1c5",
   "metadata": {},
   "source": [
    "## Unsloth: Optimizing Training and Inference Performance\n",
    "\n",
    "For many software algorithms, the performance does not only depend on the number and kind of calculations performed. Instead, the exact order and the size of chunks has an enormous influence on the calculation speed.\n",
    "For large language models, a library called `unsloth` contains optimized GPU kernels created by manually deriving all compute heavy math steps. By using these optimized kernels, a significant speed-up can be obtained.\n",
    "\n",
    "### Key Techniques in Unsloth:\n",
    "\n",
    "1. **Efficient Data Loading**: Optimizing data pipelines to reduce latency and improve throughput during training.\n",
    "2. **Batching and Padding Strategies**: Dynamically adjusting batch sizes and minimizing padding to optimize memory usage.\n",
    "3. **Half-Precision and Quantized Inference**: Using mixed precision or quantized models to speed up inference and reduce memory footprint.\n",
    "4. **Model Pruning and Distillation**: Reducing the size of the model by removing redundant parameters or training smaller models to mimic larger ones.\n",
    "\n",
    "### Benefits of Unsloth:\n",
    "\n",
    "- **Reduced Training Time**: Optimizing data loading and model architecture reduces the time required for each epoch.\n",
    "- **Lower Memory Usage**: Using techniques like mixed precision and quantization reduces the amount of GPU memory required.\n",
    "- **Faster Inference**: Optimizing the model for deployment can significantly reduce latency during inference.\n",
    "\n",
    "### Hands-On Example: Efficient Data Loading and Mixed Precision Training\n",
    "\n",
    "In this example, we take the example from the previous notebook (\"PEFT\") and adjust them to use `unsloth`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf423da",
   "metadata": {},
   "source": [
    "> **Bazzite-AI Setup Required**  \n",
    "> Run `D0_00_Bazzite_AI_Setup.ipynb` first to verify GPU access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xl0eb5yfim9",
   "source": "### vLLM Integration Status\n\nUnsloth supports `fast_inference=True` which uses vLLM as a backend for 2x faster inference. \n\n**Current Status**: vLLM 0.14.0 cu130 is installed, but `fast_inference=True` requires an Unsloth update to support the vLLM 0.14.x API. Use `fast_inference=False` for now.\n\n```python\n# Once Unsloth updates to support vLLM 0.14.x, enable fast_inference:\n# model, tokenizer = FastLanguageModel.from_pretrained(\n#     \"unsloth/tinyllama-chat-bnb-4bit\",\n#     fast_inference=True,  # Enable vLLM backend\n#     gpu_memory_utilization=0.6,\n# )\n# outputs = model.fast_generate([\"Hello!\"], max_new_tokens=50)\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "nsjf0baeyf",
   "source": "# Verify vLLM installation\nimport vllm\nprint(f\"vLLM version: {vllm.__version__}\")\nprint(\"vLLM is installed and ready for standalone server mode\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24ffe0c8-2fc5-4885-8ea1-881beb6b2447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'2.9.1+cu130'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "# Unsloth provides optimized model loading and LoRA implementation\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a8c4214-75f3-4936-b9be-263dab081a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==((====))==  Unsloth 2025.11.1: Fast Llama patching. Transformers: 4.57.2.\n",
       "   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.568 GB. Platform: Linux.\n",
       "O^O/ \\_/ \\    Torch: 2.9.1+cu130. CUDA: 8.9. CUDA Toolkit: 13.0. Triton: 3.5.1\n",
       "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
       " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
       "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Model: unsloth/tinyllama-chat-bnb-4bit\n",
       "Tokenizer padding_side: right\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Unsloth's pre-quantized TinyLlama for consistency with D3_02-D3_05\n",
    "# Unsloth models are optimized with custom CUDA kernels for faster training\n",
    "HF_LLM_MODEL = \"unsloth/tinyllama-chat-bnb-4bit\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    HF_LLM_MODEL,\n",
    "    max_seq_length=512,  # Reduced for memory efficiency on 16GB GPU\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Set padding for batch training\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "print(f\"Model: {HF_LLM_MODEL}\")\n",
    "print(f\"Tokenizer padding_side: {tokenizer.padding_side}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b7843e6-8ca3-4309-92a6-358973336018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Repo card metadata block was not found. Setting CardData to empty.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[huggingface_hub.repocard|WARNING]Repo card metadata block was not found. Setting CardData to empty.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the guanaco dataset from HuggingFace Hub\n",
    "guanaco_train = load_dataset('timdettmers/openassistant-guanaco', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c40eb6a-0884-4b8c-ae58-99fe795c5f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reformat_text(text, include_answer=True):\n",
    "    question1 = text.split('###')[1].removeprefix(' Human: ')\n",
    "    answer1 = text.split('###')[2].removeprefix(' Assistant: ')\n",
    "    if include_answer:\n",
    "        messages = [\n",
    "            {'role': 'user', 'content': question1},\n",
    "            {'role': 'assistant', 'content': answer1}\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {'role': 'user', 'content': question1}\n",
    "        ]        \n",
    "    reformatted_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    return reformatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc21c20-9d29-4697-aca3-f83d8512f174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7762bf7f-d32f-49df-a5bf-784c5dfd1f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, apply reformat_train(..) to the dataset:\n",
    "guanaco_train = guanaco_train.map(lambda entry: {\n",
    "    'reformatted_text': reformat_text(entry['text'])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65644a2c-2be2-40a8-be2f-ea57e3ef1aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unsloth 2025.11.1 patched 22 layers with 22 QKV layers, 22 O layers and 22 MLP layers.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply LoRA adapters using Unsloth's optimized implementation\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=32,  # rule: lora_alpha should be 2*r\n",
    "    lora_dropout=0,  # Use 0 for Unsloth's optimized fast patching (all layers)\n",
    "    bias='none',  # Unsloth supports any, but = 'none' is optimized\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    use_gradient_checkpointing='unsloth',  # True or 'unsloth' for very long context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0353a79e-5eb9-436a-b789-a3c83d1a8870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_arguments = SFTConfig(\n",
    "    output_dir='output/unsloth-tinyllama-chat-guanaco',\n",
    "    per_device_train_batch_size=2,  # Reduced from 8 for 16GB GPU\n",
    "    gradient_accumulation_steps=4,  # Increased to maintain effective batch size\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "    optim='adamw_torch',\n",
    "    learning_rate=2e-4,\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=10,\n",
    "    save_strategy='no',\n",
    "    max_steps=100,\n",
    "    bf16=True,\n",
    "    report_to='none',\n",
    "    max_seq_length=512,  # Reduced from 1024 for memory efficiency\n",
    "    dataset_text_field='reformatted_text',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "719e9d65-236e-4c2c-a58c-401ddf5fbdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=guanaco_train,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ab99044-63b2-4679-91bb-87ca62e0b83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
       "   \\\\   /|    Num examples = 9,846 | Num Epochs = 1 | Total steps = 100\n",
       "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
       "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
       " \"-____-\"     Trainable parameters = 12,615,680 of 1,112,664,064 (1.13% trained)\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Training result:\n",
       "TrainOutput(global_step=100, training_loss=1.5187408638000488, metrics={'train_runtime': 41.0695, 'train_samples_per_second': 19.479, 'train_steps_per_second': 2.435, 'total_flos': 1889668246855680.0, 'train_loss': 1.5187408638000488, 'epoch': 0.08125126955108673})\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "print(\"Training result:\")\n",
    "print(train_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475fa01-1660-40ea-99f8-fdac0ecaea2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43f261c-eff9-4335-bc9f-8fc4c2dd3c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "909731ac-8057-4ff6-a0a5-46bfb9731797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': False}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shut down the kernel to release memory\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f34e4a-2f3f-44d4-b3b4-91b53cd6253f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}