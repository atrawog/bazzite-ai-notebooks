{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e078694-4039-4943-9e8c-a434d395b609",
   "metadata": {},
   "source": [
    "# Fine-tuning an LLM with *plain* PyTorch\n",
    "\n",
    "In this notebook we will:\n",
    "\n",
    "- Use the **same model and dataset** as in later notebooks.\n",
    "- Build a PyTorch `Dataset` and `DataLoader` for instruction → response pairs.\n",
    "- Fine-tune a pretrained language model using a **vanilla PyTorch training loop** (no LoRA, no DeepSpeed, or any other fancy technique).\n",
    "- Save the fine-tuned model.\n",
    "- Compare **inference before and after** fine-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "After this notebook, you should be able to:\n",
    "\n",
    "- Explain what supervised fine-tuning does to an LLM.\n",
    "- Describe in words what an **epoch**, **batch size**, and **learning rate** are.\n",
    "- Read and write a standard PyTorch training loop for an LLM.\n",
    "- Run inference with a base model vs. a fine-tuned version and interpret the difference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a7500",
   "metadata": {},
   "source": [
    "> **Bazzite-AI Setup Required**  \n",
    "> Run `D0_00_Bazzite_AI_Setup.ipynb` first to verify GPU access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aff8be-e028-4cab-8d86-abee679c553d",
   "metadata": {},
   "source": [
    "## 1. What does fine-tuning actually do?\n",
    "\n",
    "We assume we start from a **pretrained language model**. It has already learned:\n",
    "\n",
    "- Grammar and spelling.\n",
    "- General knowledge.\n",
    "- How to continue text in a plausible way.\n",
    "\n",
    "Now we want the model to behave well on *our* task (for example: answer domain-specific instructions in a certain style).  \n",
    "We show it many pairs of:\n",
    "\n",
    "> **Input** (prompt, instruction, context) → **Target** (ideal response / completion)\n",
    "\n",
    "The model assigns a probability to each possible next token. During fine-tuning, we change the weights to **increase the probability of the correct tokens**.\n",
    "\n",
    "Mathematically, if\n",
    "\n",
    "- $x = (x_1, \\dots, x_T)$ is the input sequence (tokens),\n",
    "- $y = (y_1, \\dots, y_T)$ is the target sequence,\n",
    "\n",
    "we minimize the **cross-entropy loss**:\n",
    "\n",
    "$$\n",
    "L(\\theta) = - \\sum_t \\log p_\\theta(y_t \\mid x_{\\le t})\n",
    "$$\n",
    "\n",
    "You don’t need to derive this formula; the key idea is:\n",
    "\n",
    "> During fine-tuning, the model is nudged so that the *ideal answer* becomes more likely on future inputs that look similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee41b97-ed00-4298-802b-1a384903044f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85b3287c-f051-4720-92ab-81ab9c80cb5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1+cu130'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a8f69b8d-90b0-4d37-ab4e-4c3bcbdd6576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(max_length=256, batch_size=1, num_epochs=1, learning_rate=5e-06, weight_decay=0.01, warmup_ratio=0.1, gradient_accumulation_steps=16, seed=42, device='cuda')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \n",
    "    # Data\n",
    "    max_length: int = 256        # max tokens per example\n",
    "\n",
    "    # Optimization\n",
    "    batch_size: int = 1          # reduced for memory efficiency\n",
    "    num_epochs: int = 1\n",
    "    learning_rate: float = 5e-6\n",
    "    weight_decay: float = 0.01\n",
    "\n",
    "    warmup_ratio: float = 0.1\n",
    "    gradient_accumulation_steps: int = 16  # increased to compensate for smaller batch\n",
    "\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cfg = Config()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d0adcd-d497-48e0-8a48-4e078b8e2b07",
   "metadata": {},
   "source": [
    "### Understanding the main hyperparameters\n",
    "\n",
    "We will use the following hyperparameters during training:\n",
    "\n",
    "- **Epoch**:  \n",
    "  One **epoch** means one full pass through the training dataset.  \n",
    "  - If you have 1,000 training examples and `batch_size = 10`, there are 100 steps per epoch.\n",
    "  - If we train for 3 epochs, the model will see each example 3 times.\n",
    "\n",
    "- **Batch size** (`batch_size`):  \n",
    "  Number of examples processed **together** in one forward & backward pass.  \n",
    "  - Larger batch sizes give a more stable estimate of the gradient, but use more GPU memory.\n",
    "  - On limited GPU memory, we often use small batches and **gradient accumulation** (see below).\n",
    "\n",
    "- **Learning rate** (`learning_rate`):  \n",
    "  How big a step we take in the direction suggested by the gradients.  \n",
    "  - Too large → training may diverge (loss explodes).\n",
    "  - Too small → training is very slow and may get stuck in poor local minima.\n",
    "\n",
    "- **Weight decay** (`weight_decay`):  \n",
    "  A regularization term that slowly pulls weights towards zero to avoid overfitting.\n",
    "\n",
    "- **Maximum sequence length** (`max_length`):  \n",
    "  We truncate / pad sequences to this number of tokens.  \n",
    "  - Longer sequences capture more context but cost more memory and time.\n",
    "  - Shorter sequences are cheaper but might cut off important text.\n",
    "\n",
    "- **Warmup ratio** (`warmup_ratio`):  \n",
    "  Fraction of the total training steps where the learning rate increases linearly from 0 to the target value.  \n",
    "  - Helps avoid instability at the beginning of training, especially for large models.\n",
    "\n",
    "- **Gradient accumulation steps** (`gradient_accumulation_steps`):  \n",
    "  Instead of updating the model after every batch, we:\n",
    "  1. Compute gradients for several small batches, and  \n",
    "  2. Accumulate them in memory,  \n",
    "  3. Then apply one optimizer step.  \n",
    "\n",
    "  This simulates a larger effective batch size:\n",
    "\n",
    "  $$\n",
    "  \\text{effective batch size} = \\text{batch\\_size} \\times \\text{gradient\\_accumulation\\_steps}\n",
    "  $$\n",
    "\n",
    "  It’s a common trick to work around GPU memory limits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80491d4c-c87e-4ecf-ad4f-4707582a187f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "07073350-adf3-4257-b5eb-1ff550dd3b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Repo card metadata block was not found. Setting CardData to empty.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"timdettmers/openassistant-guanaco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae3cdde-0440-4e16-a8f1-51215528fc6e",
   "metadata": {},
   "source": [
    "### Dataset loading\n",
    "\n",
    "We use the HuggingFace `datasets` library to load the OpenAssistant Guanaco dataset directly from the Hub. This handles downloading, caching, and provides a clean interface for accessing train/test splits.\n",
    "\n",
    "The dataset contains conversation pairs in the format:\n",
    "```python\n",
    "{\"text\": \"### Human: ...### Assistant: ...\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "087ddc31-b353-4b2b-8ad2-07ae1ede9969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = dataset[\"train\"].to_list()\n",
    "val_data = dataset[\"test\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837062a3-dd03-4e81-b113-dd0042924509",
   "metadata": {},
   "source": [
    "To speed up the training, we will reduce the dataset size considerably for demonstration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7307ee4a-f9b5-4108-9913-66a81d00b035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Columns: ['text']\n",
       "Train size: 500\n",
       "Validation size: 100\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "                                                text\n",
       "0  ### Human: Can you write a short introduction ...\n",
       "1  ### Human: ¿CUales son las etapas del desarrol...\n",
       "2  ### Human: Can you explain contrastive learnin...\n",
       "3  ### Human: I want to start doing astrophotogra...\n",
       "4  ### Human: Método del Perceptrón biclásico: de..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data[0:500]\n",
    "val_data = val_data[0:100]\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "val_df   = pd.DataFrame(val_data)\n",
    "\n",
    "print(\"Columns:\", train_df.columns.tolist())\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Validation size:\", len(val_df))\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "861a9ee6-d938-4d49-b64a-d9c26b586ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c74123-a3d9-41c2-b19a-857d6013b1ef",
   "metadata": {},
   "source": [
    "### Dataset structure\n",
    "\n",
    "We are using the **Guanaco / OpenAssistant** dataset\n",
    "(`timdettmers/openassistant-guanaco`).\n",
    "\n",
    "- Each line in `openassistant_best_replies_train.jsonl` is a JSON object with a single key: `text`.\n",
    "- The `text` value is a *formatted conversation snippet*, for example:\n",
    "\n",
    "  ```text\n",
    "  ### Human: Hola### Assistant: ¡Hola! ¿En qué puedo ayudarte hoy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0fa903fa-3bbb-466e-9cfa-5aba0b198032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_LLM_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1afabff8-f897-4a20-9f37-617182760837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(HF_LLM_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4d33df-798c-472f-a5fc-083646272d1f",
   "metadata": {},
   "source": [
    "Ensure we have a pad token (common for causal LMs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9030808f-6c2e-4269-8f5c-0ec085819cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pad token: </s> ID: 2\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Pad token:\", tokenizer.pad_token, \"ID:\", tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2293a8-50ca-4a0f-9461-182ef9ac55a4",
   "metadata": {},
   "source": [
    "### Prompt for inference function\n",
    "The model was trained on Guanaco-style conversation strings of the form:\n",
    "```python\n",
    "### Human: <instruction>### Assistant: <response>\n",
    "```\n",
    "At inference time we only have a new user instruction, so we must recreate the same format the model saw during training.\n",
    "This function builds that template and leaves the assistant part empty, so the model can generate the response naturally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c01c2bc2-f098-45e7-9796-bc8098730c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_prompt_for_inference(user_instruction: str) -> str:\n",
    "    \"\"\"\n",
    "    Build a Guanaco-style prompt for a NEW instruction at inference time.\n",
    "    The dataset format looks like:\n",
    "    \"### Human: ...### Assistant: ...\"\n",
    "    \"\"\"\n",
    "    return f\"### Human: {user_instruction}### Assistant:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6576bf17-a873-46e4-a2f6-8667497ced89",
   "metadata": {},
   "source": [
    "### Tensorization Class\n",
    "This class converts each training example (a single \"text\" string in Guanaco format) into the tensors needed for fine-tuning a causal language model.\n",
    "\n",
    "For every row in the dataset it:\n",
    "\n",
    "1. Reads the full conversation text (e.g. `\"### Human: ...### Assistant: ...\"`).\n",
    "\n",
    "2. Tokenizes it using the model’s tokenizer.\n",
    "\n",
    "3. Pads or truncates the sequence to a fixed length.\n",
    "\n",
    "4. Creates:\n",
    "\n",
    "    - `input_ids` → the tokenized input\n",
    "\n",
    "    - `attention_mask` → which tokens are real vs padding\n",
    "\n",
    "    - `labels` → a copy of `input_ids` used as training targets\n",
    "\n",
    "5. Replaces padding positions in `labels` with -100 so they are ignored in the loss.\n",
    "\n",
    "The result is a dictionary of tensors (`input_ids`, `attention_mask`, `labels`) that PyTorch’s DataLoader can batch and feed directly into the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "21b8706e-fcf2-4098-838a-0129d7a612a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SupervisedTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each row in train_df/val_df has a 'text' field like:\n",
    "\n",
    "        \"### Human: ...### Assistant: ...\"\n",
    "\n",
    "    For supervised fine-tuning of a causal LM, we feed in the full text and\n",
    "    ask the model to learn to predict the next token at every position.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, tokenizer, max_length: int = 256):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row[\"text\"]\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = enc[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = enc[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        # For causal LM SFT: labels = input_ids (shift is handled internally)\n",
    "        labels = input_ids.clone()\n",
    "        labels[attention_mask == 0] = -100  # ignore padding in loss\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e429602-3fa9-454c-92f3-1f9bb8088edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = SupervisedTextDataset(train_df, tokenizer, max_length=cfg.max_length)\n",
    "val_dataset   = SupervisedTextDataset(val_df, tokenizer, max_length=cfg.max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd00cfd-c80f-4891-a8ea-884e8ed8e603",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "The PyTorch DataLoader is responsible for:\n",
    "\n",
    "- dividing the dataset into batches of size batch_size\n",
    "\n",
    "- shuffling the data each epoch (because shuffle=True)\n",
    "\n",
    "- fetching items by calling `__getitem__` from our SupervisedTextDataset\n",
    "\n",
    "- returning ready-to-use batches during training\n",
    "\n",
    "`train_loader` is therefore the object our training loop iterates over:\n",
    "```python\n",
    "for batch in train_loader:\n",
    "    ...\n",
    "```\n",
    "This is the standard PyTorch way to feed data into a model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "97673beb-20e3-4404-a4c3-55b286a6c109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 100)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=cfg.batch_size)\n",
    "\n",
    "len(train_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a79e654e-8e0c-4435-b3b5-93645ef80ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([1, 256]),\n",
       " 'attention_mask': torch.Size([1, 256]),\n",
       " 'labels': torch.Size([1, 256])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ff034b-00ef-466e-a8f6-645ca9ccc2f5",
   "metadata": {},
   "source": [
    "### What’s inside one batch?\n",
    "```python\n",
    "batch = next(iter(train_loader))\n",
    "{k: v.shape for k, v in batch.items()}\n",
    "```\n",
    "- `iter(train_loader)` creates a Python iterator over the batches.\n",
    "\n",
    "- `next(...)` retrieves the first batch from the `DataLoader`.\n",
    "\n",
    "- `batch` is a dictionary containing tensors like:\n",
    "\n",
    "    - `\"input_ids\"`\n",
    "\n",
    "    - `\"attention_mask\"`\n",
    "\n",
    "    - `\"labels\"`\n",
    "\n",
    "The second line builds a new dictionary showing the shape of each tensor in that batch.\n",
    "This is a quick way to inspect what one batch looks like and confirm that batching and padding work as expected.\n",
    "\n",
    "\n",
    "For a batch size of `B` and sequence length `L`, we typically get:\n",
    "\n",
    "- `input_ids`: shape `(B, L)`  \n",
    "  Integer token IDs that the model reads.\n",
    "\n",
    "- `attention_mask`: shape `(B, L)`  \n",
    "  1 = real token, 0 = padding token.\n",
    "\n",
    "- `labels`: shape `(B, L)`  \n",
    "  Token IDs that we want the model to predict, with `-100` at positions to ignore in the loss (padding).\n",
    "\n",
    "The model will compute a probability distribution over the vocabulary for each position and compare it against `labels` using cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f0f985-cb61-41a6-a12b-357354643f66",
   "metadata": {},
   "source": [
    "### Model, optimizer, and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d60146c1-0f6e-4abd-81e8-8967c6df03af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Number of parameters: 1100.0M\n",
       "Model dtype: torch.bfloat16\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_LLM_MODEL,\n",
    "    dtype=torch.bfloat16,  # Use bfloat16 for memory efficiency\n",
    ")\n",
    "\n",
    "model.to(cfg.device)\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {n_params / 1e6:.1f}M\")\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3973e2c1-92ac-4ab1-8fa1-e06df63ae742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=cfg.learning_rate,\n",
    "    weight_decay=cfg.weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26901bbd-6157-4391-8a31-33b35798ff4e",
   "metadata": {},
   "source": [
    "How many optimizer steps in total?\n",
    "Note: we divide by gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "817c846f-99dd-4cdc-84c5-4051306a1537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = math.ceil(len(train_loader))\n",
    "total_steps = (steps_per_epoch * cfg.num_epochs) // cfg.gradient_accumulation_steps\n",
    "\n",
    "warmup_steps = int(cfg.warmup_ratio * total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c117e9a0-9016-4b02-b595-37272d59e18b",
   "metadata": {},
   "source": [
    "This creates a learning-rate scheduler that changes the optimizer’s learning rate during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "580aeb2d-133d-4ae4-a953-1063dfe4bf4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 3)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "total_steps, warmup_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2218799c-1d27-4477-a20f-bb2328c99c30",
   "metadata": {},
   "source": [
    "### Time to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c95200e7-46e8-4884-96a7-839affe69284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Single batch loss: 2.5328707695007324\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "single_batch = next(iter(train_loader))\n",
    "single_batch = {k: v.to(cfg.device) for k, v in single_batch.items()}\n",
    "\n",
    "# Forward pass\n",
    "out = model(\n",
    "    input_ids=single_batch[\"input_ids\"],\n",
    "    attention_mask=single_batch[\"attention_mask\"],\n",
    "    labels=single_batch[\"labels\"],\n",
    ")\n",
    "\n",
    "loss = out.loss\n",
    "print(\"Single batch loss:\", loss.item())\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Parameter update\n",
    "optimizer.step()\n",
    "scheduler.step()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acedf758-2bc4-4e85-a688-f7e0132dea32",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "\n",
    "For one batch we did:\n",
    "\n",
    "1. **Forward pass**:  \n",
    "   `out = model(...)`  \n",
    "   The model returns:\n",
    "   - `out.logits`: raw predictions for each token.\n",
    "   - `out.loss`: cross-entropy loss between `logits` and `labels`.\n",
    "\n",
    "2. **Loss computation**:  \n",
    "   `loss = out.loss`  \n",
    "   A single scalar summarizing how “wrong” the model is on this batch.\n",
    "\n",
    "3. **Backward pass**:  \n",
    "   `loss.backward()`  \n",
    "   Computes gradients of the loss with respect to all trainable parameters.\n",
    "\n",
    "4. **Optimizer step**:  \n",
    "   `optimizer.step()`  \n",
    "   Updates the weights using the gradients (and the learning rate).\n",
    "\n",
    "5. **LR scheduler step**:  \n",
    "   `scheduler.step()`  \n",
    "   Adjusts the learning rate according to the warmup + decay schedule.\n",
    "\n",
    "6. **Zero gradients**:  \n",
    "   `optimizer.zero_grad()`  \n",
    "   Clears old gradients so they don’t accumulate accidentally.\n",
    "\n",
    "The full training loop is just many repetitions of this pattern over all batches and epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16097a1f-f8ed-4387-b159-f34d12fc4711",
   "metadata": {},
   "source": [
    "### Evaluation function\n",
    "This function computes the **average validation loss** of the model without updating its weights.\n",
    "\n",
    "Step-by-step:\n",
    "\n",
    "1. **`model.eval()`**  \n",
    "   Puts the model in evaluation mode (disables dropout, layer norm behavior, etc.).\n",
    "\n",
    "2. **`torch.no_grad()`**  \n",
    "   Turns off gradient calculation → faster and uses less memory.\n",
    "\n",
    "3. **Iterate over the validation dataloader**  \n",
    "   - Move each batch to the correct device  \n",
    "   - Run a forward pass with `model(**batch)` (The double asterisk ** unpacks the dictionary into keyword arguments.)\n",
    "   - Extract the loss (`out.loss.item()`) and store it\n",
    "\n",
    "4. **Return the model to training mode** with `model.train()`\n",
    "\n",
    "5. **Compute and return the mean loss** across all validation batches.\n",
    "\n",
    "This function is used at the end of each epoch to check how well the model performs on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b9b687c4-f7e2-4d59-a924-c592cb145314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(cfg.device) for k, v in batch.items()}\n",
    "            out = model(**batch)\n",
    "            losses.append(out.loss.item())\n",
    "\n",
    "    model.train()\n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308fe1bf-1e3e-4765-aad8-4678fdc7d51a",
   "metadata": {},
   "source": [
    "### Full training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c9f1e1c5-9d53-4471-a4ad-d5e1e464d2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 1/1:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Validation loss after epoch 1: 1.8335\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "→ New best model saved to ft_model\n",
       "\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "global_step = 0\n",
    "best_val_loss = float(\"inf\")\n",
    "save_dir = \"ft_model\"\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(cfg.num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.num_epochs}\")\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        batch = {k: v.to(cfg.device) for k, v in batch.items()}\n",
    "\n",
    "        out = model(**batch)\n",
    "        loss = out.loss / cfg.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % cfg.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            avg_loss = running_loss / cfg.gradient_accumulation_steps\n",
    "            running_loss = 0.0\n",
    "\n",
    "            progress_bar.set_postfix(\n",
    "                train_loss=f\"{avg_loss:.4f}\",\n",
    "                lr=f\"{scheduler.get_last_lr()[0]:.2e}\",\n",
    "            )\n",
    "\n",
    "    # Validation at the end of the epoch\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    print(f\"\\nValidation loss after epoch {epoch+1}: {val_loss:.4f}\")\n",
    "\n",
    "    # Simple checkpointing: keep the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        model.save_pretrained(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        print(f\"→ New best model saved to {save_dir}\\n\")\n",
    "    else:\n",
    "        print(\"No improvement, keeping previous best model.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9ada338b-f451-4924-aad5-8cc0585b716a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tuned model – load from checkpoint\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    save_dir,\n",
    "    dtype=torch.bfloat16,\n",
    ").to(cfg.device)\n",
    "ft_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16576035-d169-4b2b-b961-a55a8b8cf356",
   "metadata": {},
   "source": [
    "### Time for inference\n",
    "This function runs **inference**: it takes a new user instruction and makes the model generate a reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "13f12296-619e-40cd-92db-69f11dfd26f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_response(model, instruction: str, max_new_tokens: int = 128):\n",
    "    \"\"\"\n",
    "    Generate a reply from the model given a human instruction.\n",
    "    We create a Guanaco-style prompt:\n",
    "        \"### Human: ...### Assistant:\"\n",
    "    and let the model continue.\n",
    "    \"\"\"\n",
    "    prompt_text = build_prompt_for_inference(instruction)\n",
    "\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(cfg.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # greedy for clarity\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49fdb68-f183-4514-944f-dfba683418c9",
   "metadata": {},
   "source": [
    "Let's find an example text that is actually in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d04ebf14-b760-4be3-a3aa-6a9990015df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Human: How do I build a PC?### Assistant: Building a PC can be a rewarding experience, but it requires some technical knowledge and patience. Here are the general steps you can follow:\\n\\n1. Determine your budget and what you want to use the PC for (gaming, work, etc.). This will help you choose the appropriate components.\\n\\n2. Choose the components you want to use (CPU, motherboard, RAM, storage, GPU, power supply, case, and peripherals).\\n\\n3. Make sure all the components are compatible with each other. Check the manufacturer's website and online forums for compatibility information.\\n\\n4. Gather the necessary tools (screwdriver, thermal paste, etc.).\\n\\n5. Install the CPU onto the motherboard, following the manufacturer's instructions.\\n\\n6. Install the RAM onto the motherboard, following the manufacturer's instructions.\\n\\n7. Install the storage (hard drive or solid-state drive) onto the motherboard or into the case, following the manufacturer's instructions.\\n\\n8. Install the power supply into the case, following the manufacturer's instructions.\\n\\n9. Install the GPU onto the motherboard, following the manufacturer's instructions.\\n\\n10. Connect all the necessary cables to the motherboard, power supply, and peripherals.\\n\\n11. Turn on the PC and make sure it boots up properly.\\n\\n12. Install the operating system and necessary drivers.\\n\\n13. Test the PC to make sure all the components are working properly.\\n\\nThese are the basic steps, but there may be some variation depending on the specific components you choose. It's important to follow the manufacturer's instructions and take your time to avoid damaging any components. There are also many online resources, such as YouTube tutorials and PC building forums, that can provide additional guidance and tips.### Human: Thank you.  Is it better to build my own PC or to just buy one off the shelf?  Give me the pros and cons of each approach.### Assistant: Building a computer \\nPros:\\n* Cheaper in the long run\\n* Easier to fix\\n* Better overall quality\\n\\nCons:\\n* Can be more expensive upfront\\n* Higher chance of user error\\n* You need to figure out part compatibility\\n\\n---\\n\\nOff-the-Shelf computer\\nPros:\\n* Faster to buy than to build\\n* Plug and Play\\n* Normally Cheaper to buy rather than build\\n\\nCons:\\n* Repairs are harder to do\\n* Build Quality can be lower\\n* Limited configuration available\\n\\nThere are plenty of other reasons that can influence your decisions but it comes down to how soon you need a computer, and how confident you are working on a computer.\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = val_df.iloc[11][\"text\"]\n",
    "example_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2932a660-10d2-4271-837c-d9d044d7127f",
   "metadata": {},
   "source": [
    "Let's also load the original model that has not been fine-tuned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1547facf-fb41-4f26-ab9b-97449698933d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_LLM_MODEL,\n",
    "    dtype=torch.bfloat16,\n",
    ").to(cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "02f69fcd-ef96-40d0-bfd0-635dad135937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "### HUMAN (PROMPT) ###\n",
       "How do I build a PC?\n",
       "\n",
       "### GROUND TRUTH ASSISTANT ###\n",
       "Building a PC can be a rewarding experience, but it requires some technical knowledge and patience. Here are the general steps you can follow:\n",
       "\n",
       "1. Determine your budget and what you want to use the PC for (gaming, work, etc.). This will help you choose the appropriate components.\n",
       "\n",
       "2. Choose the components you want to use (CPU, motherboard, RAM, storage, GPU, power supply, case, and peripherals).\n",
       "\n",
       "3. Make sure all the components are compatible with each other. Check the manufacturer's website and online forums for compatibility information.\n",
       "\n",
       "4. Gather the necessary tools (screwdriver, thermal paste, etc.).\n",
       "\n",
       "5. Install the CPU onto the motherboard, following the manufacturer's instructions.\n",
       "\n",
       "6. Install the RAM onto the motherboard, following the manufacturer's instructions.\n",
       "\n",
       "7. Install the storage (hard drive or solid-state drive) onto the motherboard or into the case, following the manufacturer's instructions.\n",
       "\n",
       "8. Install the power supply into the case, following the manufacturer's instructions.\n",
       "\n",
       "9. Install the GPU onto the motherboard, following the manufacturer's instructions.\n",
       "\n",
       "10. Connect all the necessary cables to the motherboard, power supply, and peripherals.\n",
       "\n",
       "11. Turn on the PC and make sure it boots up properly.\n",
       "\n",
       "12. Install the operating system and necessary drivers.\n",
       "\n",
       "13. Test the PC to make sure all the components are working properly.\n",
       "\n",
       "These are the basic steps, but there may be some variation depending on the specific components you choose. It's important to follow the manufacturer's instructions and take your time to avoid damaging any components. There are also many online resources, such as YouTube tutorials and PC building forums, that can provide additional guidance and tips.### Human: Thank you.  Is it better to build my own PC or to just buy one off the shelf?  Give me the pros and cons of each approach.\n",
       "\n",
       "### BASE MODEL ###\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "### Human: How do I build a PC?### Assistant: Sure, here's a step-by-step guide on how to build a PC:\n",
       "\n",
       "1. Choose your components\n",
       "\n",
       "- Choose the right components for your needs. For example, if you're a gamer, you'll want a powerful CPU, GPU, and RAM.\n",
       "- Consider the budget. You don't want to spend too much on components that won't be used.\n",
       "\n",
       "2. Choose your motherboard\n",
       "\n",
       "- Choose a motherboard that meets your needs. Some motherboards come with pre-installed components, while others require additional components.\n",
       "- Consider\n",
       "\n",
       "### FINE-TUNED MODEL ###\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "### Human: How do I build a PC?### Assistant: Sure, here's a step-by-step guide on how to build a PC:\n",
       "\n",
       "1. Choose your components\n",
       "\n",
       "- CPU: Choose a processor that meets your needs. Intel Core i5 or AMD Ryzen 5 are good options.\n",
       "- RAM: Choose a RAM speed that matches your processor speed. DDR4 is the most common speed.\n",
       "- Motherboard: Choose a motherboard that meets your needs. Intel and AMD motherboards are the most common.\n",
       "- Graphics card: Choose a graphics card that meets your needs. Nvidia GeForce GTX \n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# crude split to get the human message\n",
    "if \"### Human:\" in example_text and \"### Assistant:\" in example_text:\n",
    "    human_part = example_text.split(\"### Human:\")[1].split(\"### Assistant:\")[0].strip()\n",
    "    assistant_part = example_text.split(\"### Assistant:\")[1].strip()\n",
    "else:\n",
    "    human_part = example_text\n",
    "    assistant_part = \"\"\n",
    "\n",
    "print(\"### HUMAN (PROMPT) ###\")\n",
    "print(human_part)\n",
    "\n",
    "print(\"\\n### GROUND TRUTH ASSISTANT ###\")\n",
    "print(assistant_part)\n",
    "\n",
    "print(\"\\n### BASE MODEL ###\")\n",
    "print(generate_response(base_model, human_part))\n",
    "\n",
    "print(\"\\n### FINE-TUNED MODEL ###\")\n",
    "print(generate_response(ft_model, human_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7c3594eb-5deb-45e5-9f81-384cd6cce44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "FINE-TUNED MODEL:\n",
       "\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "### Human: WRITE YOUR INSTRUCTION HERE### Assistant: Here's an example:\n",
       "\n",
       "1. Start by setting up your project on your computer.\n",
       "\n",
       "2. Install the necessary software and tools, such as a text editor, a development environment, and a database management system.\n",
       "\n",
       "3. Create a database schema and design the database tables.\n",
       "\n",
       "4. Write the database queries and stored procedures to retrieve and update data.\n",
       "\n",
       "5. Implement security measures, such as user authentication and authorization, to ensure data privacy and security.\n",
       "\n",
       "6. Test the application thoroughly to ensure it works as expected and meets the requirements.\n",
       "\n",
       "7. Deploy the\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_instruction = \"WRITE YOUR INSTRUCTION HERE\"\n",
    "\n",
    "print(\"\\nFINE-TUNED MODEL:\\n\")\n",
    "print(generate_response(ft_model, my_instruction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bdfd237d-c046-4448-96fd-fcbf2fbaa20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Using AMP (mixed precision)? True\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_amp = torch.cuda.is_available()  # only useful on GPU\n",
    "print(\"Using AMP (mixed precision)?\", use_amp)\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=use_amp)\n",
    "\n",
    "def train_one_epoch_amp(model, dataloader, optimizer, scheduler, scaler, epoch_idx: int):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"[AMP] Epoch {epoch_idx+1}/{cfg.num_epochs}\")\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        batch = {k: v.to(cfg.device) for k, v in batch.items()}\n",
    "\n",
    "        with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "            out = model(**batch)\n",
    "            loss = out.loss / cfg.gradient_accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % cfg.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "\n",
    "            avg_loss = running_loss / cfg.gradient_accumulation_steps\n",
    "            running_loss = 0.0\n",
    "\n",
    "            progress_bar.set_postfix(\n",
    "                train_loss=f\"{avg_loss:.4f}\",\n",
    "                lr=f\"{scheduler.get_last_lr()[0]:.2e}\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c6b8bd-6d81-4066-9029-721e21884c9a",
   "metadata": {},
   "source": [
    "> **Note:** For the main run we implemented above, we used full precision for simplicity.  \n",
    "> If you switch to this AMP-based loop instead, you will usually see:\n",
    ">\n",
    "> - Lower GPU memory usage.\n",
    "> - Faster training (especially on modern GPUs like A100/H100)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aebbeed-9820-4ba2-aa7c-12fd78fc53a9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook you:\n",
    "\n",
    "1. Loaded a pretrained language model and tokenizer.\n",
    "2. Prepared a dataset of `(prompt, response)` pairs.\n",
    "3. Tokenized and formatted the data for **causal language modeling**.\n",
    "4. Implemented a **vanilla PyTorch training loop**:\n",
    "   - Forward pass → loss\n",
    "   - Backward pass → gradients\n",
    "   - Optimizer + scheduler → weight updates\n",
    "5. Saved the fine-tuned model.\n",
    "6. Compared inference:\n",
    "   - Base model vs. fine-tuned model on real examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722e7564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down the kernel to release memory\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
