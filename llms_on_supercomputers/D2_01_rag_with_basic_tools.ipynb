{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a816a32-96ce-4bf6-b575-334559375061",
   "metadata": {},
   "source": [
    "# RAG Introduction with Ollama OpenAI API\n",
    "\n",
    "This notebook demonstrates building a RAG (Retrieval Augmented Generation) application from scratch using:\n",
    "\n",
    "- **Ollama** as the LLM backend (OpenAI-compatible API)\n",
    "- **llama3.2** for both embeddings and chat completions\n",
    "- **Pandas DataFrame** as a simple vector database\n",
    "\n",
    "## How RAG Works\n",
    "\n",
    "1. **Chunk** the source document into smaller pieces\n",
    "2. **Embed** each chunk into a vector representation\n",
    "3. **Store** embeddings in a vector database\n",
    "4. **Query**: When a user asks a question:\n",
    "   - Embed the question\n",
    "   - Find the most similar chunks (cosine similarity)\n",
    "   - Include those chunks as context in the LLM prompt\n",
    "5. **Generate** a response using the LLM with retrieved context\n",
    "\n",
    "## Sample Document\n",
    "\n",
    "We use a sample excerpt about COVID-19 variants to demonstrate RAG capabilities.\n",
    "\n",
    "The notebook will automatically pull required models if they're not already available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3751bc50",
   "metadata": {},
   "source": [
    "> **Bazzite-AI Setup Required**  \n",
    "> Run `D0_00_Bazzite_AI_Setup.ipynb` first to configure Ollama, pull models, and verify GPU access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de54927-df90-4a45-ae65-05f09f680c84",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9cd6340e-add8-4086-830a-305565675ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama host: http://ollama:11434\n",
       "Embedding model: llama3.2:latest\n",
       "LLM model: llama3.2:latest\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textwrap import wrap\n",
    "from math import sqrt\n",
    "from openai import OpenAI\n",
    "\n",
    "# === Configuration ===\n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\n",
    "\n",
    "# === Model Configuration ===\n",
    "OLLAMA_EMBEDDING_MODEL = \"llama3.2:latest\"\n",
    "OLLAMA_LLM_MODEL = \"llama3.2:latest\"\n",
    "\n",
    "# Initialize OpenAI client pointing to Ollama\n",
    "client = OpenAI(\n",
    "    base_url=f\"{OLLAMA_HOST}/v1\",\n",
    "    api_key=\"ollama\"  # Required by library but ignored by Ollama\n",
    ")\n",
    "\n",
    "print(f\"Ollama host: {OLLAMA_HOST}\")\n",
    "print(f\"Embedding model: {OLLAMA_EMBEDDING_MODEL}\")\n",
    "print(f\"LLM model: {OLLAMA_LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c7ebd7-64a5-4f74-88fa-14a5b2c43649",
   "metadata": {},
   "source": [
    "## 2. Verify Models\n",
    "\n",
    "Models should already be pulled by D0_00. If you see errors below, run `D0_00_Bazzite_AI_Setup.ipynb` first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb1fa09-5bb6-4341-975d-8cf3f4672e08",
   "metadata": {},
   "source": [
    "## 3. Load and Chunk Document\n",
    "\n",
    "For this demo, we use a sample excerpt about COVID-19 variants. The text is embedded directly in the notebook to make it self-contained.\n",
    "\n",
    "The `wrap` function from `textwrap` splits text into chunks of a specified character length, breaking at word boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcd1376b-c31f-4e78-96f2-a19dd46e7459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document chunked into 3 pieces\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample document: COVID-19 Omicron variant information\n",
    "# (Embedded for self-contained demo - based on scientific literature)\n",
    "\n",
    "SAMPLE_TEXT = \"\"\"\n",
    "The Omicron variant of SARS-CoV-2, first identified in South Africa in November 2021, \n",
    "rapidly spread across the globe and became the dominant variant in many countries by early 2022. \n",
    "This variant exhibited significant mutations in the spike protein, raising concerns about \n",
    "vaccine efficacy and therapeutic interventions.\n",
    "\n",
    "In France, the emergence of Omicron led to a rapid replacement of the Delta variant during \n",
    "the winter of 2021-2022. Epidemiological surveillance showed that Omicron cases doubled \n",
    "approximately every two to three days during its initial spread, significantly faster than \n",
    "previous variants.\n",
    "\n",
    "The Omicron variant is characterized by approximately 30 mutations in the spike protein alone, \n",
    "including mutations at positions K417N, N440K, G446S, S477N, T478K, E484A, Q493R, G496S, \n",
    "Q498R, N501Y, and Y505H. Many of these mutations are located in the receptor-binding domain \n",
    "(RBD), which is crucial for viral entry into host cells.\n",
    "\n",
    "Studies in France demonstrated that while Omicron showed increased transmissibility compared \n",
    "to Delta, it was associated with reduced severity of disease. Hospitalization rates and \n",
    "intensive care unit admissions were lower per infection compared to the Delta wave, though \n",
    "the sheer number of cases still strained healthcare systems.\n",
    "\n",
    "The immune evasion properties of Omicron were substantial. Research showed reduced neutralization \n",
    "by antibodies elicited by previous infection with earlier variants or by primary vaccination \n",
    "series. However, booster doses significantly improved protection against severe disease.\n",
    "\n",
    "Mathematical modeling of the Omicron invasion in France utilized multi-variant epidemiological \n",
    "models to understand the dynamics of variant replacement. These models incorporated factors \n",
    "such as cross-immunity between variants, vaccine coverage, and waning immunity over time.\n",
    "\n",
    "The basic reproduction number (R0) of Omicron was estimated to be significantly higher than \n",
    "Delta, with estimates ranging from 8 to 15 depending on the population and setting. This \n",
    "high transmissibility was a key factor in its rapid global spread.\n",
    "\n",
    "French public health authorities responded to the Omicron wave with enhanced testing capacity, \n",
    "acceleration of booster vaccination campaigns, and implementation of sanitary passes requiring \n",
    "up-to-date vaccination status for access to certain venues and activities.\n",
    "\n",
    "Subsequent sub-lineages of Omicron, including BA.2, BA.4, BA.5, and later BQ and XBB variants, \n",
    "continued to evolve with additional mutations conferring further immune evasion properties. \n",
    "This ongoing evolution necessitated updates to vaccine formulations and continued surveillance.\n",
    "\n",
    "The experience with Omicron in France and globally highlighted the importance of genomic \n",
    "surveillance, rapid response capabilities, and adaptable public health strategies in managing \n",
    "emerging variants of concern during a pandemic.\n",
    "\"\"\"\n",
    "\n",
    "# Chunk the text into smaller pieces for embedding\n",
    "wrapped_text = wrap(SAMPLE_TEXT.strip(), 1000)\n",
    "print(f\"Document chunked into {len(wrapped_text)} pieces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b43d5-0f37-4c89-94ed-f10a0211001b",
   "metadata": {},
   "source": [
    "The text is wrapped into chunks of maximum 1000 characters each. The `wrap` function breaks at word boundaries, so actual chunk sizes vary slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1288462a-3790-4c83-ab7f-754790b2c038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094ff9c8-4693-47c2-a7ce-e72bec306140",
   "metadata": {},
   "source": [
    "## 4. Generate Embeddings\n",
    "\n",
    "First, let's test embedding a single chunk using the OpenAI-compatible API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d65d589d-466b-4816-b048-2e54388a52e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text chunk (first 100 chars): The Omicron variant of SARS-CoV-2, first identified in South Africa in November 2021,  rapidly sprea...\n",
       "Embedding dimensions: 3072\n",
       "First 5 values: [-0.005387044046074152, 0.0025414149276912212, -0.02499299868941307, -0.006784075405448675, -0.008193740621209145]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test embedding a single chunk\n",
    "response = client.embeddings.create(\n",
    "    model=OLLAMA_EMBEDDING_MODEL,\n",
    "    input=wrapped_text[0]\n",
    ")\n",
    "\n",
    "embedding = response.data[0].embedding\n",
    "print(f\"Text chunk (first 100 chars): {wrapped_text[0][:100]}...\")\n",
    "print(f\"Embedding dimensions: {len(embedding)}\")\n",
    "print(f\"First 5 values: {embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f02e7d1-3f17-4358-9996-b0306ff8ec38",
   "metadata": {},
   "source": [
    "Now let's create embeddings for all text chunks. Each embedding is stored as a numpy array in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42902178-7c06-4f18-96fa-3bfea8b1db7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Created 3 embeddings\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate embeddings for all chunks\n",
    "embeddings = []\n",
    "\n",
    "for i, text in enumerate(wrapped_text):\n",
    "    response = client.embeddings.create(\n",
    "        model=OLLAMA_EMBEDDING_MODEL,\n",
    "        input=text\n",
    "    )\n",
    "    embedding = np.array(response.data[0].embedding)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "print(f\"Created {len(embeddings)} embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad58fb9-5b48-4acc-bf89-da373ba88b79",
   "metadata": {},
   "source": [
    "Each embedding vector contains multiple dimensions (the exact count depends on the model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb1432c6-c096-488b-ba2f-7db0f54ed0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c2ba8a-2815-4a47-bf44-998a3e5c22af",
   "metadata": {},
   "source": [
    "The total number of embeddings matches our chunk count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "318c87da-5e40-4583-b7cb-e3fa9cf26d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d00e423-1087-4387-ac55-175912d447f7",
   "metadata": {},
   "source": [
    "## 5. Create Vector Database\n",
    "\n",
    "For simplicity, we use a Pandas DataFrame as our vector database. Each row contains a text chunk and its corresponding embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59c09120-d9a3-4864-b010-9f917dcedc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vector_data_base = pd.DataFrame({ 'text': wrapped_text,\n",
    "                                  'embeddings': embeddings })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "540f857a-11d1-4c85-812b-b88907463c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                text  \\\n",
       "0  The Omicron variant of SARS-CoV-2, first ident...   \n",
       "1  Omicron showed increased transmissibility comp...   \n",
       "2  depending on the population and setting. This ...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [-0.005387044046074152, 0.0025414149276912212,...  \n",
       "1  [0.020577766001224518, -0.0041334908455610275,...  \n",
       "2  [-0.001117408974096179, -0.004059193190187216,...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_data_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "00efc076-25b3-4383-ab96-41641a51fdd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00538704,  0.00254141, -0.024993  , ..., -0.02018412,\n",
       "       -0.02819293,  0.03002225], shape=(3072,))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_data_base['embeddings'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5751d8f-b0d0-4ef0-9ca8-e84131414e1b",
   "metadata": {},
   "source": [
    "## 6. Query Functions\n",
    "\n",
    "To search the vector database, we use cosine distance to find the most similar chunks to a query embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "90b91fad-8701-4045-9c16-0949a104172e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosine_distance(a,b):\n",
    "    return(float(1.-((np.dot(a,b))/(sqrt(np.dot(a,a))*sqrt(np.dot(b,b))))))\n",
    "\n",
    "def best_answers(n,query,database):\n",
    "    distances = []\n",
    "    for i in range(len(vector_data_base)):\n",
    "        distances.append(cosine_distance(database['embeddings'][i],query))\n",
    "    local_db = database.copy()\n",
    "    local_db['distances'] = distances\n",
    "    local_db = local_db.nsmallest(n,'distances')\n",
    "    return(list(local_db['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1fa9e9-5bb3-42a6-a8c6-5bc9b8367961",
   "metadata": {},
   "source": [
    "## 7. Query Encoding\n",
    "\n",
    "To search for relevant context, we need to encode user questions into embeddings. For long queries, we chunk them, embed each chunk, and combine by averaging and normalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "19652d8d-e767-45d8-aeb4-d1167feabccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_into_single_embedding(intext):\n",
    "    \"\"\"Encode text into a single embedding vector using Ollama.\"\"\"\n",
    "    embedded_chunks = []\n",
    "    wrapped = wrap(intext, 1000)\n",
    "    \n",
    "    for text in wrapped:\n",
    "        response = client.embeddings.create(\n",
    "            model=OLLAMA_EMBEDDING_MODEL,\n",
    "            input=text\n",
    "        )\n",
    "        embedding = np.array(response.data[0].embedding)\n",
    "        embedded_chunks.append(embedding)\n",
    "    \n",
    "    # If only one chunk, return it directly\n",
    "    if len(embedded_chunks) == 1:\n",
    "        return embedded_chunks[0]\n",
    "    \n",
    "    # Combine multiple embeddings by averaging\n",
    "    combined = np.mean(embedded_chunks, axis=0)\n",
    "    \n",
    "    # Normalize the combined embedding\n",
    "    norm = np.linalg.norm(combined)\n",
    "    if norm > 0:\n",
    "        combined = combined / norm\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8684630e-d17b-4777-b722-bb29368ed9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Query embedding shape: 3072 dimensions\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test query encoding\n",
    "encoded = encode_into_single_embedding(\"What do you know about the Omicron variant?\")\n",
    "print(f\"Query embedding shape: {len(encoded)} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "29563a86-bc83-4501-b69e-a825e0a29590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Top 3 relevant chunks:\n",
       "\n",
       "--- Chunk 1 ---\n",
       "depending on the population and setting. This  high transmissibility was a key factor in its rapid global spread.  French public health authorities responded to the Omicron wave with enhanced testing ...\n",
       "\n",
       "--- Chunk 2 ---\n",
       "The Omicron variant of SARS-CoV-2, first identified in South Africa in November 2021,  rapidly spread across the globe and became the dominant variant in many countries by early 2022.  This variant ex...\n",
       "\n",
       "--- Chunk 3 ---\n",
       "Omicron showed increased transmissibility compared  to Delta, it was associated with reduced severity of disease. Hospitalization rates and  intensive care unit admissions were lower per infection com...\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most relevant chunks\n",
    "relevant_chunks = best_answers(3, encoded, vector_data_base)\n",
    "print(\"Top 3 relevant chunks:\")\n",
    "for i, chunk in enumerate(relevant_chunks):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(chunk[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598b0f95-40e3-442d-bb22-677c0d462e44",
   "metadata": {},
   "source": [
    "## 8. RAG Chat System\n",
    "\n",
    "With the OpenAI-compatible API, we don't need to manually construct prompt tokens. The API handles the chat template automatically through the `messages` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "13120407-e710-43ad-b024-f46fb99582c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# System prompt for RAG\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant. Answer questions based on the provided context.\n",
    "If the answer is not in the context, say so clearly. Be concise but thorough.\"\"\"\n",
    "\n",
    "# Conversation history for multi-turn chat\n",
    "conversation_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d06aa-e675-4f5c-ad75-ada4e570e238",
   "metadata": {},
   "source": [
    "The `build_messages` function constructs the messages list with retrieved context for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "25539426-1ed3-45ef-957e-71c899017ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_messages(context_chunks, user_query, history=None):\n",
    "    \"\"\"Build messages list for chat completion with RAG context.\"\"\"\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_chunks)\n",
    "    \n",
    "    system_content = f\"\"\"{SYSTEM_PROMPT}\n",
    "\n",
    "## Retrieved Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_content}]\n",
    "    \n",
    "    # Add conversation history if provided\n",
    "    if history:\n",
    "        messages.extend(history)\n",
    "    \n",
    "    # Add current user query\n",
    "    messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2084d66-66cc-4107-a926-2d733c7846fb",
   "metadata": {},
   "source": [
    "The `get_llm_response` function calls the LLM using the OpenAI-compatible chat completions API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da9669b2-5e0e-48a3-94d6-47fa30471712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_llm_response(messages, max_tokens=500):\n",
    "    \"\"\"Get response from LLM via Ollama's OpenAI-compatible API.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=OLLAMA_LLM_MODEL,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6dde2f-d345-46a1-8b1d-d2f7ce7dfbaf",
   "metadata": {},
   "source": [
    "The main `chat` function ties everything together: encode query → retrieve context → build messages → get response → update history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "76644b84-1ccd-47d5-ad33-18edb748773c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat(user_query, n_docs=5):\n",
    "    \"\"\"RAG chat function with conversation memory.\"\"\"\n",
    "    global conversation_history\n",
    "    \n",
    "    # Encode query and retrieve relevant chunks\n",
    "    query_embedding = encode_into_single_embedding(user_query)\n",
    "    relevant_chunks = best_answers(n_docs, query_embedding, vector_data_base)\n",
    "    \n",
    "    # Build messages with context and history\n",
    "    messages = build_messages(relevant_chunks, user_query, conversation_history)\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = get_llm_response(messages)\n",
    "    \n",
    "    # Update conversation history (without the system message)\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_query})\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    \n",
    "    print(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b9a0d0-63d7-4baf-a0ad-949df8fd914c",
   "metadata": {},
   "source": [
    "## 9. Try It Out!\n",
    "\n",
    "Now let's chat with our RAG-enabled assistant. The conversation history is maintained across calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dd275205-0f7b-4744-8d0a-b0c6a25d0d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Based on the provided context, here's what I know about the Omicron variant in France:\n",
       "\n",
       "1. **Rapid spread**: The Omicron variant rapidly spread across France and became the dominant variant by early 2022.\n",
       "2. **High transmissibility**: The variant exhibited significant mutations in the spike protein, raising concerns about vaccine efficacy and therapeutic interventions. Omicron was associated with increased transmissibility compared to the Delta variant.\n",
       "3. **Reduced severity of disease**: Despite its high transmissibility, Omicron showed reduced severity of disease in France, with lower hospitalization rates and intensive care unit admissions per infection compared to the Delta wave.\n",
       "4. **Immune evasion properties**: The immune evasion properties of Omicron were substantial, with research showing reduced neutralization by antibodies elicited by previous infection with earlier variants or by primary vaccination series.\n",
       "5. **Booster doses improved protection**: Booster doses significantly improved protection against severe disease caused by Omicron.\n",
       "6. **Enhanced testing capacity and vaccination campaigns**: French public health authorities responded to the Omicron wave by enhancing testing capacity, accelerating booster vaccination campaigns, and implementing sanitary passes requiring up-to-date vaccination status for access to certain venues and activities.\n",
       "\n",
       "Overall, France's experience with the Omicron variant highlighted the importance of genomic surveillance, rapid response capabilities, and adaptable public health strategies in managing emerging variants of concern during a pandemic.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, here's what I know about the Omicron variant in France:\\n\\n1. **Rapid spread**: The Omicron variant rapidly spread across France and became the dominant variant by early 2022.\\n2. **High transmissibility**: The variant exhibited significant mutations in the spike protein, raising concerns about vaccine efficacy and therapeutic interventions. Omicron was associated with increased transmissibility compared to the Delta variant.\\n3. **Reduced severity of disease**: Despite its high transmissibility, Omicron showed reduced severity of disease in France, with lower hospitalization rates and intensive care unit admissions per infection compared to the Delta wave.\\n4. **Immune evasion properties**: The immune evasion properties of Omicron were substantial, with research showing reduced neutralization by antibodies elicited by previous infection with earlier variants or by primary vaccination series.\\n5. **Booster doses improved protection**: Booster doses significantly improved protection against severe disease caused by Omicron.\\n6. **Enhanced testing capacity and vaccination campaigns**: French public health authorities responded to the Omicron wave by enhancing testing capacity, accelerating booster vaccination campaigns, and implementing sanitary passes requiring up-to-date vaccination status for access to certain venues and activities.\\n\\nOverall, France's experience with the Omicron variant highlighted the importance of genomic surveillance, rapid response capabilities, and adaptable public health strategies in managing emerging variants of concern during a pandemic.\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First question about the Omicron variant\n",
    "chat(\"What do you know about the Omicron variant in France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4d745f61-dbc7-4fcc-9c9e-858cc67ec49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "According to the provided context, the Omicron variant has approximately 30 mutations in the spike protein alone, including:\n",
       "\n",
       "1. K417N\n",
       "2. N440K\n",
       "3. G446S\n",
       "4. S477N\n",
       "5. T478K\n",
       "6. E484A\n",
       "7. Q493R\n",
       "8. G496S\n",
       "9. Q498R\n",
       "10. N501Y\n",
       "11. Y505H\n",
       "\n",
       "These mutations are primarily located in the receptor-binding domain (RBD), which is crucial for viral entry into host cells.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'According to the provided context, the Omicron variant has approximately 30 mutations in the spike protein alone, including:\\n\\n1. K417N\\n2. N440K\\n3. G446S\\n4. S477N\\n5. T478K\\n6. E484A\\n7. Q493R\\n8. G496S\\n9. Q498R\\n10. N501Y\\n11. Y505H\\n\\nThese mutations are primarily located in the receptor-binding domain (RBD), which is crucial for viral entry into host cells.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Follow-up question (uses conversation history)\n",
    "chat(\"What mutations does it have?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "383015db-6a89-41c9-a192-f7cb3b42d75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Conversation History ===\n",
       "\n",
       "[USER]: What do you know about the Omicron variant in France?\n",
       "\n",
       "[ASSISTANT]: Based on the provided context, here's what I know about the Omicron variant in France:\n",
       "\n",
       "1. **Rapid spread**: The Omicron variant rapidly spread across France and became the dominant variant by early 2...\n",
       "\n",
       "[USER]: What mutations does it have?\n",
       "\n",
       "[ASSISTANT]: According to the provided context, the Omicron variant has approximately 30 mutations in the spike protein alone, including:\n",
       "\n",
       "1. K417N\n",
       "2. N440K\n",
       "3. G446S\n",
       "4. S477N\n",
       "5. T478K\n",
       "6. E484A\n",
       "7. Q493R\n",
       "8. G496S\n",
       "9....\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View conversation history\n",
    "print(\"=== Conversation History ===\")\n",
    "for msg in conversation_history:\n",
    "    role = msg[\"role\"].upper()\n",
    "    content = msg[\"content\"][:200] + \"...\" if len(msg[\"content\"]) > 200 else msg[\"content\"]\n",
    "    print(f\"\\n[{role}]: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ec273956-b3fb-4a06-8385-8e51c42c9fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 10. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4a587413-ba94-4a17-8612-34c7043db5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reset_conversation():\n",
    "    \"\"\"Reset conversation history to start fresh.\"\"\"\n",
    "    global conversation_history\n",
    "    conversation_history = []\n",
    "    print(\"✓ Conversation history cleared\")\n",
    "\n",
    "# Uncomment to reset:\n",
    "# reset_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4aea794d-3891-43a8-80ea-3498aacc7623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try another question!\n",
    "# chat(\"How did France respond to the Omicron wave?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4891e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Unload Ollama Model & Shutdown Kernel ===\n",
    "# Unloads the model from GPU memory before shutting down\n",
    "\n",
    "try:\n",
    "    import ollama\n",
    "    print(f\"Unloading Ollama model: {OLLAMA_LLM_MODEL}\")\n",
    "    ollama.generate(model=OLLAMA_LLM_MODEL, prompt=\"\", keep_alive=0)\n",
    "    print(\"Model unloaded from GPU memory\")\n",
    "except Exception as e:\n",
    "    print(f\"Model unload skipped: {e}\")\n",
    "\n",
    "# Shut down the kernel to fully release resources\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
