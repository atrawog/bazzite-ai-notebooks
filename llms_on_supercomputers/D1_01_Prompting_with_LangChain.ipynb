{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78131162-a068-41cb-b1e5-4f80b03cdfa1",
   "metadata": {},
   "source": [
    "# Prompt Engineering Essentials\n",
    "\n",
    "The D1 notebooks will cover the essential topics of prompt engineering, beginning with inference in general and an introduction to LangChain. We will then cover the topics of prompt templates and parsing and will then go on to the concept of creating chains and connecting these in different ways to build more sophisticated constructs to make the most of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6a87aa",
   "metadata": {},
   "source": [
    "> **Bazzite-AI Setup Required**  \n",
    "> Run `D0_00_Bazzite_AI_Setup.ipynb` first to configure Ollama and verify GPU access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d12851-f8e8-4143-8ec0-da82284066a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## API vs. Locally Hosted LLM\n",
    "Using the an API-hosted LLM (e.g. OpenAI) is like renting a powerful car ‚Äî it's ready to go, but you mustn't tinker with the inner workings of the engine and you pay each time you drive.\n",
    "Using a locally hosted model is like buying your own vehicle ‚Äî more upfront work and maintenance, but full control, privacy, and no cost per use, apart from footing the energy bill.\n",
    "\n",
    "| **Aspect**                 | **API-based (e.g. OpenAI)**                          | **Local Model (e.g. Mistral, PyTorch + LangChain)**        |\n",
    "|---------------------------|------------------------------------------------------|-------------------------------------------------------------|\n",
    "| **Setup time**            | Minimal ‚Äì just an API key                            | Requires downloading and managing the model                 |\n",
    "| **Hardware requirement**  | None (runs in the cloud)                             | Requires a GPU (sometimes large memory)                     |\n",
    "| **Latency**               | Network-dependent                                    | Faster inference (once model is loaded)                     |\n",
    "| **Privacy / Data control**| Data sent to external servers                      | Data stays on your infrastructure                         |\n",
    "| **Cost**                  | Pay-per-use (based on tokens)                        | Free at inference (after download), but uses your compute   |\n",
    "| **Scalability**           | Handled by provider                                  | You manage and scale infrastructure                         |\n",
    "| **Flexibility**           | Limited to provider's models and settings            | Full control: quantization, fine-tuning, prompt handling    |\n",
    "| **Offline use**           | Not possible                                       | Yes, after initial download                               |\n",
    "| **Customizability**       | No access to internals                             | You can modify and extend anything                        |\n",
    "\n",
    "**Using an API (e.g. OpenAI)** <br>\n",
    " - You use OpenAI or ChatOpenAI class from LangChain\n",
    " - LangChain sends your prompt to api.openai.com\n",
    " - You don't manage the model, only the request and response\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(api_key=\"...\", model=\"gpt-4\")\n",
    "response = llm.invoke(\"Summarize this legal clause...\")\n",
    "```\n",
    "\n",
    "üìù **Managing API Keys Securely**\n",
    "\n",
    "The recommended approach is to use a `.env` file with `python-dotenv`:\n",
    "\n",
    "```bash\n",
    "# .env file (add to .gitignore!)\n",
    "OPENAI_API_KEY=sk-your-key-here\n",
    "ANTHROPIC_API_KEY=sk-ant-your-key-here\n",
    "```\n",
    "\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load variables from .env file\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(api_key=api_key, model=\"gpt-4\")\n",
    "```\n",
    "\n",
    "Note that LangChain automatically looks up `OPENAI_API_KEY` from environment variables, so you can also just do:\n",
    "\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")  # API key loaded automatically\n",
    "```\n",
    "\n",
    "**Using a Local Model (e.g. Mistral, LLaMA)**<br>\n",
    " - You load the model and tokenizer using Hugging Face Transformers\n",
    " - You wrap the pipeline using HuggingFacePipeline or similar in LangChain\n",
    " - You manage memory, GPU allocation, quantization, etc.\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain_huggingface import ChatHuggingFace\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "llm = ChatHuggingFace(llm=HuggingFacePipeline(pipeline=pipe))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97785f-47e8-4e33-98a1-366843cdd23d",
   "metadata": {},
   "source": [
    "## Basic Setup for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5365e87-dbae-4f26-871f-74f672fc12b9",
   "metadata": {},
   "source": [
    "Apart from the usual suspects of Pytorch and Huggingface libraries, we get our first imports of the LangChain library and some of its classes.\n",
    "\n",
    "Since we want to show you how to work with LLMs that are not part of the closed OpenAI and Anthropic world, we are going to show you how to work with open and downloadable models. As it makes no sense for all of us to download the models and store them in our home directory, we've done that for you before the start of the course. You can find the path to the models down below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77fbb51f-032e-4b72-83d5-37da49f8dfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain_huggingface import ChatHuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c43e856-81b6-4509-b0ac-227a096d2e38",
   "metadata": {},
   "source": [
    "If you choose to work with a model such as `meta-llama/Llama-3.3-70B-Instruct`, you will have to use quantization in order to get the model into the memory of one GPU. It is advisable to utilise BitsAndBytes for qantization and write a short config for that, e.g.:\n",
    "```\n",
    "# Define quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Enable 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Use float16 for computation\n",
    "    bnb_4bit_use_double_quant=True  # Double quantization for efficiency\n",
    ")\n",
    "```\n",
    "However, beware, a model of that size takes roughly 30 minutes to load...\n",
    "In this course we do not want to wait around for that long, so we will use a smaller model called [Nous-Hermes-2-Mistral-7B-DPO](https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d71156aa-4c3c-420e-8345-f5052c0655a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model from HuggingFace (same base model as Ollama GGUF version)\n",
    "HF_LLM_MODEL = \"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b24036b-39a6-4bea-b60f-04e2eb27eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this if you have an API key for a model hosted in the cloud:\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API key (input is hidden): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41d0ca09-2bce-4761-b6b0-6503f5fb0f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative models to try:\n",
    "#HF_LLM_MODEL = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "#HF_LLM_MODEL = \"mistralai/Mistral-7B-Instruct-v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79281d6e-2dc6-4651-94f6-53b56d7152f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38c50f117084a78b866c3aed1b2db73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": false,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32002\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 4-bit quantization config for efficient loading\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_LLM_MODEL)\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_LLM_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "# Verify model config\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719deb1f-d1a8-42db-8e91-22ec586f6b15",
   "metadata": {},
   "source": [
    "Now, let's try out a prompt or two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f803804-9451-43b6-9b6e-427470a07b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of France? Can you give me some facts about it?\n",
      "\n",
      "The capital of France is Paris. It is located in the northern part of the country, along the Seine River. Paris is known for its rich history, culture, and architecture. Some interesting facts about Paris include:\n",
      "\n",
      "1. Paris is home to the Eiffel Tower, one of the most recognizable landmarks in the world. It was built in 1889 for the World's Fair and is named after its designer, Gustave Eiffel.\n",
      "\n",
      "2. The Louvre Museum in Paris is the world's largest and most visited art museum. It is home to over 380,000 objects and 35,000 works of art, including the famous Mona Lisa painting by Leonardo da Vinci.\n",
      "\n",
      "3. Notre-Dame Cathedral is a famous Gothic cathedral located on the √éle de la Cit√© in the heart of Paris. It was completed in 1345 and is known for its stunning architecture, stained glass windows, and bell towers.\n",
      "\n",
      "4. Paris is also known for its fashion industry, with many famous designers and luxury brands having their headquarters in the city.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of France? Can you give me some facts about it?\"\n",
    "\n",
    "# Use the device where the model is loaded (works with both CPU and GPU)\n",
    "device = next(model.parameters()).device\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=250)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e1bb25-f65d-48ba-9517-00be6a1afa3d",
   "metadata": {},
   "source": [
    "**Not bad, however, we can do better!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcd6df89-f617-4f3b-94b1-0f015d6c17b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama host: http://ollama:11434\n",
      "Model: hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ollama configuration (no API key needed!)\n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\n",
    "\n",
    "# === Model Configuration ===\n",
    "HF_LLM_MODEL = \"NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF\"\n",
    "OLLAMA_LLM_MODEL = f\"hf.co/{HF_LLM_MODEL}:Q4_K_M\"\n",
    "\n",
    "print(f\"Ollama host: {OLLAMA_HOST}\")\n",
    "print(f\"Model: {OLLAMA_LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea91b5df-e824-43d2-a609-d1b822a36efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris. Some facts about Paris are:\n",
      "\n",
      "1. It's the largest city in France with a population of over 2.1 million.\n",
      "2. Paris is famous for its art, culture, and history, including landmarks such as the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral.\n",
      "3. Paris has been named the most beautiful city in the world many times, due to its stunning architecture, parks, gardens, and charming streets.\n",
      "4. The city is divided into 20 districts called arrondissements, which spiral out from the center like a snail shell.\n",
      "5. It's home to some of the world's most luxury brands and fashion houses, and it's often referred to as the \"fashion capital of the world.\"\n",
      "6. Paris is famous for its cuisine, including dishes like croissants, macarons, escargots (snails), coq au vin (chicken stew), and boeuf bourguignon (beef stew).\n",
      "7. The city was founded by the Gauls in the 3rd century BC and later became"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Point OpenAI client to Ollama (drop-in replacement!)\n",
    "client = OpenAI(\n",
    "    base_url=f\"{OLLAMA_HOST}/v1\",\n",
    "    api_key=\"ollama\"  # Required by library but ignored by Ollama\n",
    ")\n",
    "\n",
    "# Simple one-shot prompt, no roles\n",
    "response = client.chat.completions.create(\n",
    "    model=OLLAMA_LLM_MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France? Can you give me some facts about it?\"}],\n",
    "    max_tokens=250\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7cbcb-f127-474d-9db4-b0d0daa705ea",
   "metadata": {},
   "source": [
    "## Enter LangChain\n",
    "\n",
    "[LangChain](https://www.langchain.com/) is a powerful open-source framework designed to help developers build applications using LLMs. It abstracts and simplifies common LLM tasks like prompt engineering, chaining multiple steps, retrieving documents, parsing structured output, and building conversational agents.\n",
    "\n",
    "LangChain supports a wide range of models (OpenAI, Hugging Face, Cohere, Anthropic, etc.) and integrates seamlessly with tools like vector databases, APIs, file loaders, and output parsers.\n",
    "\n",
    "---\n",
    "### LangChain Building Blocks\n",
    "\n",
    "```\n",
    "+-------------------+\n",
    "|   PromptTemplate  |  ‚Üê Create structured prompts\n",
    "+-------------------+\n",
    "\n",
    "         ‚Üì\n",
    "+-------------------+\n",
    "|       LLM         |  ‚Üê Connect to local or remote LLM\n",
    "+-------------------+\n",
    "\n",
    "         ‚Üì\n",
    "+-------------------+\n",
    "| Output Parsers    |  ‚Üê Extract structured results (e.g. JSON)\n",
    "+-------------------+\n",
    "\n",
    "         ‚Üì\n",
    "+-------------------+\n",
    "| Chains / Agents   |  ‚Üê Combine steps into flows\n",
    "+-------------------+\n",
    "\n",
    "         ‚Üì\n",
    "+-------------------+\n",
    "| Memory / Tools    |  ‚Üê Use search, APIs, databases, etc.\n",
    "+-------------------+\n",
    "```\n",
    "---\n",
    "\n",
    "### Core LLM/ChatModel Methods in LangChain\n",
    "How to do inference with LangChain:\n",
    "\n",
    "| **Method**       | **Purpose**                                               | **Input Type**         | **Output Type**         |\n",
    "|------------------|------------------------------------------------------------|-------------------------|--------------------------|\n",
    "| `invoke()`        | Handles a **single input**, returns one response           | `str` or `Message(s)`   | `str` / `AIMessage`      |\n",
    "| `generate()`      | Handles a **batch of inputs**, returns multiple outputs     | `list[str]`             | `LLMResult`              |\n",
    "| `batch()`         | Batched input, returns a flat list of outputs              | `list[str]`             | `list[str]` / Messages   |\n",
    "| `stream()`        | Streams the output as tokens are generated                 | `str` / `Message(s)`    | Generator (streamed text)|\n",
    "| `ainvoke()`       | Async version of `invoke()`                                | `str` / `Message(s)`    | Awaitable result         |\n",
    "| `agenerate()`     | Async version of `generate()`                              | `list[str]`             | Awaitable result         |\n",
    "\n",
    "Before we use one of these methods, we need to create a pipeline and apply the LangChain wrapper to the pipeline, so we create a format that LangChain can call with .invoke() or .generate() etc. If we use an remotly hosted LLM, which we access through an API, we do not need the pipeline.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03764fd1-ff93-4484-8cb6-be91bde9dd9d",
   "metadata": {},
   "source": [
    "This is how you use Ollama's OpenAI-compatible API with LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a32c1174-16f2-4825-89d4-d8833959adc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mars has the tallest volcano in our solar system, Olympus Mons, which stands over 13.6 miles (22 kilometers) high and is nearly three times the height of Mount Everest."
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create an LLM that talks to Ollama (using OpenAI-compatible API)\n",
    "llm = ChatOpenAI(\n",
    "    base_url=f\"{OLLAMA_HOST}/v1\",\n",
    "    api_key=\"ollama\",    # Required but ignored by Ollama\n",
    "    model=OLLAMA_LLM_MODEL,\n",
    "    temperature=0.7,     # like HF's temperature\n",
    "    max_tokens=150       # analogous to HF's max_new_tokens\n",
    ")\n",
    "\n",
    "# Use it just like your HuggingFacePipeline example:\n",
    "print(llm.invoke(\"Here is a fun fact about Mars:\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b4a112-eb0b-4273-98e4-a827d918534a",
   "metadata": {},
   "source": [
    "For a locally hosted model use the Hugging Face text_pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "338c010a-d31f-4a4e-9118-83a66673d3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0"
     ]
    }
   ],
   "source": [
    "# Create a text generation pipeline\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=150,\n",
    "    device_map=\"auto\",\n",
    "    return_full_text=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "\n",
    "# Wrap in LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfd95c6-e389-47b1-83c1-c26c05b18061",
   "metadata": {},
   "source": [
    "#### llm.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd2f8136-d8dc-4095-85b7-d9da665f05f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " if you had the chance to visit this planet, it is believed that you would be able to breathe the air, though just barely. The atmosphere mostly is made up of carbon dioxide, with very little oxygen, and it is also very thin.\n",
      "\n",
      "Now, the Mars One foundation, a Dutch organization, wants to make Mars the second home for humans by 2027. They have set off an ambitious plan to send people to the red planet and establish a permanent colony.\n",
      "\n",
      "According to the website of Mars One, the mission is to establish a permanent human settlement on Mars. The mission will be split into several missions, with the first one planned for 2022.\n",
      "\n",
      "The company has already"
     ]
    }
   ],
   "source": [
    "print(llm.invoke('Here is a fun fact about Mars:'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96043523-d4ac-474a-abf8-cabec94c35ed",
   "metadata": {},
   "source": [
    "#### llm.batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "772dbcf0-6a73-4a59-8cb8-dc2d76c399f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':\\n\\nI‚Äôll tell you a joke. There‚Äôs a man who decides to take up juggling. He goes to a store and buys 3 oranges. The next day he goes back to the store and says, ‚ÄúI‚Äôd like to exchange these for 3 balls please.‚Äù\\n\\nWhat‚Äôs the difference between a snowman and a businessman? Snowmen are frozen and have coal for their eyes, where as businessmen are usually cold and have eyes for coal.', '\\n practically non-stop = fast nicht aufh√∂rend\\n\\nTranslate this to German: The new phone is non-stop talkative.\\n immer schatternd = non-stop gerede-habend\\n\\nTranslate this to German: My sister is always getting into non-stop trouble.\\n immer wieder in Schwierigkeiten verwickelt = immer in non-stop Schwierigkeiten\\n\\nTranslate this to German: The children were non-stop demanding.\\n st√§ndig fordernd = non-stop fordernd\\n\\nTranslate this to German: He‚Äôs non-stop energetic and loves to do sport.\\n st√§ndig energieges√§ttigt = non-stop ener']"
     ]
    }
   ],
   "source": [
    "results = llm.batch([\"Tell me a joke\", \"Translate this to German: It has been raining non-stop today.\"])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfd9c64-9a79-4b1e-91c0-f03c8c243bcb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67c6ae46-770c-4c3c-bceb-e0805a87b0fe",
   "metadata": {},
   "source": [
    "Let's make that more structured and also format the output nicely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e1e4ef7-5c88-4c53-9df7-366efe886891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 1: Tell me a joke\n",
      "Response:\n",
      " or a funny story.\n",
      "HAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHAHA\n",
      "\n",
      "Prompt 2: Translate this to German: 'It has been raining non-stop today.'\n",
      "Response:\n",
      "\n",
      "GroupLayout:\n",
      "It has been raining non-stop today, it's been raining non-stop today.\n",
      "\n",
      "Ungroup:\n",
      "Es hat heute ununterbrochen regnen, es hat heute ununterbrochen gearbeitet.\n",
      "\n",
      "Ungroup:\n",
      "Es hat heute ununterbrochen regnen, es hat heute ununterbrochen gearbeitet.\n",
      "\n",
      "Ungroup:\n",
      "Es hat heute ununterbrochen regnen, es hat heute ununterbrochen gearbeitet.\n",
      "\n",
      "Ungroup:\n",
      "Es hat heute ununterbrochen regnen, es hat heute ununterbrochen gearbeitet.\n",
      "\n",
      "Ungroup:\n",
      "Es hat heute ununterbro"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Tell me a joke\",\n",
    "    \"Translate this to German: 'It has been raining non-stop today.'\"\n",
    "]\n",
    "\n",
    "# Run batch generation\n",
    "results = llm.batch(prompts)\n",
    "\n",
    "# Nicely format the output\n",
    "for i, (prompt, response) in enumerate(zip(prompts, results), 1):\n",
    "    print(f\"\\nPrompt {i}: {prompt}\")\n",
    "    print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a0e03-4f13-474e-ba70-361bfbfef70b",
   "metadata": {},
   "source": [
    "#### llm.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb04bb-2c6c-450a-bae0-40ac0144ab04",
   "metadata": {},
   "source": [
    "`llm.generate()` yields much more output than `llm.batch()` and is used if you actually want more metadata, such as the token count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "802ffd32-a6bc-4319-875c-ea1dffc5e324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='\\n\\nA luxury African safari can be experienced in various countries in Africa. However, the top luxury safari destinations in Africa are Tanzania, Kenya, Botswana, South Africa, Namibia, Zambia, and Zimbabwe. These countries offer an unforgettable luxury safari experience with first-class accommodation, expert guides, and magnificent wildlife sightings.\\n\\nWhat is the best time to visit?\\n\\nThe best time to visit for a luxury safari in Africa is during the dry season, which typically runs from June to October. During this time, the weather is cooler, and the vegetation is thinner, making it easier to spot wildlife. However, the best time')], [Generation(text='\\n\\n1. Everglades National Park, Florida: This park has some of the most unique wildlife in the United States, and offers unbeatable opportunities for hiking, kayaking, and fishing. It is also home to the famous Anhinga Trail, which is great for bird watching. If you‚Äôre looking for a more remote experience, you can head to the backcountry areas where you can camp away from the crowds.\\n\\n2. The Grand Canyon, Arizona: This is one of the most iconic national parks in the United States, and for good reason. The views are simply stunning, and there are plenty of opportunities for hiking, camping, and river rafting. The Bright Angel')]] llm_output=None run=[RunInfo(run_id=UUID('019b616b-077e-7dc1-bd87-a2376917bd48')), RunInfo(run_id=UUID('019b616b-077e-7dc1-bd87-a24c741672c5'))] type='LLMResult'"
     ]
    }
   ],
   "source": [
    "results = llm.generate([\"Where should my customer go for a luxurious Safari?\",\n",
    "                     \"What are your top three suggestions for backpacking destinations?\"])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df5955f-e494-42bd-b21e-416b73cd5fb5",
   "metadata": {},
   "source": [
    "We need to prittyfy the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf0c3c6a-eab9-4b6c-a785-63f799cc23a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A luxury African safari can be experienced in various countries in Africa. However, the top luxury safari destinations in Africa are Tanzania, Kenya, Botswana, South Africa, Namibia, Zambia, and Zimbabwe. These countries offer an unforgettable luxury safari experience with first-class accommodation, expert guides, and magnificent wildlife sightings.\n",
      "\n",
      "What is the best time to visit?\n",
      "\n",
      "The best time to visit for a luxury safari in Africa is during the dry season, which typically runs from June to October. During this time, the weather is cooler, and the vegetation is thinner, making it easier to spot wildlife. However, the best time\n",
      "\n",
      "\n",
      "1. Everglades National Park, Florida: This park has some of the most unique wildlife in the United States, and offers unbeatable opportunities for hiking, kayaking, and fishing. It is also home to the famous Anhinga Trail, which is great for bird watching. If you‚Äôre looking for a more remote experience, you can head to the backcountry areas where you can camp away from the crowds.\n",
      "\n",
      "2. The Grand Canyon, Arizona: This is one of the most iconic national parks in the United States, and for good reason. The views are simply stunning, and there are plenty of opportunities for hiking, camping, and river rafting. The Bright Angel"
     ]
    }
   ],
   "source": [
    "for gen in results.generations:\n",
    "    print(gen[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd0f876-80b6-4047-afb2-4ede13659e9d",
   "metadata": {},
   "source": [
    "#### llm.stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec2cf74f-410b-4581-bce2-5a362ee7ae2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Once upon a time, there was a cat. This cat was unlike any other, for it had magical powers. This cat could do anything it wanted, and it had a special ability to heal people. This cat was loved by all who met it, and it traveled the world, helping people in need.\n",
      "\n",
      "One day, the cat encountered a sickly old woman who had no one to care for her. The cat knew that this woman was in desperate need of help, so it decided to stay with her and take care of her. The woman was overjoyed to have the cat by her side, and soon she was feeling better.\n",
      "\n",
      "As the two grew closer, the woman and the cat formed an unbreak"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\"Tell me a story about a cat.\"):\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b731f92-425d-4958-9a43-0abf290e5f95",
   "metadata": {},
   "source": [
    "### Model Types in LangChain\n",
    "\n",
    "LangChain supports two main types of language models:\n",
    "\n",
    "| Model Type     | Description                                                  | Examples                              |\n",
    "|----------------|--------------------------------------------------------------|----------------------------------------|\n",
    "| **LLMs**       | Models that take a plain text string as input and return generated text | GPT-2, Falcon, LLaMA, Mistral (raw)    |\n",
    "| **Chat Models**| Models that work with structured chat messages (system, user, assistant) | GPT-4, Claude, LLaMA-Instruct, Mistral-Instruct|\n",
    "\n",
    "---\n",
    "\n",
    "**Why the distinction?**\n",
    "\n",
    "Chat models are designed to understand multi-turn conversation and role-based prompting. Their input format includes a structured message history, making them ideal for:\n",
    "- Instruction following\n",
    "- Contextual reasoning\n",
    "- Assistant-like behavior\n",
    "\n",
    "LLMs, on the other hand, expect a single flat prompt string. They still power many applications and are worth understanding, especially when using older models.\n",
    "\n",
    "---\n",
    "\n",
    "**Do Chat Models matter more now?**\n",
    "\n",
    "Yes ‚Äî most modern instruction-tuned models (like GPT-4, Claude, Mistral-Instruct, or LLaMA-3-Instruct) are designed as chat models, and LangChain's agent and memory systems are built around them.\n",
    "\n",
    "However, LLMs are still important:\n",
    "- Some models only support the LLM interface\n",
    "- LLMs are useful in batch processing and structured generation\n",
    "- Understanding their behavior helps you build better prompts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6d8dffd-f286-42a3-877d-c439d11e62a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LLM-style output ---\n",
      "\n",
      "‚ÄúLangChain is an AI-powered application that allows developers to easily integrate natural language processing into their projects, enabling them to create more sophisticated and intelligent chatbots, virtual assistants, and search engines.‚Äù\n",
      "\n",
      "Explain the benefits of using LangChain.\n",
      "LangChain offers several benefits to developers who use it in their projects. Some of these benefits include:\n",
      "\n",
      "1. Easy integration: LangChain is designed to be easily integrated into existing projects, allowing developers to quickly add natural language processing capabilities to their applications.\n",
      "\n",
      "2. Increased intelligence: By leveraging the power of AI, LangChain enables developers to create chatbots, virtual assistants, and search engines that are more intelligent and can provide more accurate and\n",
      "--- Chat-style output ---\n",
      "LangChain is an open-source Python library that enables developers to build applications with large language models, such as chatbots, language translation, and text summarization, by providing tools and techniques to work with and understand natural language data."
     ]
    }
   ],
   "source": [
    "# Plain LLM (single prompt string)\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline)\n",
    "print(\"--- LLM-style output ---\\n\")\n",
    "print(llm.invoke(\"Explain LangChain in one sentence.\"))\n",
    "\n",
    "# Use as a ChatModel (structured messages)\n",
    "chat_llm = ChatHuggingFace(llm=llm)\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
    "    HumanMessage(content=\"Explain LangChain in one sentence.\")\n",
    "]\n",
    "print(\"\\n--- Chat-style output ---\\n\")\n",
    "print(chat_llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d6131-8adc-4074-a892-229fa6aa62b8",
   "metadata": {},
   "source": [
    "The raw output you're seeing includes special chat formatting tokens (like <|im_start|>, <|im_end|>, etc.) which are used internally by the model (e.g., Mistral, LLaMA, GPT-J-style models) to distinguish between roles in a chat.\n",
    "\n",
    "These tokens help the model understand who is speaking, but they're not intended for humans to see. <br>\n",
    "<br>\n",
    "So, to prettyfy the ouput we will define a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf6b6a61-d684-422c-a136-9747508a6cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      " LangChain is an open-source Python library designed to handle and process natural language data, allowing for the creation of AI models that can interact with and reason about natural language input."
     ]
    }
   ],
   "source": [
    "def clean_output(raw: str) -> str:\n",
    "    # If the assistant marker is in the output, split on it and take the last part\n",
    "    if \"<|im_start|>assistant\" in raw:\n",
    "        return raw.split(\"<|im_start|>assistant\")[-1].replace(\"<|im_end|>\", \"\").strip()\n",
    "    return raw.strip()\n",
    "\n",
    "raw_output = chat_llm.invoke(messages).content\n",
    "cleaned = clean_output(raw_output)\n",
    "print(\"Cleaned Response:\\n\",cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149c68d7-2cce-429a-b34c-c02721c15104",
   "metadata": {},
   "source": [
    "An even simpler approach would be to pass the following argument earlier on:\n",
    "```\n",
    "llm = HuggingFacePipeline(pipeline=text_pipe, model_kwargs={\"clean_up_tokenization_spaces\": True})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a359ab1-39d5-4160-94de-1358f170b870",
   "metadata": {},
   "source": [
    "**Confused?** <br>\n",
    "You are not alone. Until recently, LangChain had a different wrapper for LLMs and Chat Models, but in recent versions of LangChain, the HuggingFacePipeline class implements the ChatModel interface under the hood ‚Äî it can accept structured chat messages (SystemMessage, HumanMessage, etc.) even though it wasn't originally designed to.\n",
    "\n",
    "So yes:\n",
    "You can now do:\n",
    "```\n",
    "llm = HuggingFacePipeline(pipeline=text_pipe)\n",
    "response = llm.invoke([\n",
    "    SystemMessage(content=\"You are a helpful legal assistant.\"),\n",
    "    HumanMessage(content=\"Simplify this clause: ...\")\n",
    "])\n",
    "```\n",
    "Even though you're not explicitly using ChatHuggingFace, LangChain detects the message types and processes them correctly using the underlying text-generation model.\n",
    "<br>\n",
    "<br>\n",
    "The same would apply if you used a remotly hosted LLM/Chat Model through an API:\n",
    "```\n",
    "from langchain_openai import ChatOpenAI\n",
    "chat = ChatOpenAI(openai_api_key=api_key)\n",
    "result = chat.invoke([HumanMessage(content=\"Can you tell me a fact about Dolphins?\")])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb524f31-3d3a-4a4b-bc00-d21843490193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (AIMessage, HumanMessage, SystemMessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "225a8a9c-bb3c-46fa-b57d-3bc0e0696885",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"clean_up_tokenization_spaces\": True})\n",
    "chat_llm = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c6555a0-3bfb-49e3-97aa-186eb5a528a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat_llm.invoke([HumanMessage(content=\"Can you tell me a fact about dolphins?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58180fce-e04b-41d2-9ae8-87a41b232890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Dolphins are highly intelligent, social mammals that live in a variety of marine habitats all over the world. They have a complex system of communication using clicks, whistles, and body language. They are also known for their playful behavior and strong family bonds.', additional_kwargs={}, response_metadata={}, id='lc_run--019b616b-9c17-7163-81fd-6bd9cdf1012a-0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "375586e2-7880-49c5-abf3-dbdf70593b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dolphins are highly intelligent, social mammals that live in a variety of marine habitats all over the world. They have a complex system of communication using clicks, whistles, and body language. They are also known for their playful behavior and strong family bonds."
     ]
    }
   ],
   "source": [
    "print(clean_output(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be3fb5bc-13e3-45de-98a9-45fe1361d30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat_llm.invoke([SystemMessage(content='You are a gumpy 5-year old child who only wants to get new toys and not answer questions'),\n",
    "               HumanMessage(content='Can you tell me a fact about dophins?')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "013daa46-53e4-47dc-83c9-5037c8286feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No."
     ]
    }
   ],
   "source": [
    "print(clean_output(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c34f9d0c-3a14-4595-a1fc-96f25c5b206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset"
     ]
    }
   ],
   "source": [
    "result = chat_llm.invoke(\n",
    "                [SystemMessage(content='You are a University Professor'),\n",
    "               HumanMessage(content='Can you tell me a fact about dolphins?')]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8c5f0ec-3989-461b-abd3-d1494cdc9b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dolphins are highly intelligent marine mammals known for their playful and social nature. One interesting fact about them is that they have a complex language and can communicate using a variety of clicks, whistles, and body movements, making them one of the few animals known to have a sophisticated communication system."
     ]
    }
   ],
   "source": [
    "print(clean_output(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73f872ab-8822-4155-974f-94ccd1c6fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat_llm.generate([\n",
    "    [\n",
    "        SystemMessage(content='You are a University Professor.'),\n",
    "        HumanMessage(content='Can you tell me a fact about dolphins?')\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content='You are a University Professor.'),\n",
    "        HumanMessage(content='What is the difference between whales and dolphins?')\n",
    "    ]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "099f89a0-115e-4147-8d71-618fd90cab83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 1:\n",
      "Certainly! Did you know that dolphins are one of the most intelligent animals on the planet? They have a large brain-to-body size ratio and are known for their problem-solving abilities and complex social behavior.\n",
      "\n",
      "Prompt 2:\n",
      "Whales and dolphins are both marine mammals, but they belong to different families. Whales belong to the family Cetacea and are larger in size, with some species reaching lengths of over 100 feet. Dolphins, on the other hand, belong to the family Delphinidae and are generally smaller in size, with most species not exceeding 30 feet in length. Additionally, whales have a more streamlined body shape, while dolphins have a more rounded body shape. Whales also have a blowhole on the top of their heads, while dolphins have two blowholes on the left side of their heads. \n",
      "The two groups also have different feeding habits,"
     ]
    }
   ],
   "source": [
    "for i, generation in enumerate(result.generations, 1):\n",
    "    raw = generation[0].text\n",
    "    cleaned = clean_output(raw)\n",
    "    print(f\"\\nPrompt {i}:\\n{cleaned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ddef5f18-3f61-488d-a75d-1fc93794f40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0"
     ]
    }
   ],
   "source": [
    "# Create a text generation pipeline\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    device_map=\"auto\",\n",
    "    return_full_text=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "\n",
    "# Wrap in LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"clean_up_tokenization_spaces\": True})\n",
    "chat_llm = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8929bb69-a993-4acb-b2c5-84550dd6eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token_id = tokenizer.eos_token_id\n",
    "result = chat_llm.generate([\n",
    "    [\n",
    "        SystemMessage(content='You are a University Professor.'),\n",
    "        HumanMessage(content='Can you tell me a fact about dolphins?')\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content='You are a University Professor.'),\n",
    "        HumanMessage(content='What is the difference between whales and dolphins?')\n",
    "    ]\n",
    "], eos_token_id=eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01734b2c-22e6-4fab-ac9e-43e83bf85d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 1:\n",
      "Dolphins are known for their intelligence and are considered one of the most intelligent species of animals. They have a complex social structure and are known for their ability to display affection towards each other. They are also known for their playful behavior and their ability to communicate using a series of clicks, whistles, and body movements.\n",
      "\n",
      "Prompt 2:\n",
      "Whales and dolphins are both marine mammals, but they belong to different families and have several anatomical and behavioral differences. Whales belong to the family Cetacea and have fully adapted to living in the water, while dolphins belong to the family Delphinidae and have retained some characteristics of their terrestrial ancestors. Some key differences between whales and dolphins are as follows:\n",
      "\n",
      "1. Size: Whales are generally larger than dolphins, with the largest species being the blue whale, which can grow up to 100 feet in length, while the largest dolphin species, the killer whale, can grow up to 32 feet.\n",
      "\n",
      "2. Body Shape: Whales tend to have a more streamlined body shape, while dolphins have a more rounded body shape. Whales also have a dorsal fin that varies in shape and size, while dolphins have a well-defined dorsal fin.\n",
      "\n",
      "3. Diet: Whales are mostly filter feeders, eating tiny organisms like krill, while dolphins hunt and eat larger fish and squid.\n",
      "\n",
      "4. Vocalizations: Whales use a variety of vocalizations, including songs and vocalizations used for communication and echolocation. Dolphins, on the other hand, use echolocation to navigate and find prey, but they also use vocalizations for communication, socializing, and mating.\n",
      "\n",
      "5. Reproduction: Whales usually give birth to one calf at a time, while dolphins can have one or two calves at a time. Dolphins also have a longer gestation period than whales.\n",
      "\n",
      "In summary, while whales and dolphins share many similarities, they are different in terms of size, body shape, diet, vocalizations, and reproductive behavior."
     ]
    }
   ],
   "source": [
    "for i, generation in enumerate(result.generations, 1):\n",
    "    raw = generation[0].text\n",
    "    cleaned = clean_output(raw)\n",
    "    print(f\"\\nPrompt {i}:\\n{cleaned}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d5902-6a1f-42c0-b8bc-1cc56e3254b3",
   "metadata": {},
   "source": [
    "This code connects Hugging Face Transformers to LangChain‚Äôs prompt management:\n",
    "- Load model into Hugging Face pipeline.\n",
    "- Wrap it in LangChain (HuggingFacePipeline).\n",
    "- Build structured prompts (system + user).\n",
    "- Format prompt with user input.\n",
    "- Send it to the model and get a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa51ce40-782d-4112-9af0-6c160fe221c9",
   "metadata": {},
   "source": [
    "<br>\n",
    "Feel free to experiment with different system and human prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efdf0dda-f69c-487d-bdc2-a58b4acfa76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of France is Paris. Paris is special for many reasons, including its rich history, iconic landmarks, and vibrant culture. Paris is home to numerous museums housing some of the world's most famous art collections, including the Louvre Museum, which houses the famous painting \"Mona Lisa\" by Leonardo da Vinci. The city is also known for its architectural icons such as the Eiffel Tower, Notre-Dame Cathedral, and the Arc de Triomphe. It is a global center for art, fashion, gastronomy, and culture. Additionally, Paris is known for its romantic atmosphere and is often referred to as the \"City of Love.\""
     ]
    }
   ],
   "source": [
    "# Create a text generation pipeline\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    device_map=\"auto\",\n",
    "    return_full_text=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "\n",
    "# Wrap in LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline)\n",
    "\n",
    "# Define the system and user messages\n",
    "system_message_1 = SystemMessagePromptTemplate.from_template(\"You are a polite and professional assistant who answers concisely.\")\n",
    "system_message_2 = SystemMessagePromptTemplate.from_template(\"You're a friendly AI that gives fun and engaging responses.\")\n",
    "system_message_3 = SystemMessagePromptTemplate.from_template(\"You are a research assistant providing precise, well-cited responses.\")\n",
    "\n",
    "user_message = HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "\n",
    "# Create a prompt template\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_3, user_message])\n",
    "\n",
    "# Format the prompt\n",
    "formatted_prompt = chat_prompt.format_messages(question=\"What is the capital of France and what is special about it?\")\n",
    "\n",
    "# Run inference\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5026430-caaa-4f9c-938e-328b2f383c5b",
   "metadata": {},
   "source": [
    "### Extra Parameters and Args\n",
    "\n",
    "Here we add in some extra parameters and args, to get the model to respond in a certain way.\n",
    "<br>\n",
    "Some of the most important parameters are:\n",
    "\n",
    "\n",
    "| **Parameter**        | **Purpose**                                                                 | **Range / Default**       | **Analogy / Effect**                        |\n",
    "|----------------------|------------------------------------------------------------------------------|----------------------------|---------------------------------------------|\n",
    "| `do_sample`          | Enables random sampling instead of greedy or beam-based decoding             | `True` / `False`           | üé≤ Adds randomness to output                |\n",
    "| `temperature`        | Controls randomness of token selection                                       | `> 0`, typically `0.7‚Äì1.0` | üå°Ô∏è Higher = more creative / chaotic         |\n",
    "| `top_p`              | Nucleus sampling: sample from top % of likely tokens                         | `0.0‚Äì1.0`, default `1.0`   | üß† Focuses on most probable words           |\n",
    "| `num_beams`          | Beam search: explore multiple continuations and pick the best                | `1+`, default `1`          | üîç Smart guessing with multiple options     |\n",
    "| `repetition_penalty` | Penalizes repeated tokens to reduce redundancy                               | `‚â• 1.0`, e.g. `1.2`        | ‚ôªÔ∏è Discourages repetition                   |\n",
    "| `max_new_tokens`     | Limits the number of tokens the model can generate **per prompt**            | Integer, e.g. `300`        | ‚úÇÔ∏è Controls response length                 |\n",
    "| `eos_token_id`       | Token ID that forces the model to stop when encountered                      | Integer                    | üõë Defines end of output (if supported)     |\n",
    "\n",
    "#### Detailed Explanation of Generation Parameters\n",
    "\n",
    "##### `do_sample=True`\n",
    "- If `False`: the model always picks the **most likely next token** (deterministic, greedy decoding).\n",
    "- If `True`: the model will **randomly sample** from a probability distribution over tokens (non-deterministic).\n",
    "- Required if you want `temperature` or `top_p` to have any effect.\n",
    "\n",
    "‚úÖ Enables creativity and variation  \n",
    "‚ùå Disables reproducibility (unless random seed is fixed)\n",
    "\n",
    "---\n",
    "\n",
    "##### `temperature=1.0`\n",
    "- Controls the **randomness** or \"creativity\" of the output.\n",
    "- Lower values ‚Üí more predictable (safe), higher values ‚Üí more diverse (risky).\n",
    "- Affects how \"flat\" or \"peaky\" the probability distribution is during sampling.\n",
    "\n",
    "**Typical values:**\n",
    "- `0.0` ‚Üí deterministic (most likely token only)\n",
    "- `0.7‚Äì1.0` ‚Üí balanced\n",
    "- `>1.5` ‚Üí chaotic, often incoherent\n",
    "\n",
    "---\n",
    "\n",
    "##### üîπ `top_p=0.9` *(a.k.a. nucleus sampling)*\n",
    "- The model samples only from the **top tokens whose cumulative probability ‚â• `p`**.\n",
    "- Unlike `top_k`, this is dynamic based on the shape of the probability distribution.\n",
    "- Often used in combination with `temperature`.\n",
    "\n",
    "‚úÖ Focuses output on high-probability words  \n",
    "‚ùå Too low ‚Üí model may miss useful words\n",
    "\n",
    "---\n",
    "\n",
    "##### `num_beams=4` *(beam search)*\n",
    "- Explores **multiple candidate completions** and picks the best one based on likelihood.\n",
    "- Slower, but often more optimal (when `do_sample=False`).\n",
    "- Does not work with sampling (`do_sample=True`).\n",
    "\n",
    "**Typical values:**\n",
    "- `1` = greedy decoding  \n",
    "- `3‚Äì5` = moderate beam search  \n",
    "- `>10` = can become very slow\n",
    "\n",
    "---\n",
    "\n",
    "##### `repetition_penalty=1.2`\n",
    "- Penalizes tokens that have already been generated, making the model **less likely to repeat itself**.\n",
    "- Higher values reduce repetition but may hurt fluency.\n",
    "\n",
    "‚úÖ Helps avoid \"looping\" or redundant outputs  \n",
    "üìù Use with long-form or factual responses\n",
    "\n",
    "---\n",
    "\n",
    "##### `max_new_tokens=300`\n",
    "- Sets the **maximum number of tokens** the model is allowed to generate in the response.\n",
    "- Does not include input prompt tokens.\n",
    "\n",
    "‚úÖ Controls output length  \n",
    "‚úÖ Prevents runaway generation or memory issues\n",
    "‚úÖ Prevents truncated output.\n",
    "\n",
    "---\n",
    "\n",
    "##### `eos_token_id`\n",
    "- Tells the model to **stop generation** once it emits this token ID.\n",
    "- Useful for enforcing custom stopping conditions.\n",
    "\n",
    "Optional ‚Äî most models use their own `<eos>` or `</s>` tokens by default.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3314edc-1689-49ce-94bb-b198a8ca1059",
   "metadata": {},
   "source": [
    "Feel free to experiment with these parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a01b796b-364e-42ae-b866-e2454ad9c679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0"
     ]
    }
   ],
   "source": [
    "# Create a text generation pipeline\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,  # Balanced creativity (was 5.0 - too chaotic)\n",
    "    top_p=0.9,\n",
    "    max_new_tokens=300,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Wrap in LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline)\n",
    "chat_llm = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe01c99b-b14f-4358-a532-765a19bb5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat_llm.invoke([HumanMessage(content='Can you tell me a fact about Earth?')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91a67f85-9c18-4ac3-9b2e-ff021c121a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Earth is the third planet from the Sun and the only known planet in the Universe capable of supporting life. It is approximately 79% water-covered, with the remaining surface area being land."
     ]
    }
   ],
   "source": [
    "print(clean_output(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964b698-ba1b-4c2f-a23a-5e757dd84e2a",
   "metadata": {},
   "source": [
    "### Caching\n",
    "\n",
    "Making the same exact request often? You could use a cache to store results **note, you should only do this if the prompt is the exact same and the historical replies are okay to return**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1639f253-5b37-4ffc-b028-c22b3df2b877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mars is the fourth planet from the Sun and is often referred to as the \"Red Planet\" due to its reddish appearance caused by iron oxide (rust) on its surface. It is the second-smallest planet in the solar system, and it has the largest mountain in the solar system, called Olympus Mons, which is three times the height of Mount Everest. Mars has the longest and deepest canyon system in the solar system, called Valles Marineris, which is over 4,000 km long and up to 7 km deep. Mars has a very thin atmosphere, mainly composed of carbon dioxide, with traces of nitrogen and argon. Its average temperature is around -63 degrees Fahrenheit (-53 degrees Celsius), and it has the coldest known temperature in the solar system, which is -191 degrees Fahrenheit (-120 degrees Celsius)."
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain_community.cache import InMemoryCache\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "print(clean_output(chat_llm.invoke(\"Tell me a fact about Mars\").content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d706fd5-6067-4d7e-80ab-cd96ccd4a912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mars is the fourth planet from the Sun and is often referred to as the \"Red Planet\" due to its reddish appearance caused by iron oxide (rust) on its surface. It is the second-smallest planet in the Solar System, and is about half the size of Earth. The Mars atmosphere is very thin and is only 1% as dense as Earth's, primarily composed of carbon dioxide, nitrogen, and argon, with trace amounts of other gases. Mars has the largest mountain in the Solar System, Olympus Mons, which is three times higher than Mount Everest. It also has the deepest canyon in the Solar System, Valles Marineris, which is over 4 kilometers deep and 4,000 kilometers long. Mars has two small, natural satellites, Phobos and Deimos, which are irregularly shaped and are believed to be captured asteroids."
     ]
    }
   ],
   "source": [
    "# You will notice this reply is instant!\n",
    "print(clean_output(chat_llm.invoke(\"Tell me a fact about Mars\").content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbf6d5d-6ddd-4741-8e9e-cf9a14c653e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd788759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Unload Ollama Model & Shutdown Kernel ===\n",
    "# Unloads the model from GPU memory before shutting down\n",
    "\n",
    "try:\n",
    "    import ollama\n",
    "    print(f\"Unloading Ollama model: {OLLAMA_LLM_MODEL}\")\n",
    "    ollama.generate(model=OLLAMA_LLM_MODEL, prompt=\"\", keep_alive=0)\n",
    "    print(\"Model unloaded from GPU memory\")\n",
    "except Exception as e:\n",
    "    print(f\"Model unload skipped: {e}\")\n",
    "\n",
    "# Shut down the kernel to fully release resources\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
