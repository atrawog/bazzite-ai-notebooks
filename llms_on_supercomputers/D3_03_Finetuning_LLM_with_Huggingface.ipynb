{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2be8feac-0c53-4601-8f0c-40b369e79ec4",
   "metadata": {},
   "source": [
    "# Fine-tuning an LLM with Hugging Face Trainer\n",
    "\n",
    "In the previous notebook we fine-tuned a language model using a **manual PyTorch training loop**:\n",
    "\n",
    "- custom `Dataset` and `DataLoader`\n",
    "- explicit `optimizer.step()`, `scheduler.step()`, `loss.backward()`\n",
    "- manual evaluation\n",
    "\n",
    "This notebook uses the **same model and dataset**, but relies on **Hugging Face `datasets` and `Trainer`** to handle most of the training boilerplate.\n",
    "\n",
    "Goal:\n",
    "\n",
    "- Show how to fine-tune the same model and dataset with less code.\n",
    "- Connect the high-level `Trainer` API to the manual loop from the previous notebook.\n",
    "\n",
    "We still:\n",
    "\n",
    "- Load a pretrained Llama model from disk.\n",
    "- Load the Guanaco / OpenAssistant JSONL dataset from disk.\n",
    "- Tokenize the data for causal language modeling.\n",
    "- Train for a small number of steps.\n",
    "- Run inference and compare base vs fine-tuned model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e272ff12",
   "metadata": {},
   "source": [
    "> **Bazzite-AI Setup Required**  \n",
    "> Run `D0_00_Bazzite_AI_Setup.ipynb` first to verify GPU access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08890364-d781-4c18-8b24-e34521b2ab48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1+cu130'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e33149a-ad5e-4756-87a3-4367172fc800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(HF_LLM_MODEL='TinyLlama/TinyLlama-1.1B-Chat-v1.0', output_dir='ft_model_trainer', max_length=256, batch_size=1, num_epochs=1, learning_rate=5e-06, weight_decay=0.01, warmup_ratio=0.1, gradient_accumulation_steps=16, train_subset_size=500, val_subset_size=100, seed=42, device='cuda')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Model from HuggingFace Hub (same as D3_02)\n",
    "    HF_LLM_MODEL: str = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "    # Output directory for fine-tuned model\n",
    "    output_dir: str = \"ft_model_trainer\"\n",
    "\n",
    "    # Data\n",
    "    max_length: int = 256\n",
    "\n",
    "    # Optimization\n",
    "    batch_size: int = 1          # reduced for memory efficiency\n",
    "    num_epochs: int = 1\n",
    "    learning_rate: float = 5e-6\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_ratio: float = 0.1\n",
    "    gradient_accumulation_steps: int = 16  # increased to compensate\n",
    "\n",
    "    # For demo, use small subsets\n",
    "    train_subset_size: int = 500\n",
    "    val_subset_size: int = 100\n",
    "\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cfg = Config()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a217238-75c5-4f63-b868-7472d59c3786",
   "metadata": {},
   "source": [
    "The hyperparameters here mirror those from the manual PyTorch notebook:\n",
    "\n",
    "- `batch_size`, `num_epochs`, `learning_rate`, `weight_decay`, `warmup_ratio`, and `gradient_accumulation_steps` have the same meaning.\n",
    "- `max_length` controls the maximum sequence length after tokenization.\n",
    "- For the demo we only use small subsets of the full dataset (`train_subset_size` and `val_subset_size`) so that fine-tuning finishes quickly.\n",
    "\n",
    "The difference is that we will pass these values into `TrainingArguments` instead of using them directly in a manual training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc8a4dcb-a578-4435-b8cd-353c12360af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_seed(seed: int):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d413229-108b-4f18-b04b-140b46e21afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pad token: </s> ID: 2\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Number of parameters: 1100.0M\n",
       "Model dtype: torch.bfloat16\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.HF_LLM_MODEL)\n",
    "\n",
    "# Many causal LMs do not define a pad token by default\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Pad token:\", tokenizer.pad_token, \"ID:\", tokenizer.pad_token_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.HF_LLM_MODEL,\n",
    "    dtype=torch.bfloat16,  # Use bfloat16 for memory efficiency\n",
    ")\n",
    "model.to(cfg.device)\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {n_params / 1e6:.1f}M\")\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd4f272-0fd9-4d0f-8f60-b9fa3189349b",
   "metadata": {},
   "source": [
    "## Dataset format and inference prompt\n",
    "\n",
    "We use the Guanaco / OpenAssistant dataset. Each entry in the JSONL files has a single field:\n",
    "\n",
    "```python\n",
    "\"text\": \"### Human: <instruction>### Assistant: <ideal answer>\"\n",
    "```\n",
    "During fine-tuning:\n",
    "\n",
    "- The model sees the full text string.\n",
    "\n",
    "- It is trained as a causal language model to predict the next token at each position.\n",
    "\n",
    "At inference time we only have a new user instruction. We must recreate the same format the model saw during training, for example:\n",
    "```python\n",
    "### Human: How do I build a PC?### Assistant:\n",
    "```\n",
    "and let the model generate the continuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fe37041-dada-436e-95cf-a466f076b8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_prompt_for_inference(user_instruction: str) -> str:\n",
    "    \"\"\"\n",
    "    Build a Guanaco-style prompt for a new instruction at inference time.\n",
    "    The dataset format looks like:\n",
    "        \"### Human: ...### Assistant: ...\"\n",
    "    \"\"\"\n",
    "    return f\"### Human: {user_instruction}### Assistant:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9ab24eb-d38f-4727-8b51-14523a7ef2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Repo card metadata block was not found. Setting CardData to empty.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 9846\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 518\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset from HuggingFace Hub (same as D3_02)\n",
    "dataset = load_dataset(\"timdettmers/openassistant-guanaco\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e310f09-67cd-462c-994e-65f9cd1ebbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 9846\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 518\n",
       "    })\n",
       "})\n",
       "Example training entry:\n",
       "{'text': '### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\\n\\nRecent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\\n\\nOverall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\\n\\nReferences:\\nBivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.### Human: Now explain it to a dog'}\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(500, 100)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(\"Example training entry:\")\n",
    "print(dataset[\"train\"][0])\n",
    "\n",
    "# For the demo, restrict to small subsets\n",
    "train_dataset = dataset[\"train\"].select(range(cfg.train_subset_size))\n",
    "val_dataset   = dataset[\"test\"].select(range(cfg.val_subset_size))  # Note: \"test\" not \"validation\"\n",
    "\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4831db-274e-4e49-9bea-fcf66b720fbe",
   "metadata": {},
   "source": [
    "## Tokenization and preprocessing with `datasets`\n",
    "\n",
    "Instead of writing a custom PyTorch `Dataset` class and a `DataLoader`, we now:\n",
    "\n",
    "1. Use `datasets.load_dataset` to read the JSONL files.\n",
    "2. Define a tokenization function that:\n",
    "   - tokenizes the `\"text\"` field,\n",
    "   - truncates or pads to `max_length`,\n",
    "   - sets `labels` equal to `input_ids` for causal language modeling.\n",
    "3. Apply this function to the whole dataset with `dataset.map(...)`.\n",
    "\n",
    "The result is a `Dataset` object that already returns tokenized fields, which `Trainer` can use directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3bee38ce-a916-481b-888b-25acca4e236a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1,\n",
       "  835,\n",
       "  12968,\n",
       "  29901,\n",
       "  1815,\n",
       "  366,\n",
       "  2436,\n",
       "  263,\n",
       "  3273,\n",
       "  18707,\n",
       "  1048,\n",
       "  278,\n",
       "  29527,\n",
       "  749,\n",
       "  310,\n",
       "  278,\n",
       "  1840,\n",
       "  376,\n",
       "  3712,\n",
       "  459,\n",
       "  1100,\n",
       "  29891,\n",
       "  29908,\n",
       "  297,\n",
       "  7766,\n",
       "  1199,\n",
       "  29973,\n",
       "  3529,\n",
       "  671,\n",
       "  6455,\n",
       "  4475,\n",
       "  304,\n",
       "  7037,\n",
       "  1601,\n",
       "  459,\n",
       "  1100,\n",
       "  583,\n",
       "  297,\n",
       "  278,\n",
       "  23390,\n",
       "  9999,\n",
       "  322,\n",
       "  274,\n",
       "  568,\n",
       "  8018,\n",
       "  5925,\n",
       "  29889,\n",
       "  2277,\n",
       "  29937,\n",
       "  4007,\n",
       "  22137,\n",
       "  29901,\n",
       "  376,\n",
       "  7185,\n",
       "  459,\n",
       "  1100,\n",
       "  29891,\n",
       "  29908,\n",
       "  14637,\n",
       "  304,\n",
       "  263,\n",
       "  9999,\n",
       "  3829,\n",
       "  988,\n",
       "  727,\n",
       "  338,\n",
       "  871,\n",
       "  697,\n",
       "  1321,\n",
       "  7598,\n",
       "  363,\n",
       "  263,\n",
       "  3153,\n",
       "  1781,\n",
       "  470,\n",
       "  2669,\n",
       "  29889,\n",
       "  512,\n",
       "  7766,\n",
       "  1199,\n",
       "  29892,\n",
       "  445,\n",
       "  1840,\n",
       "  338,\n",
       "  10734,\n",
       "  8018,\n",
       "  297,\n",
       "  278,\n",
       "  10212,\n",
       "  9999,\n",
       "  29892,\n",
       "  988,\n",
       "  263,\n",
       "  1601,\n",
       "  459,\n",
       "  1100,\n",
       "  29891,\n",
       "  5703,\n",
       "  261,\n",
       "  756,\n",
       "  7282,\n",
       "  3081,\n",
       "  975,\n",
       "  278,\n",
       "  281,\n",
       "  1179,\n",
       "  322,\n",
       "  1985,\n",
       "  5855,\n",
       "  310,\n",
       "  1009,\n",
       "  22873,\n",
       "  29889,\n",
       "  450,\n",
       "  10122,\n",
       "  310,\n",
       "  263,\n",
       "  1601,\n",
       "  459,\n",
       "  1100,\n",
       "  29891,\n",
       "  508,\n",
       "  1121,\n",
       "  297,\n",
       "  5224,\n",
       "  281,\n",
       "  1179,\n",
       "  322,\n",
       "  12212,\n",
       "  5703,\n",
       "  358,\n",
       "  28602,\n",
       "  1907,\n",
       "  363,\n",
       "  17162,\n",
       "  29892,\n",
       "  408,\n",
       "  278,\n",
       "  5703,\n",
       "  261,\n",
       "  756,\n",
       "  2217,\n",
       "  297,\n",
       "  1760,\n",
       "  573,\n",
       "  304,\n",
       "  7910,\n",
       "  281,\n",
       "  1179,\n",
       "  470,\n",
       "  3867,\n",
       "  2253,\n",
       "  1985,\n",
       "  5855,\n",
       "  29889,\n",
       "  13,\n",
       "  13,\n",
       "  4789,\n",
       "  296,\n",
       "  5925,\n",
       "  756,\n",
       "  15659,\n",
       "  7037,\n",
       "  1601,\n",
       "  459,\n",
       "  1100,\n",
       "  583,\n",
       "  297,\n",
       "  6397,\n",
       "  2722,\n",
       "  1316,\n",
       "  408,\n",
       "  3240,\n",
       "  737,\n",
       "  322,\n",
       "  5172,\n",
       "  9687,\n",
       "  29892,\n",
       "  988,\n",
       "  263,\n",
       "  2846,\n",
       "  2919,\n",
       "  14582,\n",
       "  2761,\n",
       "  263,\n",
       "  7282,\n",
       "  11910,\n",
       "  310,\n",
       "  278,\n",
       "  9999,\n",
       "  313,\n",
       "  29933,\n",
       "  440,\n",
       "  575,\n",
       "  669,\n",
       "  341,\n",
       "  728,\n",
       "  295,\n",
       "  29892,\n",
       "  29871,\n",
       "  29906,\n",
       "  29900,\n",
       "  29896,\n",
       "  29941,\n",
       "  467,\n",
       "  512,\n",
       "  1438,\n",
       "  6397,\n",
       "  2722,\n",
       "  29892,\n",
       "  17162,\n",
       "  4049,\n",
       "  3700,\n",
       "  4482,\n",
       "  281,\n",
       "  1179,\n",
       "  29892,\n",
       "  9078,\n",
       "  23633,\n",
       "  29892,\n",
       "  322,\n",
       "  12212,\n",
       "  289,\n",
       "  1191,\n",
       "  17225,\n",
       "  3081,\n",
       "  29892,\n",
       "  8236,\n",
       "  304,\n",
       "  263,\n",
       "  6434,\n",
       "  988,\n",
       "  896,\n",
       "  526,\n",
       "  14278,\n",
       "  373,\n",
       "  278,\n",
       "  5703,\n",
       "  261,\n",
       "  363,\n",
       "  1009,\n",
       "  7294,\n",
       "  22342,\n",
       "  29889,\n",
       "  910,\n",
       "  26307,\n",
       "  508,\n",
       "  1121,\n",
       "  297,\n",
       "  4340,\n",
       "  1462,\n",
       "  23881,\n",
       "  310,\n",
       "  281,\n",
       "  1179,\n",
       "  322],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [1,\n",
       "  835,\n",
       "  12968,\n",
       "  29901,\n",
       "  1815,\n",
       "  366,\n",
       "  2436,\n",
       "  263,\n",
       "  3273,\n",
       "  18707,\n",
       "  1048,\n",
       "  278,\n",
       "  29527,\n",
       "  749,\n",
       "  310,\n",
       "  278,\n",
       "  1840,\n",
       "  376,\n",
       "  3712,\n",
       "  459,\n",
       "  1100,\n",
       "  29891,\n",
       "  29908,\n",
       "  297,\n",
       "  7766,\n",
       "  1199,\n",
       "  29973,\n",
       "  3529,\n",
       "  671,\n",
       "  6455,\n",
       "  4475,\n",
       "  304,\n",
       "  7037,\n",
       "  1601,\n",
       "  459,\n",
       "  1100,\n",
       "  583,\n",
       "  297,\n",
       "  278,\n",
       "  23390,\n",
       "  9999,\n",
       "  322,\n",
       "  274,\n",
       "  568,\n",
       "  8018,\n",
       "  5925,\n",
       "  29889,\n",
       "  2277,\n",
       "  29937,\n",
       "  4007,\n",
       "  22137,\n",
       "  29901,\n",
       "  376,\n",
       "  7185,\n",
       "  459,\n",
       "  1100,\n",
       "  29891,\n",
       "  29908,\n",
       "  14637,\n",
       "  304,\n",
       "  263,\n",
       "  9999,\n",
       "  3829,\n",
       "  988,\n",
       "  727,\n",
       "  338,\n",
       "  871,\n",
       "  697,\n",
       "  1321,\n",
       "  7598,\n",
       "  363,\n",
       "  263,\n",
       "  3153,\n",
       "  1781,\n",
       "  470,\n",
       "  2669,\n",
       "  29889,\n",
       "  512,\n",
       "  7766,\n",
       "  1199,\n",
       "  29892,\n",
       "  445,\n",
       "  1840,\n",
       "  338,\n",
       "  10734,\n",
       "  8018,\n",
       "  297,\n",
       "  278,\n",
       "  10212,\n",
       "  9999,\n",
       "  29892,\n",
       "  988,\n",
       "  263,\n",
       "  1601,\n",
       "  459,\n",
       "  1100,\n",
       "  29891,\n",
       "  5703,\n",
       "  261,\n",
       "  756,\n",
       "  7282,\n",
       "  3081,\n",
       "  975,\n",
       "  278,\n",
       "  281,\n",
       "  1179,\n",
       "  322,\n",
       "  1985,\n",
       "  5855,\n",
       "  310,\n",
       "  1009,\n",
       "  22873,\n",
       "  29889,\n",
       "  450,\n",
       "  10122,\n",
       "  310,\n",
       "  263,\n",
       "  1601,\n",
       "  459,\n",
       "  1100,\n",
       "  29891,\n",
       "  508,\n",
       "  1121,\n",
       "  297,\n",
       "  5224,\n",
       "  281,\n",
       "  1179,\n",
       "  322,\n",
       "  12212,\n",
       "  5703,\n",
       "  358,\n",
       "  28602,\n",
       "  1907,\n",
       "  363,\n",
       "  17162,\n",
       "  29892,\n",
       "  408,\n",
       "  278,\n",
       "  5703,\n",
       "  261,\n",
       "  756,\n",
       "  2217,\n",
       "  297,\n",
       "  1760,\n",
       "  573,\n",
       "  304,\n",
       "  7910,\n",
       "  281,\n",
       "  1179,\n",
       "  470,\n",
       "  3867,\n",
       "  2253,\n",
       "  1985,\n",
       "  5855,\n",
       "  29889,\n",
       "  13,\n",
       "  13,\n",
       "  4789,\n",
       "  296,\n",
       "  5925,\n",
       "  756,\n",
       "  15659,\n",
       "  7037,\n",
       "  1601,\n",
       "  459,\n",
       "  1100,\n",
       "  583,\n",
       "  297,\n",
       "  6397,\n",
       "  2722,\n",
       "  1316,\n",
       "  408,\n",
       "  3240,\n",
       "  737,\n",
       "  322,\n",
       "  5172,\n",
       "  9687,\n",
       "  29892,\n",
       "  988,\n",
       "  263,\n",
       "  2846,\n",
       "  2919,\n",
       "  14582,\n",
       "  2761,\n",
       "  263,\n",
       "  7282,\n",
       "  11910,\n",
       "  310,\n",
       "  278,\n",
       "  9999,\n",
       "  313,\n",
       "  29933,\n",
       "  440,\n",
       "  575,\n",
       "  669,\n",
       "  341,\n",
       "  728,\n",
       "  295,\n",
       "  29892,\n",
       "  29871,\n",
       "  29906,\n",
       "  29900,\n",
       "  29896,\n",
       "  29941,\n",
       "  467,\n",
       "  512,\n",
       "  1438,\n",
       "  6397,\n",
       "  2722,\n",
       "  29892,\n",
       "  17162,\n",
       "  4049,\n",
       "  3700,\n",
       "  4482,\n",
       "  281,\n",
       "  1179,\n",
       "  29892,\n",
       "  9078,\n",
       "  23633,\n",
       "  29892,\n",
       "  322,\n",
       "  12212,\n",
       "  289,\n",
       "  1191,\n",
       "  17225,\n",
       "  3081,\n",
       "  29892,\n",
       "  8236,\n",
       "  304,\n",
       "  263,\n",
       "  6434,\n",
       "  988,\n",
       "  896,\n",
       "  526,\n",
       "  14278,\n",
       "  373,\n",
       "  278,\n",
       "  5703,\n",
       "  261,\n",
       "  363,\n",
       "  1009,\n",
       "  7294,\n",
       "  22342,\n",
       "  29889,\n",
       "  910,\n",
       "  26307,\n",
       "  508,\n",
       "  1121,\n",
       "  297,\n",
       "  4340,\n",
       "  1462,\n",
       "  23881,\n",
       "  310,\n",
       "  281,\n",
       "  1179,\n",
       "  322]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(batch):\n",
    "    \"\"\"\n",
    "    Tokenize the 'text' field for causal language modeling.\n",
    "\n",
    "    We:\n",
    "    - truncate or pad to cfg.max_length,\n",
    "    - set labels = input_ids (shift is handled by the model internally).\n",
    "    \"\"\"\n",
    "    enc = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=cfg.max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # For causal LM supervised fine-tuning, labels are often the same as input_ids\n",
    "    enc[\"labels\"] = enc[\"input_ids\"].copy()\n",
    "    return enc\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],  # drop original string to keep only tokenized fields\n",
    ")\n",
    "\n",
    "tokenized_val = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "tokenized_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20992266-df51-4369-846d-72a4b4c72ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure the datasets return PyTorch tensors\n",
    "tokenized_train.set_format(type=\"torch\")\n",
    "tokenized_val.set_format(type=\"torch\")\n",
    "\n",
    "tokenized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1df6b565-565f-46a2-9b12-adc94280df54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "average_tokens_across_devices=True,\n",
       "batch_eval_metrics=False,\n",
       "bf16=True,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=True,\n",
       "dataloader_prefetch_factor=None,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_do_concat_batches=True,\n",
       "eval_on_start=False,\n",
       "eval_steps=None,\n",
       "eval_strategy=IntervalStrategy.EPOCH,\n",
       "eval_use_gather_object=False,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=16,\n",
       "gradient_checkpointing=False,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=False,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=None,\n",
       "hub_revision=None,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_for_metrics=[],\n",
       "include_inputs_for_metrics=False,\n",
       "include_num_input_tokens_seen=no,\n",
       "include_tokens_per_second=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-06,\n",
       "length_column_name=length,\n",
       "liger_kernel_config=None,\n",
       "load_best_model_at_end=True,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=ft_model_trainer/runs/Dec29_02-12-28_25f2198b1a05,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=10,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_kwargs={},\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=loss,\n",
       "mp_parameters=,\n",
       "neftune_noise_alpha=None,\n",
       "no_cuda=False,\n",
       "num_train_epochs=1,\n",
       "optim=OptimizerNames.ADAMW_TORCH_FUSED,\n",
       "optim_args=None,\n",
       "optim_target_modules=None,\n",
       "output_dir=ft_model_trainer,\n",
       "overwrite_output_dir=False,\n",
       "parallelism_config=None,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=1,\n",
       "per_device_train_batch_size=1,\n",
       "prediction_loss_only=False,\n",
       "project=huggingface,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "restore_callback_states_from_checkpoint=False,\n",
       "resume_from_checkpoint=None,\n",
       "run_name=None,\n",
       "save_on_each_node=False,\n",
       "save_only_model=False,\n",
       "save_safetensors=True,\n",
       "save_steps=500,\n",
       "save_strategy=SaveStrategy.EPOCH,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torch_empty_cache_steps=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "trackio_space_id=trackio,\n",
       "use_cpu=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_liger_kernel=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.1,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.01,\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=cfg.output_dir,\n",
    "    per_device_train_batch_size=cfg.batch_size,\n",
    "    per_device_eval_batch_size=cfg.batch_size,\n",
    "    num_train_epochs=cfg.num_epochs,\n",
    "    learning_rate=cfg.learning_rate,\n",
    "    weight_decay=cfg.weight_decay,\n",
    "    warmup_ratio=cfg.warmup_ratio,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=cfg.gradient_accumulation_steps,\n",
    "    bf16=torch.cuda.is_available(),  # Use bf16 instead of fp16\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10d4e9b-d2e1-4656-8ebf-41ad0dab2e42",
   "metadata": {},
   "source": [
    "## Using `TrainingArguments` and `Trainer`\n",
    "\n",
    "`TrainingArguments` defines how training should work:\n",
    "\n",
    "- `per_device_train_batch_size`, `num_train_epochs`, `learning_rate`, `weight_decay`, `warmup_ratio`, and `gradient_accumulation_steps` correspond directly to the values we used in the manual PyTorch loop.\n",
    "- `evaluation_strategy=\"epoch\"` and `save_strategy=\"epoch\"` tell `Trainer` to run evaluation and save checkpoints at the end of each epoch.\n",
    "- `fp16=True` enables mixed precision training on GPU, similar to using `torch.cuda.amp`.\n",
    "\n",
    "`Trainer` will:\n",
    "\n",
    "- construct DataLoaders internally,\n",
    "- run the training loop (forward, loss, backward, optimizer step, scheduler step),\n",
    "- handle evaluation and model saving.\n",
    "\n",
    "Conceptually, under the hood it performs the same sequence of operations as our manual loop in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5baf0f16-ab8b-4cc5-80d6-563eb2948602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63dab063-89d2-4f72-8ec2-fb112bbe0458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=32, training_loss=2.4102229923009872, metrics={'train_runtime': 30.9312, 'train_samples_per_second': 16.165, 'train_steps_per_second': 1.035, 'total_flos': 794505510912000.0, 'train_loss': 2.4102229923009872, 'epoch': 1.0})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01e7583c-e995-4fba-9afd-18c741a01dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model saved to ft_model_trainer\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(cfg.output_dir)\n",
    "tokenizer.save_pretrained(cfg.output_dir)\n",
    "print(f\"Model saved to {cfg.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82f76522-dd89-48e3-abe2-aba3d5df1ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.7101176977157593,\n",
       " 'eval_runtime': 1.1446,\n",
       " 'eval_samples_per_second': 87.366,\n",
       " 'eval_steps_per_second': 87.366,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8fca91-813f-4300-b35c-c95212068af8",
   "metadata": {},
   "source": [
    "After calling `trainer.train()`:\n",
    "\n",
    "- `Trainer` has iterated over the training dataset for `num_train_epochs` epochs.\n",
    "- For each step, it has:\n",
    "  - computed the loss,\n",
    "  - backpropagated the gradients,\n",
    "  - updated the optimizer and learning rate scheduler.\n",
    "- After each epoch, it has:\n",
    "  - evaluated on the validation dataset,\n",
    "  - saved a checkpoint,\n",
    "  - optionally kept the best-performing model in memory.\n",
    "\n",
    "The `metrics` dictionary from `trainer.evaluate()` contains at least the validation loss (`eval_loss`), which is directly comparable to the validation loss from the manual PyTorch notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62955cf2-4316-45ed-9d74-5548b663dc2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base model (unmodified) from HuggingFace Hub\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.HF_LLM_MODEL,\n",
    "    dtype=torch.bfloat16,\n",
    ").to(cfg.device)\n",
    "base_model.eval()\n",
    "\n",
    "# Fine-tuned model (from Trainer output)\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.output_dir,\n",
    "    dtype=torch.bfloat16,\n",
    ").to(cfg.device)\n",
    "ft_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3d62f87e-ab06-4be7-b963-6c15becf78e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_response(model, instruction: str, max_new_tokens: int = 128):\n",
    "    \"\"\"\n",
    "    Generate a reply from the model given a human instruction.\n",
    "    We create a Guanaco-style prompt:\n",
    "        \"### Human: ...### Assistant:\"\n",
    "    and let the model continue.\n",
    "    \"\"\"\n",
    "    prompt_text = build_prompt_for_inference(instruction)\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(cfg.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a97190e1-ab0a-4c12-9136-59ebe0bed046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "### HUMAN (PROMPT) ###\n",
       "How do I build a PC?\n",
       "\n",
       "### GROUND TRUTH ASSISTANT ###\n",
       "Building a PC can be a rewarding experience, but it requires some technical knowledge and patience. Here are the general steps you can follow:\n",
       "\n",
       "1. Determine your budget and what you want to use the PC for (gaming, work, etc.). This will help you choose the appropriate components.\n",
       "\n",
       "2. Choose the components you want to use (CPU, motherboard, RAM, storage, GPU, power supply, case, and peripherals).\n",
       "\n",
       "3. Make sure all the components are compatible with each other. Check the manufacturer's website and online forums for compatibility information.\n",
       "\n",
       "4. Gather the necessary tools (screwdriver, thermal paste, etc.).\n",
       "\n",
       "5. Install the CPU onto the motherboard, following the manufacturer's instructions.\n",
       "\n",
       "6. Install the RAM onto the motherboard, following the manufacturer's instructions.\n",
       "\n",
       "7. Install the storage (hard drive or solid-state drive) onto the motherboard or into the case, following the manufacturer's instructions.\n",
       "\n",
       "8. Install the power supply into the case, following the manufacturer's instructions.\n",
       "\n",
       "9. Install the GPU onto the motherboard, following the manufacturer's instructions.\n",
       "\n",
       "10. Connect all the necessary cables to the motherboard, power supply, and peripherals.\n",
       "\n",
       "11. Turn on the PC and make sure it boots up properly.\n",
       "\n",
       "12. Install the operating system and necessary drivers.\n",
       "\n",
       "13. Test the PC to make sure all the components are working properly.\n",
       "\n",
       "These are the basic steps, but there may be some variation depending on the specific components you choose. It's important to follow the manufacturer's instructions and take your time to avoid damaging any components. There are also many online resources, such as YouTube tutorials and PC building forums, that can provide additional guidance and tips.### Human: Thank you.  Is it better to build my own PC or to just buy one off the shelf?  Give me the pros and cons of each approach.\n",
       "\n",
       "### BASE MODEL ###\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "### Human: How do I build a PC?### Assistant: Sure, let's break it down. Building a PC involves putting together different parts and components to create an overall system that meets your computing needs. Here are the steps involved:\n",
       "\n",
       "1. Choose your computer hardware - You need to choose the motherboard, processor, RAM, graphics card, storage devices, power supply unit (PSU), and other essential components such as fans, heat sinks, and cables.\n",
       "\n",
       "2. Install the motherboard - The motherboard is the brains of your PC, it controls everything from power supply, CPU, RAM, to video output. It connects all the other\n",
       "\n",
       "### FINE-TUNED MODEL ###\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "### Human: How do I build a PC?### Assistant: To build a personal computer, you need to follow these steps:\n",
       "\n",
       "1. Determine your budget and needs. Consider the type of operating system you want (Windows or Linux), storage capacity, processing power, graphics card, monitor, and other peripherals like keyboards, mice, speakers, and printers. 2. Choose a case and motherboard. A case is where your components go, while a motherboard is what connects them together. Check for compatibility with your chosen processor, RAM, and storage device. 3. Select a processor. There are many types of processors available depending\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick an example from the validation set\n",
    "example = val_dataset[11]\n",
    "example_text = example[\"text\"]\n",
    "\n",
    "# Crude split to get the human instruction and assistant answer, for display\n",
    "if \"### Human:\" in example_text and \"### Assistant:\" in example_text:\n",
    "    human_part = example_text.split(\"### Human:\")[1].split(\"### Assistant:\")[0].strip()\n",
    "    assistant_part = example_text.split(\"### Assistant:\")[1].strip()\n",
    "else:\n",
    "    human_part = example_text\n",
    "    assistant_part = \"\"\n",
    "\n",
    "print(\"### HUMAN (PROMPT) ###\")\n",
    "print(human_part)\n",
    "\n",
    "print(\"\\n### GROUND TRUTH ASSISTANT ###\")\n",
    "print(assistant_part)\n",
    "\n",
    "print(\"\\n### BASE MODEL ###\")\n",
    "print(generate_response(base_model, human_part))\n",
    "\n",
    "print(\"\\n### FINE-TUNED MODEL ###\")\n",
    "print(generate_response(ft_model, human_part))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb8313-8996-4d3e-9bda-baa22883c6af",
   "metadata": {},
   "source": [
    "## Summary and comparison to manual PyTorch fine-tuning\n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "1. Loaded the same pretrained Llama model and Guanaco dataset as in the manual PyTorch notebook.\n",
    "2. Used `datasets.load_dataset` to load the JSONL files directly into a `DatasetDict`.\n",
    "3. Applied tokenization with `dataset.map`, creating `input_ids`, `attention_mask`, and `labels` fields.\n",
    "4. Configured training behaviour via `TrainingArguments`.\n",
    "5. Used `Trainer` to handle:\n",
    "   - batching and shuffling,\n",
    "   - gradient accumulation,\n",
    "   - mixed precision (on GPU),\n",
    "   - learning rate scheduling,\n",
    "   - evaluation and checkpointing.\n",
    "6. Saved the fine-tuned model and compared its outputs to the base model.\n",
    "\n",
    "Conceptually, `Trainer` performs the same operations as the manual PyTorch training loop from the previous notebook:\n",
    "forward pass, loss computation, backward pass, optimizer step, and scheduler step. The main difference is that these steps are now handled by a higher-level API, letting us focus on model, data, and hyperparameters rather than on training boilerplate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962fbaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down the kernel to release memory\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
