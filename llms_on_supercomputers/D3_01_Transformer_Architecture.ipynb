{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a2dcdc-5f78-4553-a255-080bc7574cd3",
   "metadata": {},
   "source": [
    "# Transformer Anatomy\n",
    "\n",
    "The Transformer architecture, introduced in the paper \"Attention is All You Need\" by Vaswani et al., 2017, has revolutionized the field of NLP. Unlike previous models that relied on recurrent or convolutional layers, Transformers use self-attention mechanisms to capture dependencies between words in a sentence, regardless of their distance.\n",
    "\n",
    "### Key Components of the Transformer Architecture:\n",
    "\n",
    "1. **Positional Encoding**: Adds information about the position of words in a sequence since the model itself does not inherently understand word order.\n",
    "2. **Self-Attention Mechanism**: Allows the model to weigh the importance of each word in a sentence relative to all other words.\n",
    "3. **Multi-Head Attention**: Enables the model to focus on different parts of the input simultaneously.\n",
    "4. **Layer Normalization and Residual Connections**: Stabilizes training and helps with gradient flow.\n",
    "5. **Feed-Forward Neural Networks**: Applies a point-wise feed-forward layer to each position independently and destilles the information further to output probabilities.\n",
    "\n",
    "Let's explore each of these components in detail and understand how they work together to create powerful language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176534dc",
   "metadata": {},
   "source": [
    "> **Bazzite-AI Setup Required**  \n",
    "> Run `D0_00_Bazzite_AI_Setup.ipynb` first to verify GPU access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2a6418-583d-4787-b410-18e97b74afdc",
   "metadata": {},
   "source": [
    "## Self-Attention Mechanism\n",
    "\n",
    "The self-attention mechanism is the core component of the Transformer architecture. It allows the model to dynamically assign different levels of importance to different words in a sentence when encoding a particular word.\n",
    "\n",
    "### How Self-Attention Works:\n",
    "\n",
    "1. **Input Embeddings**: Before we can apply self-attention, the input words must first be converted into embeddings (dense vector representations).\n",
    "2. **Query, Key, and Value Vectors**: For each word, the model creates three vectors: a Query vector (Q), a Key vector (K), and a Value vector (V).\n",
    "3. **Attention Scores**: The attention score is computed as the dot product of the Query vector of a word with the Key vectors of all words. This score determines how much focus should be on the other words.\n",
    "4. **Weighted Sum**: Each word's output representation is computed as a weighted sum of the Value vectors, where the weights are the normalized attention scores.\n",
    "5. **Softmax Normalization**: After the attention heads, the scores are passed through a softmax function to convert them into probabilities.\n",
    "\n",
    "### Visualizing Self-Attention\n",
    "\n",
    "Let's visualize how the self-attention mechanism works for a simple sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec12499-686a-44f4-946c-32e12b30be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55685b95-6dd3-49df-95c0-b50b504a49f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentence and tokens\n",
    "sentence = \"Transformers are revolutionary in NLP.\"\n",
    "tokens = [\"Transformers\", \"are\", \"revolutionary\", \"in\", \"NLP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c785b3ac-0477-49ab-ac96-d52f179ad192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding dimension\n",
    "embedding_dim = 8\n",
    "\n",
    "# Random input embeddings (for illustration purposes)\n",
    "torch.manual_seed(42)\n",
    "input_embeddings = torch.randn(len(tokens), embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d759c400-2430-4d6f-96ea-8f1df5b58653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431, -1.6047],\n",
       "        [-0.7521,  1.6487, -0.3925, -1.4036, -0.7279, -0.5594, -0.7688,  0.7624],\n",
       "        [ 1.6423, -0.1596, -0.4974,  0.4396, -0.7581,  1.0783,  0.8008,  1.6806],\n",
       "        [ 0.0349,  0.3211,  1.5736, -0.8455,  1.3123,  0.6872, -1.0892, -0.3553],\n",
       "        [-1.4181,  0.8963,  0.0499,  2.2667,  1.1790, -0.4345, -1.3864, -1.2862]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this example we end up with a 5x8 matrix\n",
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f987c5e-2b8f-4a1e-a508-aa3c8994c548",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Initialize Query, Key, and Value weight matrices\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m Q = torch.randn(\u001b[43membedding_dim\u001b[49m, embedding_dim)\n\u001b[32m      3\u001b[39m K = torch.randn(embedding_dim, embedding_dim)\n\u001b[32m      4\u001b[39m V = torch.randn(embedding_dim, embedding_dim)\n",
      "\u001b[31mNameError\u001b[39m: name 'embedding_dim' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize Query, Key, and Value weight matrices\n",
    "Q = torch.randn(embedding_dim, embedding_dim)\n",
    "K = torch.randn(embedding_dim, embedding_dim)\n",
    "V = torch.randn(embedding_dim, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84aedcd8-a1ce-4da2-bba2-ec05ada89ba3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Compute Query, Key, and Value vectors\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m queries = \u001b[43minput_embeddings\u001b[49m @ Q\n\u001b[32m      3\u001b[39m keys = input_embeddings @ K\n\u001b[32m      4\u001b[39m values = input_embeddings @ V\n",
      "\u001b[31mNameError\u001b[39m: name 'input_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute Query, Key, and Value vectors\n",
    "queries = input_embeddings @ Q\n",
    "keys = input_embeddings @ K\n",
    "values = input_embeddings @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "788cd987-4b13-4075-a850-a84ee8fc130f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'queries' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Calculate attention scores using dot product of queries and keys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m attention_scores = \u001b[43mqueries\u001b[49m @ keys.T\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Apply softmax to normalize the scores\u001b[39;00m\n\u001b[32m      5\u001b[39m attention_weights = F.softmax(attention_scores/math.sqrt(embedding_dim), dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'queries' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate attention scores using dot product of queries and keys\n",
    "attention_scores = queries @ keys.T\n",
    "\n",
    "# Apply softmax to normalize the scores\n",
    "attention_weights = F.softmax(attention_scores/math.sqrt(embedding_dim), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c207b2d-273a-4d62-9f3f-969a79c80677",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attention_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Compute the weighted sum of values\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m output = \u001b[43mattention_weights\u001b[49m @ values\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Display the attention weights\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAttention Weights:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, attention_weights)\n",
      "\u001b[31mNameError\u001b[39m: name 'attention_weights' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute the weighted sum of values\n",
    "output = attention_weights @ values\n",
    "\n",
    "# Display the attention weights\n",
    "print(\"Attention Weights:\\n\", attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8399995-3f3a-44fb-a3f9-322338475a4d",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "The multi-head attention mechanism allows the model to focus on different parts of the input simultaneously. Instead of having a single attention mechanism, the model uses multiple attention \"heads\" in parallel. Each head can learn different aspects of the input.\n",
    "\n",
    "### How Multi-Head Attention Works:\n",
    "\n",
    "1. The input is projected into multiple sets of Query, Key, and Value vectors.\n",
    "2. Each set of vectors is processed independently through a self-attention mechanism.\n",
    "3. The outputs from each head are concatenated and projected back into a single vector space.\n",
    "\n",
    "This approach provides the model with a richer understanding of the input by capturing different types of relationships between words.\n",
    "\n",
    "### Example: Multi-Head Attention\n",
    "Let's visualize how multi-head attention works using multiple attention heads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ab9e0c3-6ca4-4909-88ca-85910c43cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of attention heads\n",
    "num_heads = 2\n",
    "\n",
    "# Initialize weight matrices for each head\n",
    "Q_heads = [torch.randn(embedding_dim, embedding_dim) for _ in range(num_heads)]\n",
    "K_heads = [torch.randn(embedding_dim, embedding_dim) for _ in range(num_heads)]\n",
    "V_heads = [torch.randn(embedding_dim, embedding_dim) for _ in range(num_heads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa5d3b8f-713b-4dfb-a276-eeed1d3b6ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute outputs for each head\n",
    "head_outputs = []\n",
    "for i in range(num_heads):\n",
    "    queries = input_embeddings @ Q_heads[i]\n",
    "    keys = input_embeddings @ K_heads[i]\n",
    "    values = input_embeddings @ V_heads[i]\n",
    "    \n",
    "    # Calculate attention scores and apply softmax\n",
    "    attention_scores = queries @ keys.T\n",
    "    attention_weights = F.softmax(attention_scores/math.sqrt(embedding_dim), dim=-1)\n",
    "    \n",
    "    # Compute the weighted sum of values\n",
    "    output = attention_weights @ values\n",
    "    head_outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c897be67-1f30-4df4-884b-59730fa006cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Head Attention Output:\n tensor([[-3.2237e-01,  4.6591e+00, -9.6314e+00, -2.9663e+00,  3.3225e+00,\n          2.9357e+00, -2.3207e+00, -2.0601e+00,  2.7245e+00, -1.1831e+00,\n          5.6527e+00,  2.6269e-01,  4.7597e+00, -4.0497e-01, -1.5315e+00,\n         -3.2463e+00],\n        [ 1.0108e+00, -4.0638e+00,  2.3292e+00, -2.8314e-01,  3.4551e+00,\n          5.5581e+00,  5.0915e+00,  1.3194e+00, -6.4749e+00,  5.8671e+00,\n          3.8078e+00, -2.3065e-01, -2.7182e+00, -1.3306e+00, -2.0265e+00,\n          3.9932e-01],\n        [-3.4254e-01,  4.6145e+00, -9.4725e+00, -2.9815e+00,  3.2473e+00,\n          2.8808e+00, -2.3042e+00, -1.9888e+00,  1.6466e+00, -1.7977e+00,\n         -1.3634e+00,  1.1788e-01, -2.7043e+00,  1.2903e+00,  2.6171e+00,\n          4.7654e+00],\n        [-8.3613e-03,  2.6027e+00, -6.7415e+00, -2.3407e+00,  3.3221e+00,\n          3.5544e+00, -6.2675e-01, -1.1993e+00,  2.7992e+00, -4.4668e+00,\n         -4.1611e+00,  4.2510e-01, -5.3059e+00,  2.4657e+00,  4.6805e+00,\n          6.6150e+00],\n        [ 3.3992e-01,  2.0579e+00, -1.5704e+00, -2.4118e+00,  1.6728e+00,\n          4.4862e+00, -4.1120e+00,  3.4354e+00, -5.8075e+00,  4.1165e+00,\n          1.6684e-01, -1.3309e-01, -6.1007e+00, -2.1936e-01,  3.0305e-01,\n          4.0173e+00]])"
     ]
    }
   ],
   "source": [
    "# Concatenate outputs from all heads\n",
    "multi_head_output = torch.cat(head_outputs, dim=-1)\n",
    "\n",
    "# Display multi-head attention output\n",
    "print(\"Multi-Head Attention Output:\\n\", multi_head_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e11aba7-5d36-4e39-a90f-ea27d42f2882",
   "metadata": {},
   "source": [
    "## Feed-Forward Neural Networks\n",
    "\n",
    "Each position's output from the multi-head attention mechanism is passed through a point-wise feed-forward neural network. This consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "### Example: Feed-Forward Network\n",
    "Let's implement a simple feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "110b82cc-6d53-4522-a459-62a20656f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feed-forward neural network\n",
    "class FeedForwardNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.relu(self.linear1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c3d922e-283c-4567-8ded-db2e52265560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed-Forward Network Output:\n tensor([[-0.1076, -1.1561, -0.2368,  0.8209,  0.9425,  0.8352,  0.1717,  0.3996,\n         -0.5560,  0.5732,  1.0187, -1.3614,  0.8661, -0.9439, -0.5560, -1.0347],\n        [-0.8371,  0.5195,  1.0020,  0.8108, -0.2805,  1.2257, -1.6926,  2.2672,\n         -0.3883,  1.3553, -0.2897, -1.1592,  0.0278, -0.1767, -0.2364, -0.0273],\n        [-0.8777, -0.5523, -0.6139, -0.7321,  1.3447,  0.6218,  1.4753,  1.1127,\n          0.3491, -0.4577,  0.2886, -0.6492,  1.5801, -1.6984,  0.0520, -0.7382],\n        [-1.5672, -0.7098, -0.1675, -1.0434,  0.8221,  0.5423,  1.9750,  1.2772,\n          0.7321, -0.5789, -0.1112,  0.3048,  1.4322, -1.9399,  0.0625, -0.1988],\n        [-1.0258,  0.3024,  0.9928, -0.9082,  0.7688,  1.2261, -0.4238,  1.2061,\n          0.2265,  0.5295,  0.1424, -1.2343, -0.9645,  0.4392,  0.1343, -0.5169]],\n       grad_fn=<AddmmBackward0>)"
     ]
    }
   ],
   "source": [
    "# Instantiate and apply the feed-forward network\n",
    "ffn = FeedForwardNN(input_dim=embedding_dim * num_heads, hidden_dim=32)\n",
    "ffn_output = ffn(multi_head_output)\n",
    "\n",
    "# Display the feed-forward network output\n",
    "print(\"Feed-Forward Network Output:\\n\", ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca96b7b-d1b4-4b65-a95a-9d4e220f8c88",
   "metadata": {},
   "source": [
    "## Layer Normalization and Residual Connections\n",
    "\n",
    "Layer normalization is used to stabilize training by normalizing the inputs to each layer. Residual connections help maintain gradient flow through the network, enabling deeper architectures.\n",
    "\n",
    "### Example: Adding Layer Normalization and Residual Connections\n",
    "Let's see how these components are added to the Transformer block.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ed5179f-ebc2-4a06-aa91-1fbe79547f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output with Residual Connections:\n tensor([[-0.1115,  0.8620, -2.4476, -0.5361,  1.0506,  0.9283, -0.5369, -0.4160,\n          0.5317, -0.1560,  1.6463, -0.2770,  1.3875, -0.3389, -0.5217, -1.0647],\n        [-0.1859, -1.1866,  0.6639, -0.0906,  0.6218,  1.5932,  0.6822,  0.7327,\n         -2.0799,  1.7112,  0.7142, -0.6068, -0.9568, -0.6383, -0.8417, -0.1325],\n        [-0.3199,  1.1613, -2.8059, -1.0190,  1.3098,  1.0044, -0.2102, -0.2234,\n          0.5818, -0.6101, -0.2791, -0.1267, -0.2930, -0.0922,  0.7707,  1.1515],\n        [-0.4497,  0.4527, -1.8375, -0.9203,  1.0386,  1.0262,  0.3111, -0.0195,\n          0.8791, -1.3527, -1.1514,  0.1501, -1.0477,  0.0970,  1.1944,  1.6297],\n        [-0.2064,  0.6243, -0.1769, -0.9248,  0.6465,  1.5385, -1.2563,  1.2464,\n         -1.5414,  1.2477,  0.0650, -0.3923, -1.9461,  0.0406,  0.0999,  0.9353]],\n       grad_fn=<NativeLayerNormBackward0>)"
     ]
    }
   ],
   "source": [
    "# Define Layer Normalization\n",
    "layer_norm = torch.nn.LayerNorm(embedding_dim * num_heads)\n",
    "\n",
    "# Add residual connection and apply layer normalization\n",
    "residual_output = layer_norm(multi_head_output + ffn_output)\n",
    "\n",
    "# Display the final output with residual connection\n",
    "print(\"Output with Residual Connections:\\n\", residual_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4602fe3-732c-4882-9715-f9c41cf382c5",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Since the Transformer does not inherently capture the order of words, positional encoding is added to provide the model with information about the relative position of words in a sentence.\n",
    "\n",
    "### Example: Implementing Positional Encoding\n",
    "Let's implement positional encoding for a sequence of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a93fb9d4-2f26-4ddb-a5d4-476d79eb46a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def positional_encoding(seq_len, model_dim):\n",
    "    pos_enc = np.zeros((seq_len, model_dim))\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, model_dim, 2):\n",
    "            pos_enc[pos, i] = np.sin(pos / (10000 ** (2 * i / model_dim)))\n",
    "            pos_enc[pos, i + 1] = np.cos(pos / (10000 ** (2 * i / model_dim)))\n",
    "    return torch.tensor(pos_enc, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f8cd123-65df-4cf3-a60b-a7a5b19b2862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random input embeddings (for illustration purposes)\n",
    "torch.manual_seed(42)\n",
    "input_embeddings = torch.randn(len(tokens), embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13c51fae-f4fd-4f9b-b6b1-d71124f1629f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encodings:\n tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n          1.0000e+00,  0.0000e+00,  1.0000e+00],\n        [ 8.4147e-01,  5.4030e-01,  9.9998e-03,  9.9995e-01,  1.0000e-04,\n          1.0000e+00,  1.0000e-06,  1.0000e+00],\n        [ 9.0930e-01, -4.1615e-01,  1.9999e-02,  9.9980e-01,  2.0000e-04,\n          1.0000e+00,  2.0000e-06,  1.0000e+00],\n        [ 1.4112e-01, -9.8999e-01,  2.9996e-02,  9.9955e-01,  3.0000e-04,\n          1.0000e+00,  3.0000e-06,  1.0000e+00],\n        [-7.5680e-01, -6.5364e-01,  3.9989e-02,  9.9920e-01,  4.0000e-04,\n          1.0000e+00,  4.0000e-06,  1.0000e+00]])\nEncoded Input with Positional Information:\n tensor([[ 1.9269,  2.4873,  0.9007, -1.1055,  0.6784, -0.2345, -0.0431, -0.6047],\n        [ 0.0893,  2.1890, -0.3825, -0.4037, -0.7278,  0.4406, -0.7688,  1.7624],\n        [ 2.5516, -0.5757, -0.4774,  1.4394, -0.7579,  2.0783,  0.8008,  2.6806],\n        [ 0.1760, -0.6689,  1.6036,  0.1541,  1.3126,  1.6872, -1.0892,  0.6447],\n        [-2.1749,  0.2426,  0.0899,  3.2659,  1.1794,  0.5655, -1.3864, -0.2862]])"
     ]
    }
   ],
   "source": [
    "# Apply positional encoding\n",
    "position_encodings = positional_encoding(len(tokens), embedding_dim)\n",
    "\n",
    "# Add positional encoding to input embeddings\n",
    "encoded_input = input_embeddings + position_encodings\n",
    "\n",
    "print(\"Positional Encodings:\\n\", position_encodings)\n",
    "print(\"Encoded Input with Positional Information:\\n\", encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0def0ae9-1661-4364-9a86-019ae9394d63",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored the key components of the Transformer architecture, including self-attention, multi-head attention, feed-forward networks, layer normalization, and positional encoding. These components work together to form the basis of modern NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "328d6745-120e-4a35-af72-056526046a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': False}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shut down the kernel to release memory\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
