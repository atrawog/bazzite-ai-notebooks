{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Bazzite-AI Environment Setup\n",
    "\n",
    "**One-time setup for the LLMs on Supercomputers Course**\n",
    "\n",
    "This notebook configures your bazzite-ai environment for running the course notebooks. Run this once at the start of each JupyterLab session.\n",
    "\n",
    "---\n",
    "\n",
    "## Attribution\n",
    "\n",
    "The course notebooks are adapted from the [Foundations of LLM Mastery](https://events.asc.ac.at/category/4/) training series by:\n",
    "\n",
    "- **Simeon Harrison** (INiTS and AI Factory Austria AI:AT)\n",
    "- **Thomas Haschka** (Campus IT / HPC, TU Wien)\n",
    "- **Martin Pfister** (Advanced Computing Austria ACA GmbH)\n",
    "\n",
    "Original source: [gitlab.tuwien.ac.at/vsc-public/training/LLMs-on-supercomputers](https://gitlab.tuwien.ac.at/vsc-public/training/LLMs-on-supercomputers)\n",
    "\n",
    "**Adapted for Bazzite.AI** by Andreas Trawöger\n",
    "\n",
    "**License:** [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f69a3-e45a-415d-9250-e09ae6367170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "## Bazzite-AI vs Supercomputer Environment\n",
    "\n",
    "The original course was designed for the Vienna Scientific Cluster (VSC) supercomputer. In bazzite-ai, we run everything locally with:\n",
    "\n",
    "| Aspect | VSC Supercomputer | Bazzite-AI |\n",
    "|--------|------------------|------------|\n",
    "| **GPU Access** | SLURM job scheduler | Direct GPU access via container |\n",
    "| **LLM Inference** | vLLM server | Ollama pod (containerized) |\n",
    "| **Model Loading** | Shared NFS storage | HuggingFace Hub / Ollama pull |\n",
    "| **API Compatibility** | OpenAI-compatible vLLM | OpenAI-compatible Ollama |\n",
    "\n",
    "### Key Difference: Ollama as OpenAI Drop-in\n",
    "\n",
    "Instead of OpenAI's paid API or a vLLM server, we use **Ollama** which provides an OpenAI-compatible endpoint locally:\n",
    "\n",
    "```python\n",
    "# OpenAI (paid cloud API)\n",
    "client = OpenAI(api_key=\"sk-...\")\n",
    "\n",
    "# Ollama (free local inference - same code works!)\n",
    "client = OpenAI(base_url=\"http://ollama:11434/v1\", api_key=\"ollama\")\n",
    "```\n",
    "\n",
    "This means most code samples work unchanged - just point to Ollama instead of OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gpu-check-header",
   "metadata": {},
   "source": [
    "## 1. GPU Access & Environment Testing\n",
    "\n",
    "First, let's verify GPU access and check available memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "gpu-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "GPU & Environment Status\n",
      "==================================================\n",
      "\n",
      "PyTorch version: 2.9.1+cu130\n",
      "CUDA available: True\n",
      "CUDA version: 13.0\n",
      "GPU count: 1\n",
      "Current device: 0\n",
      "GPU name: NVIDIA GeForce RTX 4080 SUPER\n",
      "\n",
      "--- System-Wide GPU Memory ---\n",
      "\n",
      "GPU 0: b'NVIDIA GeForce RTX 4080 SUPER'\n",
      "  Total:  15.99 GB\n",
      "  Used:   2.07 GB (13.0%)\n",
      "  Free:   13.92 GB\n",
      "\n",
      "--- GPU Computation Test ---\n",
      "✅ GPU computation test passed!\n",
      "\n",
      "=================================================="
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"GPU & Environment Status\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check PyTorch CUDA availability\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# System-wide GPU memory check using pynvml\n",
    "try:\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()\n",
    "    device_count = pynvml.nvmlDeviceGetCount()\n",
    "    \n",
    "    print(f\"\\n--- System-Wide GPU Memory ---\")\n",
    "    \n",
    "    for i in range(device_count):\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "        name = pynvml.nvmlDeviceGetName(handle)\n",
    "        info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        \n",
    "        total_gb = info.total / 1024**3\n",
    "        used_gb = info.used / 1024**3\n",
    "        free_gb = info.free / 1024**3\n",
    "        usage_pct = (info.used / info.total) * 100\n",
    "        \n",
    "        print(f\"\\nGPU {i}: {name}\")\n",
    "        print(f\"  Total:  {total_gb:.2f} GB\")\n",
    "        print(f\"  Used:   {used_gb:.2f} GB ({usage_pct:.1f}%)\")\n",
    "        print(f\"  Free:   {free_gb:.2f} GB\")\n",
    "        \n",
    "        # Warning thresholds\n",
    "        if free_gb < 4.0:\n",
    "            print(f\"  \\u26a0\\ufe0f  CRITICAL: Very low GPU memory!\")\n",
    "            print(f\"      Shutdown other notebook kernels before proceeding.\")\n",
    "        elif free_gb < 6.0:\n",
    "            print(f\"  \\u26a0\\ufe0f  WARNING: Low GPU memory.\")\n",
    "            print(f\"      7B models need ~5GB with 4-bit quantization.\")\n",
    "    \n",
    "    pynvml.nvmlShutdown()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\n\\u26a0\\ufe0f  pynvml not installed - using PyTorch memory info (per-process only)\")\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            print(f\"  GPU {i}: {allocated:.2f} / {total:.2f} GB (this process only)\")\n",
    "\n",
    "# Quick GPU test\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n--- GPU Computation Test ---\")\n",
    "    try:\n",
    "        x = torch.randn(1000, 1000, device=\"cuda\")\n",
    "        y = torch.matmul(x, x)\n",
    "        del x, y\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"\\u2705 GPU computation test passed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\u274c GPU test failed: {e}\")\n",
    "else:\n",
    "    print(\"\\n\\u26a0\\ufe0f  No GPU available - running on CPU only\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ollama-header",
   "metadata": {},
   "source": [
    "## 2. Ollama Pod Management\n",
    "\n",
    "Ollama runs as a containerized pod in bazzite-ai. Let's check if it's running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ollama-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama host: http://ollama:11434\n",
      "\n",
      "--- Checking Ollama Connection ---\n",
      "✅ Ollama server is running!\n",
      "\n",
      "Available models (2):\n",
      "  - hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M (4.1 GB)\n",
      "  - llama3.2:latest (1.9 GB)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Ollama configuration\n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\n",
    "\n",
    "print(f\"Ollama host: {OLLAMA_HOST}\")\n",
    "print(\"\\n--- Checking Ollama Connection ---\")\n",
    "\n",
    "def check_ollama_health():\n",
    "    \"\"\"Check if Ollama server is running and healthy.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            return True, response.json()\n",
    "        return False, f\"Unexpected status: {response.status_code}\"\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return False, \"Connection refused - Ollama pod not running\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        return False, \"Connection timed out\"\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "is_running, result = check_ollama_health()\n",
    "\n",
    "if is_running:\n",
    "    print(\"\\u2705 Ollama server is running!\")\n",
    "    models = result.get(\"models\", [])\n",
    "    if models:\n",
    "        print(f\"\\nAvailable models ({len(models)}):\")\n",
    "        for m in models:\n",
    "            name = m.get(\"name\", \"Unknown\")\n",
    "            size_gb = m.get(\"size\", 0) / 1024**3\n",
    "            print(f\"  - {name} ({size_gb:.1f} GB)\")\n",
    "    else:\n",
    "        print(\"\\nNo models pulled yet (we'll pull them in the next step).\")\n",
    "else:\n",
    "    print(f\"\\u274c Ollama is not running: {result}\")\n",
    "    print(\"\\n--- How to Start Ollama ---\")\n",
    "    print(\"Run this command in a terminal:\")\n",
    "    print(\"\")\n",
    "    print(\"    ujust ollama start\")\n",
    "    print(\"\")\n",
    "    print(\"Then re-run this cell to verify the connection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models-header",
   "metadata": {},
   "source": [
    "## 3. Model Management (Auto-Pull)\n",
    "\n",
    "The course notebooks require specific models. Let's check if they're available and pull any missing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "models-pull",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking required models...\n",
      "\n",
      "✅ hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\n",
      "   Used by: D1 notebooks (Prompt Engineering)\n",
      "✅ llama3.2:latest\n",
      "   Used by: D2 notebooks (RAG)\n",
      "\n",
      "==================================================\n",
      "✅ All required models are available!"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Required models for the course\n",
    "REQUIRED_MODELS = [\n",
    "    {\n",
    "        \"name\": \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\",\n",
    "        \"used_by\": \"D1 notebooks (Prompt Engineering)\",\n",
    "        \"size_hint\": \"~4.4 GB\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"llama3.2:latest\",\n",
    "        \"used_by\": \"D2 notebooks (RAG)\",\n",
    "        \"size_hint\": \"~2.0 GB\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def get_available_models():\n",
    "    \"\"\"Get list of models available in Ollama.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            return [m.get(\"name\", \"\") for m in response.json().get(\"models\", [])]\n",
    "    except:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "def pull_model(model_name):\n",
    "    \"\"\"Pull a model from Ollama, showing progress.\"\"\"\n",
    "    print(f\"\\nPulling '{model_name}'...\")\n",
    "    print(\"(This may take several minutes for large models)\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_HOST}/api/pull\",\n",
    "            json={\"name\": model_name},\n",
    "            stream=True,\n",
    "            timeout=1800  # 30 minute timeout for large models\n",
    "        )\n",
    "        \n",
    "        last_status = \"\"\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                data = json.loads(line)\n",
    "                status = data.get(\"status\", \"\")\n",
    "                \n",
    "                # Show download progress\n",
    "                if \"pulling\" in status or \"downloading\" in status:\n",
    "                    completed = data.get(\"completed\", 0)\n",
    "                    total = data.get(\"total\", 0)\n",
    "                    if total > 0:\n",
    "                        pct = (completed / total) * 100\n",
    "                        print(f\"\\r  Progress: {pct:.1f}%\", end=\"\", flush=True)\n",
    "                elif status != last_status:\n",
    "                    if last_status:\n",
    "                        print()  # newline after progress\n",
    "                    print(f\"  {status}\")\n",
    "                    last_status = status\n",
    "                \n",
    "                if status == \"success\":\n",
    "                    print(f\"\\n\\u2705 Model '{model_name}' pulled successfully!\")\n",
    "                    return True\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\u274c Failed to pull model: {e}\")\n",
    "        return False\n",
    "\n",
    "# Check connection first\n",
    "is_running, _ = check_ollama_health()\n",
    "if not is_running:\n",
    "    print(\"\\u274c Ollama is not running. Start it first with: ujust ollama start\")\n",
    "else:\n",
    "    print(\"Checking required models...\\n\")\n",
    "    available = get_available_models()\n",
    "    \n",
    "    all_ready = True\n",
    "    for model_info in REQUIRED_MODELS:\n",
    "        model_name = model_info[\"name\"]\n",
    "        \n",
    "        # Check if model is available (exact match or prefix match)\n",
    "        is_available = any(model_name in m or m in model_name for m in available)\n",
    "        \n",
    "        if is_available:\n",
    "            print(f\"\\u2705 {model_name}\")\n",
    "            print(f\"   Used by: {model_info['used_by']}\")\n",
    "        else:\n",
    "            print(f\"\\u274c {model_name} - NOT FOUND\")\n",
    "            print(f\"   Used by: {model_info['used_by']}\")\n",
    "            print(f\"   Size: {model_info['size_hint']}\")\n",
    "            \n",
    "            # Auto-pull missing model\n",
    "            success = pull_model(model_name)\n",
    "            if not success:\n",
    "                all_ready = False\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    if all_ready:\n",
    "        print(\"\\u2705 All required models are available!\")\n",
    "    else:\n",
    "        print(\"\\u26a0\\ufe0f  Some models failed to download. Try manually:\")\n",
    "        print(\"    ujust ollama pull <model-name>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "api-header",
   "metadata": {},
   "source": [
    "## 4. Ollama as OpenAI Drop-in Replacement\n",
    "\n",
    "Ollama provides an OpenAI-compatible API, which means you can use the same code for both:\n",
    "\n",
    "| Aspect | OpenAI API | Ollama (bazzite-ai) |\n",
    "|--------|-----------|---------------------|\n",
    "| **Cost** | Pay per token | Free (runs locally) |\n",
    "| **API Key** | Required | Not needed |\n",
    "| **Privacy** | Data sent to cloud | Data stays local |\n",
    "| **Models** | OpenAI models only | Any GGUF model |\n",
    "| **base_url** | `https://api.openai.com/v1` | `http://ollama:11434/v1` |\n",
    "\n",
    "### Configuration Pattern\n",
    "\n",
    "In the course notebooks, you'll see this minimal configuration:\n",
    "\n",
    "```python\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\n",
    "MODEL = \"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=f\"{OLLAMA_HOST}/v1\",\n",
    "    api_key=\"ollama\"  # Required by library but ignored by Ollama\n",
    ")\n",
    "```\n",
    "\n",
    "The same pattern works with LangChain:\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=f\"{OLLAMA_HOST}/v1\",\n",
    "    api_key=\"ollama\",\n",
    "    model=MODEL\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "api-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing OpenAI-compatible API...✅ API test passed!\n",
      "\n",
      "Response: \"Ollama, wishing hello.\""
     ]
    }
   ],
   "source": [
    "# Quick API test\n",
    "from openai import OpenAI\n",
    "\n",
    "# === Model Configuration ===\n",
    "HF_LLM_MODEL = \"NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF\"\n",
    "OLLAMA_LLM_MODEL = f\"hf.co/{HF_LLM_MODEL}:Q4_K_M\"\n",
    "\n",
    "print(\"Testing OpenAI-compatible API...\")\n",
    "print(f\"Model: {OLLAMA_LLM_MODEL}\")\n",
    "\n",
    "try:\n",
    "    client = OpenAI(\n",
    "        base_url=f\"{OLLAMA_HOST}/v1\",\n",
    "        api_key=\"ollama\"\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=OLLAMA_LLM_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say 'Hello from Ollama!' in exactly 5 words.\"}],\n",
    "        max_tokens=20\n",
    "    )\n",
    "    \n",
    "    print(f\"\\u2705 API test passed!\")\n",
    "    print(f\"\\nResponse: {response.choices[0].message.content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\u274c API test failed: {e}\")\n",
    "    print(\"\\nMake sure Ollama is running and the model is pulled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "path-header",
   "metadata": {},
   "source": [
    "## 5. Datasets Directory\n",
    "\n",
    "Notebooks can use relative paths to access datasets. The Jupyter kernel runs in the notebook's directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "path-detection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets directory: /workspace/Sync/AI/bazzite/bazzite-ai-testing/notebooks/llms_on_supercomputers/datasets\nAvailable datasets: ['booking_queries_dataset.csv', 'code_review_dataset.csv', 'health_and_fitness_qna.csv']"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# With kernel cwd fix, notebooks run in their own directory\n",
    "# Datasets are in ./datasets/ relative to this notebook\n",
    "DATASETS_DIR = Path(\"./datasets\")\n",
    "\n",
    "print(f\"Datasets directory: {DATASETS_DIR.resolve()}\")\n",
    "datasets = list(DATASETS_DIR.glob('*.csv'))\n",
    "if datasets:\n",
    "    print(f\"Available datasets: {[d.name for d in datasets]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-header",
   "metadata": {},
   "source": [
    "## 6. Environment Verification & Readiness Check\n",
    "\n",
    "Final verification that everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "final-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENVIRONMENT READINESS CHECK\n",
      "============================================================\n",
      "\n",
      "✅ GPU Access: CUDA available\n",
      "✅ Ollama Server: Running\n",
      "✅ Ollama Models: 2 models available\n",
      "✅ API Inference: Working\n",
      "\n",
      "============================================================\n",
      "✅ ENVIRONMENT READY!\n",
      "\n",
      "You can now proceed to D1_01_Prompting_with_LangChain.ipynb\n",
      "============================================================"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ENVIRONMENT READINESS CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "checks = []\n",
    "\n",
    "# 1. GPU Check\n",
    "gpu_ok = torch.cuda.is_available()\n",
    "checks.append((\"GPU Access\", gpu_ok, \"CUDA available\" if gpu_ok else \"No GPU - CPU only\"))\n",
    "\n",
    "# 2. Ollama Check\n",
    "ollama_ok, _ = check_ollama_health()\n",
    "checks.append((\"Ollama Server\", ollama_ok, \"Running\" if ollama_ok else \"Not running\"))\n",
    "\n",
    "# 3. Models Check\n",
    "if ollama_ok:\n",
    "    available = get_available_models()\n",
    "    model_count = len(available)\n",
    "    models_ok = model_count > 0\n",
    "    checks.append((\"Ollama Models\", models_ok, f\"{model_count} models available\" if models_ok else \"No models\"))\n",
    "else:\n",
    "    checks.append((\"Ollama Models\", False, \"Ollama not running\"))\n",
    "\n",
    "# 4. API Test\n",
    "if ollama_ok and models_ok:\n",
    "    try:\n",
    "        client = OpenAI(base_url=f\"{OLLAMA_HOST}/v1\", api_key=\"ollama\")\n",
    "        # Quick test with small output\n",
    "        response = client.chat.completions.create(\n",
    "            model=available[0],\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hi\"}],\n",
    "            max_tokens=5\n",
    "        )\n",
    "        api_ok = True\n",
    "    except:\n",
    "        api_ok = False\n",
    "    checks.append((\"API Inference\", api_ok, \"Working\" if api_ok else \"Failed\"))\n",
    "else:\n",
    "    checks.append((\"API Inference\", False, \"Prerequisites not met\"))\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\")\n",
    "all_ok = True\n",
    "for name, ok, detail in checks:\n",
    "    status = \"\\u2705\" if ok else \"\\u274c\"\n",
    "    print(f\"{status} {name}: {detail}\")\n",
    "    if not ok and name not in [\"GPU Access\"]:  # GPU is optional\n",
    "        all_ok = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if all_ok:\n",
    "    print(\"\\u2705 ENVIRONMENT READY!\")\n",
    "    print(\"\\nYou can now proceed to D1_01_Prompting_with_LangChain.ipynb\")\n",
    "else:\n",
    "    print(\"\\u26a0\\ufe0f  SOME ISSUES DETECTED\")\n",
    "    print(\"\\nPlease resolve the issues above before continuing.\")\n",
    "    if not ollama_ok:\n",
    "        print(\"\\nTo start Ollama, run in a terminal:\")\n",
    "        print(\"    ujust ollama start\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Your bazzite-ai environment is configured! You can now proceed with the course:\n",
    "\n",
    "### D1 - Prompt Engineering Essentials\n",
    "- `D1_01_Prompting_with_LangChain.ipynb` - Start here!\n",
    "- `D1_02_Prompt_templates_and_parsing.ipynb`\n",
    "- `D1_05_Chaining.ipynb`\n",
    "- `D1_08_LLM_Evaluation.ipynb`\n",
    "- `D1_09_LLM_as_a_Judge.ipynb`\n",
    "- `D1_10_Prompt_Optimization.ipynb`\n",
    "\n",
    "### D2 - Retrieval Augmented Generation\n",
    "- `D2_01_rag_with_basic_tools.ipynb`\n",
    "- `D2_02_rag_with_langchain_and_chromadb.ipynb`\n",
    "\n",
    "### D3 - Fine-tuning on One GPU\n",
    "- `D3_01_Transformer_Architecture.ipynb`\n",
    "- `D3_02_Finetuning_LLM_with_PyTorch.ipynb`\n",
    "- `D3_03_Finetuning_LLM_with_Huggingface.ipynb`\n",
    "- `D3_04_Quantization.ipynb`\n",
    "- `D3_05_PEFT.ipynb`\n",
    "- `D3_06_Unsloth.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** You only need to run this setup notebook once per JupyterLab session. The Ollama pod persists between notebook runs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
