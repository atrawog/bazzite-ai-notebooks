{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a7ac63",
   "metadata": {},
   "source": [
    "# Prompt Optimization with Evidently\n",
    "Attribution & License\n",
    "\n",
    "This notebook is adapted from: [evidentlyai/community-examples](https://github.com/evidentlyai/community-examples.git), licensed under the Apache License, Version 2.0. © Original authors.\n",
    "\n",
    "Modifications: by Simeon Harrison/EuroCC Austria, © 2025.\n",
    "\n",
    "This notebook demonstrates how to use Evidently's `PromptOptimizer` API for optimizing prompts for LLM judges. \n",
    "\n",
    "## Code Review Quality Classifier\n",
    "We'll walk through optimizing a prompt that classifies the quality of code reviews written for junior developers.\n",
    "\n",
    "### What you'll learn:\n",
    "- How to set up a dataset for LLM evaluation\n",
    "- How to define an LLM judge with a prompt template\n",
    "- How to run the prompt optimization loop\n",
    "- How to retrieve and inspect the best performing prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da74fcc",
   "metadata": {},
   "source": [
    "> **Bazzite-AI Setup Required**  \n",
    "> Run `D0_00_Bazzite_AI_Setup.ipynb` first to configure Ollama and verify GPU access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "133c6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you haven't installed the required packages yet:\n",
    "# !pip install evidently openai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cc9af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from evidently import Dataset, DataDefinition, LLMClassification\n",
    "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
    "from evidently.descriptors import LLMEval\n",
    "from evidently.llm.optimization import PromptOptimizer\n",
    "from evidently.descriptors import HuggingFace, HuggingFaceToxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f66ab6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: /workspace/AI/bazzite/bazzite-ai-testing/notebooks/llms_on_supercomputers\n",
      "Datasets directory: /workspace/AI/bazzite/bazzite-ai-testing/notebooks/datasets\n",
      "Available datasets: ['code_review_dataset.csv', 'booking_queries_dataset.csv', 'health_and_fitness_qna.csv']"
     ]
    }
   ],
   "source": [
    "# === Path Detection ===\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def find_datasets_dir():\n",
    "    \"\"\"Find the datasets directory using multiple detection methods.\"\"\"\n",
    "    \n",
    "    base_paths = [\n",
    "        Path('/workspace'),\n",
    "        Path('/home/jovyan'),\n",
    "        Path.home(),\n",
    "        Path('/opt'),\n",
    "    ]\n",
    "    \n",
    "    project_patterns = [\n",
    "        'AI/bazzite/bazzite-ai-testing/notebooks',\n",
    "        'bazzite-ai-testing/notebooks',\n",
    "        'notebooks',\n",
    "    ]\n",
    "    \n",
    "    for base in base_paths:\n",
    "        if not base.exists():\n",
    "            continue\n",
    "        for pattern in project_patterns:\n",
    "            datasets_path = base / pattern / 'datasets'\n",
    "            if datasets_path.exists() and any(datasets_path.glob('*.csv')):\n",
    "                notebook_dir = datasets_path.parent / 'llms_on_supercomputers'\n",
    "                return datasets_path, notebook_dir if notebook_dir.exists() else datasets_path.parent\n",
    "    \n",
    "    cwd = Path.cwd()\n",
    "    for parent in [cwd] + list(cwd.parents)[:10]:\n",
    "        datasets_path = parent / 'datasets'\n",
    "        if datasets_path.exists() and any(datasets_path.glob('*.csv')):\n",
    "            return datasets_path, parent\n",
    "        datasets_path = parent / 'notebooks' / 'datasets'\n",
    "        if datasets_path.exists() and any(datasets_path.glob('*.csv')):\n",
    "            return datasets_path, parent / 'notebooks' / 'llms_on_supercomputers'\n",
    "    \n",
    "    raise FileNotFoundError(\"Could not find datasets directory. Please set DATASETS_DIR manually.\")\n",
    "\n",
    "DATASETS_DIR, NOTEBOOK_DIR = find_datasets_dir()\n",
    "\n",
    "print(f\"Notebook directory: {NOTEBOOK_DIR}\")\n",
    "print(f\"Datasets directory: {DATASETS_DIR}\")\n",
    "print(f\"Available datasets: {[f.name for f in DATASETS_DIR.glob('*.csv')]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dac91b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama host: http://ollama:11434\n",
      "Model: hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\n",
      "Using OpenAI-compatible API at: http://ollama:11434/v1"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict, Any, List, Optional\n",
    "from evidently.llm.utils.wrapper import OpenAIOptions, OpenAIWrapper, LLMMessage, LLMResult\n",
    "\n",
    "# === Ollama Configuration via OpenAI-Compatible API ===\n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\n",
    "\n",
    "# === Model Configuration ===\n",
    "HF_LLM_MODEL = \"NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF\"\n",
    "OLLAMA_LLM_MODEL = f\"hf.co/{HF_LLM_MODEL}:Q4_K_M\"\n",
    "\n",
    "OLLAMA_OPTIONS = OpenAIOptions(\n",
    "    api_key=\"ollama\",\n",
    "    api_url=f\"{OLLAMA_HOST}/v1\"\n",
    ")\n",
    "\n",
    "# === Patch OpenAIWrapper for smart JSON mode detection ===\n",
    "# Evidently's OpenAI wrapper doesn't enable JSON mode by default.\n",
    "# This patch detects when JSON output is expected and enables it.\n",
    "_original_openai_complete = OpenAIWrapper.complete\n",
    "\n",
    "async def _json_aware_complete(self, messages: List[LLMMessage], seed: Optional[int] = None) -> LLMResult[str]:\n",
    "    import openai\n",
    "    from openai.types.chat.chat_completion import ChatCompletion\n",
    "    \n",
    "    message_text = \" \".join(m.content for m in messages if m.content)\n",
    "    needs_json = \"json\" in message_text.lower() or '\"category\"' in message_text\n",
    "    needs_xml = \"<new_prompt>\" in message_text\n",
    "    \n",
    "    formatted_messages = [{\"role\": msg.role, \"content\": msg.content} for msg in messages]\n",
    "    \n",
    "    try:\n",
    "        kwargs = {\"model\": self.model, \"messages\": formatted_messages, \"seed\": seed}\n",
    "        if needs_json and not needs_xml:\n",
    "            kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "        \n",
    "        response: ChatCompletion = await self.client.chat.completions.create(**kwargs)\n",
    "    except openai.RateLimitError as e:\n",
    "        from evidently.llm.utils.wrapper import LLMRateLimitError\n",
    "        raise LLMRateLimitError(e.message) from e\n",
    "    except openai.APIError as e:\n",
    "        from evidently.llm.utils.wrapper import LLMRequestError\n",
    "        raise LLMRequestError(f\"Failed to call OpenAI complete API: {e.message}\", original_error=e) from e\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "    assert content is not None\n",
    "    if response.usage is None:\n",
    "        return LLMResult(content, 0, 0)\n",
    "    return LLMResult(content, response.usage.prompt_tokens, response.usage.completion_tokens)\n",
    "\n",
    "OpenAIWrapper.complete = _json_aware_complete\n",
    "\n",
    "print(f\"Ollama host: {OLLAMA_HOST}\")\n",
    "print(f\"Model: {OLLAMA_LLM_MODEL}\")\n",
    "print(f\"Using OpenAI-compatible API with smart JSON mode detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd5f6441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Generated review</th>\n",
       "      <th>Expert label</th>\n",
       "      <th>Expert comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This implementation appears to work, but the a...</td>\n",
       "      <td>bad</td>\n",
       "      <td>The tone is slighly condescending, no actionab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great job! Keep it up!</td>\n",
       "      <td>bad</td>\n",
       "      <td>Not actionable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It would be advisable to think about modularit...</td>\n",
       "      <td>bad</td>\n",
       "      <td>there is a suggestion, but no real guidance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You’ve structured the class very well, and the...</td>\n",
       "      <td>good</td>\n",
       "      <td>Good tone, actionable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great job! This is clean and well-organized. T...</td>\n",
       "      <td>bad</td>\n",
       "      <td>Pure praise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Generated review Expert label  \\\n",
       "0  This implementation appears to work, but the a...          bad   \n",
       "1                             Great job! Keep it up!          bad   \n",
       "2  It would be advisable to think about modularit...          bad   \n",
       "3  You’ve structured the class very well, and the...         good   \n",
       "4  Great job! This is clean and well-organized. T...          bad   \n",
       "\n",
       "                                      Expert comment  \n",
       "0  The tone is slighly condescending, no actionab...  \n",
       "1                                     Not actionable  \n",
       "2        there is a suggestion, but no real guidance  \n",
       "3                              Good tone, actionable  \n",
       "4                                        Pure praise  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your dataset\n",
    "review_dataset = pd.read_csv(DATASETS_DIR / \"code_review_dataset.csv\")\n",
    "review_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e464810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how Evidently should interpret your dataset\n",
    "dd = DataDefinition(\n",
    "    text_columns=[\"Generated review\", \"Expert comment\"],\n",
    "    categorical_columns=[\"Expert label\"],\n",
    "    llm=LLMClassification(input=\"Generated review\", target=\"Expert label\", reasoning=\"Expert comment\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3957c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your pandas DataFrame into an Evidently Dataset\n",
    "dataset = Dataset.from_pandas(review_dataset, data_definition=dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af027bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template and judge for classifying code review quality\n",
    "criteria = '''A review is GOOD when it's actionable and constructive.\n",
    "A review is BAD when it is non-actionable or overly critical.'''\n",
    "\n",
    "feedback_quality = BinaryClassificationPromptTemplate(\n",
    "    pre_messages=[(\"system\", \"You are evaluating the quality of code reviews given to junior developers.\")],\n",
    "    criteria=criteria,\n",
    "    target_category=\"bad\",\n",
    "    non_target_category=\"good\",\n",
    "    uncertainty=\"unknown\",\n",
    "    include_reasoning=True,\n",
    ")\n",
    "\n",
    "judge = LLMEval(\n",
    "    alias=\"Code Review Judge\",\n",
    "    provider=\"openai\",  # Use OpenAI provider with Ollama's OpenAI-compatible API\n",
    "    model=OLLAMA_LLM_MODEL,\n",
    "    column_name=\"Generated review\",\n",
    "    template=feedback_quality\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6995309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizer with Ollama via OpenAI-compatible API\n",
    "optimizer = PromptOptimizer(\n",
    "    \"code_review_example\",\n",
    "    strategy=\"feedback\",\n",
    "    provider=\"openai\",  # Use OpenAI provider with Ollama's OpenAI-compatible API\n",
    "    model=OLLAMA_LLM_MODEL\n",
    ")\n",
    "optimizer.set_input_dataset(dataset)\n",
    "await optimizer.arun(judge, \"accuracy\", options=OLLAMA_OPTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7f3162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A code review is GOOD when it's actionable and provides specific suggestions for improvement, addressing both positive aspects and potential issues while maintaining a constructive tone. A code review is BAD when it lacks actionable feedback or offers vague advice, only providing praise or criticism without suggestions for resolution."
     ]
    }
   ],
   "source": [
    "# Show the best-performing prompt template found by the optimizer\n",
    "print(optimizer.best_prompt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2893069-e592-40b3-a1a4-3eac138aca09",
   "metadata": {},
   "source": [
    "## Example 2: Bookings Query Classifier\n",
    "In this tutorial, we'll optimize a prompt for classifying different types of customer service queries (like Booking, Payment, or Technical issues) using an LLM classifier.\n",
    "\n",
    "### What you'll learn:\n",
    "- How to load a dataset for LLM classification\n",
    "- How to define a multiclass classification prompt\n",
    "- How to run prompt optimization with Evidently\n",
    "- How to retrieve the best performing prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5649d2a-9cd0-41b3-82af-ad038c7b584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from evidently import Dataset, DataDefinition, LLMClassification\n",
    "from evidently.descriptors import LLMEval\n",
    "from evidently.llm.templates import MulticlassClassificationPromptTemplate\n",
    "from evidently.llm.optimization import PromptOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c123275-866b-48fb-9de7-c19b1a935159",
   "metadata": {},
   "source": [
    "### Load Your Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1908e0b-1670-42aa-b57f-0ea3d52a54b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>booked a trip for 4 ppl, want to add a 5th now</td>\n",
       "      <td>Booking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hello team, please confirm if my hotel reserva...</td>\n",
       "      <td>Booking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i can’t see the payment options, dropdown just...</td>\n",
       "      <td>Technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I heard airlines sometimes overbook, what’s yo...</td>\n",
       "      <td>Policy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wanna reschedule my train ride to next week</td>\n",
       "      <td>Booking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query      label\n",
       "0     booked a trip for 4 ppl, want to add a 5th now    Booking\n",
       "1  hello team, please confirm if my hotel reserva...    Booking\n",
       "2  i can’t see the payment options, dropdown just...  Technical\n",
       "3  I heard airlines sometimes overbook, what’s yo...     Policy\n",
       "4        wanna reschedule my train ride to next week    Booking"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(DATASETS_DIR / \"booking_queries_dataset.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde191f7-2f78-4883-8666-9c63315f975a",
   "metadata": {},
   "source": [
    "### Define Data Structure for Evidently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47e80d62-fc0b-4582-a64c-12c6eacfd7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = DataDefinition(\n",
    "    text_columns=[\"query\"],\n",
    "    categorical_columns=[\"label\"],\n",
    "    llm=LLMClassification(input=\"query\", target=\"label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ccb4891-d8c9-4c13-9503-c1a259e1d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(data, data_definition=dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83de632-6a5c-4e7b-8f6a-92dfba189469",
   "metadata": {},
   "source": [
    "### Define a Multiclass Prompt and LLM Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f584a4dc-040d-40df-841d-5d5adca1a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = \"Classify inqueries by categories\"\n",
    "\n",
    "t = MulticlassClassificationPromptTemplate(\n",
    "    pre_messages=[(\"system\", \"You are classifying user queries.\")],\n",
    "    criteria=base_prompt,\n",
    "    category_criteria={\n",
    "        \"Booking\": \"bookings\",\n",
    "        \"Technical\": \"technical questions\",\n",
    "        \"Policy\": \"questions about policies\",\n",
    "        \"Payment\": \"payment questions\",\n",
    "        \"Escalation\": \"escalation requests\"\n",
    "    },\n",
    "    uncertainty=\"unknown\",\n",
    "    include_reasoning=True,\n",
    ")\n",
    "\n",
    "judge = LLMEval(\n",
    "    alias=\"bookings\",\n",
    "    provider=\"openai\",  # Use OpenAI provider with Ollama's OpenAI-compatible API\n",
    "    model=OLLAMA_LLM_MODEL,\n",
    "    column_name=\"query\",\n",
    "    template=t\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042c96f1-72e3-4b8a-9b46-8c33d5129300",
   "metadata": {},
   "source": [
    "### Run the Prompt Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "186551b6-9825-4849-8cb2-095862c3669b",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "optimizer = PromptOptimizer(\n",
    "    \"bookings_example\", \n",
    "    strategy=\"feedback\",\n",
    "    provider=\"openai\",  # Use OpenAI provider with Ollama's OpenAI-compatible API\n",
    "    model=OLLAMA_LLM_MODEL\n",
    ")\n",
    "optimizer.set_input_dataset(dataset)\n",
    "await optimizer.arun(judge, \"accuracy\", options=OLLAMA_OPTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf15d94-31fe-4a8d-91ae-7f25425856aa",
   "metadata": {},
   "source": [
    "### View the Best Optimized Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f16a85f0-4f13-4348-944c-8432667b7d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify inqueries by categories"
     ]
    }
   ],
   "source": [
    "print(optimizer.best_prompt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972f3946-9547-4716-9b84-536ef99bac47",
   "metadata": {},
   "source": [
    "## Example 3: Tweet Generation Example\n",
    "This tutorial shows how to optimize prompts for generating engaging tweets using Evidently's `PromptOptimizer` API. \n",
    "We'll iteratively improve a tweet generation prompt to maximize how engaging LLM-generated tweets are, according to a classifier.\n",
    "\n",
    "### What you'll learn:\n",
    "- How to define a tweet generation function with OpenAI\n",
    "- How to set up an LLM judge to classify tweet engagement\n",
    "- How to optimize a tweet generation prompt based on feedback\n",
    "- How to inspect the best optimized prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfe8ead3-2020-4538-a28e-69a8ebb49bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages if needed\n",
    "# !pip install evidently openai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6dc460f-73bc-408d-9207-c75d25e76773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "\n",
    "from evidently.descriptors import LLMEval\n",
    "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
    "from evidently.llm.optimization import PromptOptimizer, PromptExecutionLog, Params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99bac22-e206-49ff-956e-85bfdb9b6dd3",
   "metadata": {},
   "source": [
    "### Define a Tweet Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1324a5cb-fcfc-4936-8189-a9a69eba26b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def basic_tweet_generation(topic, model=OLLAMA_LLM_MODEL, instructions=\"\"):\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instructions},\n",
    "            {\"role\": \"user\", \"content\": f\"Write a short paragraph about {topic}\"}\n",
    "        ]\n",
    "    )\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af8d7d-1101-4193-ae01-6ac373cf37a7",
   "metadata": {},
   "source": [
    "### Define a Tweet Quality Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76adad10-7b03-4a28-9f95-2fdc112f9673",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_quality = BinaryClassificationPromptTemplate(\n",
    "    pre_messages=[(\"system\", \"You are evaluating the quality of tweets\")],\n",
    "    criteria=\"\"\"\n",
    "Text is ENGAGING if it meets at least one of the following:\n",
    "  - Strong hook (question, surprise, bold statement)\n",
    "  - Uses emotion, humor, or opinion\n",
    "  - Encourages interaction\n",
    "  - Shows personality or distinct tone\n",
    "  - Includes vivid language or emojis\n",
    "  - Sparks curiosity or insight\n",
    "\n",
    "Text is NEUTRAL if it lacks these qualities.\n",
    "\"\"\",\n",
    "    target_category=\"ENGAGING\",\n",
    "    non_target_category=\"NEUTRAL\",\n",
    "    uncertainty=\"non_target\",\n",
    "    include_reasoning=True,\n",
    ")\n",
    "\n",
    "judge = LLMEval(\"basic_tweet_generation.result\", template=tweet_quality,\n",
    "                provider=\"openai\", model=OLLAMA_LLM_MODEL, alias=\"Tweet quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c3cb26-7ed8-4e49-a627-bab987e32e6a",
   "metadata": {},
   "source": [
    "### Define a Prompt Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01a6c783-f823-4a33-91d0-940a98cc050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(generation_prompt: str, context) -> PromptExecutionLog:\n",
    "    \"\"\"generate engaging tweets\"\"\"\n",
    "    my_topics = [\n",
    "        \"testing in AI engineering is as important as in development\",\n",
    "        \"CI/CD is applicable in AI\",\n",
    "        \"Collaboration of subject matter experts and AI engineers improves product\",\n",
    "        \"Start LLM apps development from test cases generation\",\n",
    "        \"evidently is a great tool for LLM testing\"\n",
    "    ]\n",
    "    tweets = [basic_tweet_generation(topic, model=OLLAMA_LLM_MODEL, instructions=generation_prompt) for topic in my_topics * 3]\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501fa391-8c25-4dbf-a83f-2176dabc35eb",
   "metadata": {},
   "source": [
    "### Run the Prompt Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e075281-5175-48ba-b304-b4597e58c481",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = PromptOptimizer(\n",
    "    \"tweet_gen_example\", \n",
    "    strategy=\"feedback\",\n",
    "    provider=\"openai\",  # Use OpenAI provider with Ollama's OpenAI-compatible API\n",
    "    model=OLLAMA_LLM_MODEL\n",
    ")\n",
    "optimizer.set_param(Params.BasePrompt, \"You are tweet generator\")\n",
    "await optimizer.arun(run_prompt, scorer=judge, options=OLLAMA_OPTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7e6abd-1cdc-4425-89a4-451e4108e2d5",
   "metadata": {},
   "source": [
    "### View the Best Optimized Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da9928fc-8c61-4f09-8aba-b10e978f90c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a master tweet generator specializing in engaging and fun content. Create a series of tweets that promote the importance and benefits of collaboration between subject matter experts and AI engineers, using examples from various industries and real-life scenarios. Use humor, emojis, and strong opinions to make them more appealing and captivating."
     ]
    }
   ],
   "source": [
    "print(optimizer.best_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd657be-3f3f-46a2-b77e-9e386ed4355c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9646f257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Unload Ollama Model & Shutdown Kernel ===\n",
    "# Unloads the model from GPU memory before shutting down\n",
    "\n",
    "try:\n",
    "    import ollama\n",
    "    print(f\"Unloading Ollama model: {OLLAMA_LLM_MODEL}\")\n",
    "    ollama.generate(model=OLLAMA_LLM_MODEL, prompt=\"\", keep_alive=0)\n",
    "    print(\"Model unloaded from GPU memory\")\n",
    "except Exception as e:\n",
    "    print(f\"Model unload skipped: {e}\")\n",
    "\n",
    "# Shut down the kernel to fully release resources\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
