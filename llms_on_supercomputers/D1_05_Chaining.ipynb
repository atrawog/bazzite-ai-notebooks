{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cbfc6e8-dc82-434d-ba0e-a68e52bf3cd9",
   "metadata": {},
   "source": [
    "## LangChain Chaining Techniques\n",
    "\n",
    "### Introduction\n",
    "This notebook demonstrates key chaining functionalities in LangChain:\n",
    "- SimpleSequentialChain\n",
    "- SequentialChain\n",
    "- LLMRouterChain\n",
    "- TransformChain\n",
    "\n",
    "Each chaining method is designed for different levels of complexity and control. Use simple chains for straightforward tasks, sequential chains for workflows, router chains for conditional branching, and transform chains when integrating custom logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d40d4b00-deba-4bc0-a3eb-280c8179d02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain_huggingface import ChatHuggingFace\n",
    "from langchain_classic.chains import SimpleSequentialChain, SequentialChain, TransformChain, LLMChain, LLMMathChain\n",
    "from langchain_classic.chains.router import LLMRouterChain, MultiPromptChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7cf2a7f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Initial Resource Status ===\n",
       "\n",
       "GPU Count: 1\n",
       "\n",
       "GPU 0: b'NVIDIA GeForce RTX 4080 SUPER'\n",
       "  Total:  15.99 GB\n",
       "  Used:   2.49 GB (15.5%)\n",
       "  Free:   13.51 GB\n",
       "\n",
       "========================================\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === GPU & Model Status Check ===\n",
    "import gc\n",
    "\n",
    "print(\"=== Initial Resource Status ===\")\n",
    "\n",
    "# GPU Status - Use pynvml for SYSTEM-WIDE memory (not just this process)\n",
    "try:\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()\n",
    "    device_count = pynvml.nvmlDeviceGetCount()\n",
    "    print(f\"\\nGPU Count: {device_count}\")\n",
    "\n",
    "    for i in range(device_count):\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "        name = pynvml.nvmlDeviceGetName(handle)\n",
    "        info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "\n",
    "        total_gb = info.total / 1024**3\n",
    "        used_gb = info.used / 1024**3\n",
    "        free_gb = info.free / 1024**3\n",
    "        usage_pct = (info.used / info.total) * 100\n",
    "\n",
    "        print(f\"\\nGPU {i}: {name}\")\n",
    "        print(f\"  Total:  {total_gb:.2f} GB\")\n",
    "        print(f\"  Used:   {used_gb:.2f} GB ({usage_pct:.1f}%)\")\n",
    "        print(f\"  Free:   {free_gb:.2f} GB\")\n",
    "\n",
    "        # Warning if low on memory (7B model needs ~5GB with 4-bit quantization)\n",
    "        if free_gb < 6.0:\n",
    "            print(f\"  ⚠️  WARNING: Low GPU memory! Model loading may fail.\")\n",
    "            print(f\"      Consider running cleanup cells in other notebooks first.\")\n",
    "\n",
    "    pynvml.nvmlShutdown()\n",
    "except ImportError:\n",
    "    print(\"\\n⚠️  pynvml not installed - falling back to PyTorch (per-process only)\")\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            print(f\"  GPU {i}: {allocated:.2f} / {total:.2f} GB (THIS PROCESS ONLY)\")\n",
    "    else:\n",
    "        print(\"No GPU available - using CPU\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nGPU status check failed: {e}\")\n",
    "\n",
    "# No Ollama in this notebook - HuggingFace only\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca5ea3ee-40e4-47a1-8708-b72b15cb89da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No output generated]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download model from HuggingFace (same base model as D1_01)\n",
    "HF_LLM_MODEL = \"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2fbc3ccb-bebc-4e2c-8ad4-2626bcaa167b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "MistralConfig {\n",
       "  \"architectures\": [\n",
       "    \"MistralForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"dtype\": \"float16\",\n",
       "  \"eos_token_id\": 32000,\n",
       "  \"head_dim\": null,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 14336,\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"model_type\": \"mistral\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"quantization_config\": {\n",
       "    \"_load_in_4bit\": true,\n",
       "    \"_load_in_8bit\": false,\n",
       "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
       "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
       "    \"bnb_4bit_quant_type\": \"nf4\",\n",
       "    \"bnb_4bit_use_double_quant\": false,\n",
       "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "    \"llm_int8_has_fp16_weight\": false,\n",
       "    \"llm_int8_skip_modules\": null,\n",
       "    \"llm_int8_threshold\": 6.0,\n",
       "    \"load_in_4bit\": true,\n",
       "    \"load_in_8bit\": false,\n",
       "    \"quant_method\": \"bitsandbytes\"\n",
       "  },\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"sliding_window\": 4096,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.57.2\",\n",
       "  \"use_cache\": false,\n",
       "  \"vocab_size\": 32002\n",
       "}\n",
       "\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4-bit quantization config for efficient loading\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_LLM_MODEL)\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_LLM_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "# Verify model config\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91d434e4-911e-44fb-b21d-10bc00bef193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0"
     ]
    }
   ],
   "source": [
    "# Pipeline setup\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    return_full_text=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f670afae-b70b-4782-9824-954540dd3ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e6bdc7-dddc-4115-8736-81c4182c1bcf",
   "metadata": {},
   "source": [
    "### SimpleSequentialChain\n",
    "\n",
    "The `SimpleSequentialChain` is the most basic form of a chain. It takes a single input, passes it to a prompt, and the output of one step is directly passed as input to the next. It does not track intermediate steps or provide access to named outputs, making it suitable for linear, single-purpose chains.\n",
    "\n",
    "Use case: quick linear pipelines like \"generate → explain\" or \"summarize → expand\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d948c96f-2f90-4b4e-aeb4-9aff615b985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = \"Give me a simple bullet point outline for a blog post on {topic}\"\n",
    "prompt1 = ChatPromptTemplate.from_template(template1)\n",
    "chain1 = prompt1|chat_llm\n",
    "\n",
    "template2 = \"Write a blog post using this outline: {outline}\"\n",
    "prompt2 = ChatPromptTemplate.from_template(template2)\n",
    "chain2 = prompt2|chat_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c0151ca-c79d-41fa-8e6e-776bff075def",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain = chain1|chain2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a27103ae-0602-416c-95af-476231cf160d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Artificial Intelligence: A Game-Changer for the Future\n",
      "\n",
      "Introduction to Artificial Intelligence\n",
      "\n",
      "Artificial Intelligence (AI) is a rapidly growing field that has the potential to revolutionize various industries and our daily lives. It is a branch of computer science that deals with the development of intelligent machines that work and react like humans. AI enables systems to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and language translation.\n",
      "\n",
      "Historical Development and Evolution of AI\n",
      "\n",
      "The concept of AI dates back to the 1950s, when the first AI research began. Over the years, AI has evolved through several phases, including rule-based systems, expert systems, and machine learning. In recent times, AI has been significantly improved with the advent of deep learning and neural networks, which allow machines to learn and improve from experience without being explicitly programmed.\n",
      "\n",
      "Types of AI\n",
      "\n",
      "1. Narrow or Weak AI: This is the most common type of AI in use today. It is designed to perform specific tasks, such as playing chess or recognizing speech, but it doesn't have general intelligence.\n",
      "\n",
      "2. General or Strong AI: This"
     ]
    }
   ],
   "source": [
    "result = full_chain.invoke(\"Artificial Intelligence\") # That piece of code takes quite some time to execute\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f634a90-c37d-44eb-8b1b-a875115093fc",
   "metadata": {},
   "source": [
    "### SequentialChain\n",
    "\n",
    "`SequentialChain` is more flexible than `SimpleSequentialChain`. It supports multiple input and output variables and keeps track of intermediate outputs. Each step can depend on one or more outputs from earlier steps.\n",
    "\n",
    "Use case: more complex workflows that need to reuse or transform earlier outputs in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43f51587-1fcd-4d60-8d4e-e1b4c61c9ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = \"Give a summary of this employee's performance review:\\n{review}\"\n",
    "prompt1 = ChatPromptTemplate.from_template(template1)\n",
    "chain_1 = prompt1|chat_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "450ec48b-4e55-4385-924f-44552aa37046",
   "metadata": {},
   "outputs": [],
   "source": [
    "template2 = \"Identify key employee weaknesses in this review summary:\\n{review_summary}\"\n",
    "prompt2 = ChatPromptTemplate.from_template(template2)\n",
    "chain_2 = prompt2|chat_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4da913af-ba5a-4b89-966c-49e6d7358fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "template3 = \"Create a personalized plan to help address and fix these weaknesses:\\n{weaknesses}\"\n",
    "prompt3 = ChatPromptTemplate.from_template(template3)\n",
    "chain_3 = prompt3|chat_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99c23725-9cf9-4927-9f8d-df65bdee4d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The following PromptTemplate examples are for reference only.\n",
    "# The actual chains (chain_1, chain_2, chain_3) above use ChatPromptTemplate.from_template().\n",
    "# prompt1 = PromptTemplate(input_variables=[\"topic\"], template=\"Generate a question about {topic}.\")\n",
    "# prompt2 = PromptTemplate(input_variables=[\"question\"], template=\"Provide a short answer to: {question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd366755-5fe6-4757-a4ae-81910167d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_chain = chain_1|chain_2|chain_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2804d37c-fad2-4325-9175-d94b56e3fc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_review = '''\n",
    "Employee Information:\n",
    "Name: Simeon Harrison\n",
    "Position: Machine Learning Engineer\n",
    "Date of Review: 10 March, 2025\n",
    "\n",
    "Strengths:\n",
    "Simeon is a highly skilled machine learning engineer with a deep understanding of programming languages, algorithms, and data science. His technical expertise shines through in his ability to efficiently solve complex problems and deliver high-quality code.\n",
    "\n",
    "One of Simeon's greatest strengths is his collaborative nature. He actively engages with cross-functional teams, contributing valuable insights and seeking input from others. His open-mindedness and willingness to learn from colleagues make him a true team player.\n",
    "\n",
    "Simeon consistently demonstrates initiative and self-motivation. He takes the lead in seeking out new projects and challenges, and his proactive attitude has led to significant improvements in existing processes and systems. His dedication to self-improvement and growth is commendable.\n",
    "\n",
    "Another notable strength is Simeon's teaching skills. He has shown great prowess in developing teaching materials and delivering high-end online courses. His adaptability allows him to seamlessly transition between different projects and tasks such as teaching, which makes him a valuable asset to the team.\n",
    "\n",
    "\n",
    "Weaknesses:\n",
    "While Simeon possesses numerous strengths, there are a few areas where he could benefit from improvement. One such area is time management. Occasionally, Simeon struggles with effectively managing his time, resulting in missed deadlines or the need for additional support to complete tasks on time, especially before delivering courses for the first time. Developing better prioritization and time management techniques would greatly enhance his efficiency.\n",
    "\n",
    "Another area for improvement is Simeon's written communication skills. He does not answer customer requests promptly, as he finds it difficult to focus on several tasks simultaneously. There were also instances where his written documentation lacked clarity, leading to confusion among team members. Focusing on enhancing his written communication abilities will help him effectively convey ideas and instructions.\n",
    "\n",
    "Additionally, Simeon tends to take on too many responsibilities and hesitates to delegate tasks to others. This can result in an excessive workload and potential burnout. Encouraging him to delegate tasks appropriately will not only alleviate his own workload but also foster a more balanced and productive team environment.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8afc5155-a8e9-457e-afdc-d9e89b8fdc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = seq_chain.invoke(employee_review) # This too takes time to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1ba1328-ad9d-4909-b2dc-c9a537e05f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To address and fix these weaknesses, a personalized plan for Simeon could include the following steps:\n",
      "\n",
      "1. Time management and prioritization:\n",
      "a. Schedule regular check-ins with a supervisor or team leader to review progress and adjust priorities as needed.\n",
      "b. Implement time-blocking techniques to prioritize tasks and maximize productivity.\n",
      "c. Use tools like project management software to help manage tasks and deadlines.\n",
      "\n",
      "2. Written communication skills:\n",
      "a. Provide training resources and guidance on improving written communication skills.\n",
      "b. Encourage Simeon to focus on one task at a time, minimizing distractions and improving focus.\n",
      "c. Provide a checklist or template for writing responses to customer requests to ensure clarity and consistency.\n",
      "\n",
      "3. Delegation and workload management:\n",
      "a. Offer guidance and support for Simeon to identify tasks that can be delegated to other team members.\n",
      "b. Encourage open communication among team members to foster a culture of collaboration and support.\n",
      "c. Regularly review workload distribution to ensure it's balanced and fair across the team.\n",
      "\n",
      "By implementing this plan, Simeon can improve his"
     ]
    }
   ],
   "source": [
    "print(results.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59ff1bc7-5e9e-406b-b587-3a6993fb18d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simeon Harrison, a Machine Learning Engineer, performed exceptionally in his role, showcasing strong technical skills, collaborative nature, initiative, and adaptability. However, he could improve his time management, written communication, and delegation skills."
     ]
    }
   ],
   "source": [
    "print(chain_1.invoke(employee_review).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "951244ea-e077-4be0-9a84-a63a9a191b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key employee weaknesses identified in this review summary are:\n",
      "\n",
      "1. Time Management: Simeon Harrison needs to improve his time management skills in order to increase efficiency and productivity.\n",
      "2. Written Communication Skills: The review suggests that he needs to work on his written communication skills for better team collaboration and effective communication.\n",
      "3. Delegation: There is a need for Simeon Harrison to develop his skills in delegation, which will help in optimizing team performance and distributing workload more effectively."
     ]
    }
   ],
   "source": [
    "print((chain_1|chain_2).invoke(employee_review).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7c12adb-5e93-48d0-978c-37eebd4745d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To address and fix these weaknesses, a personalized plan can be created for Simeon as follows:\n",
      "\n",
      "1. Time management:\n",
      "\n",
      "   a. Break tasks into smaller, more manageable parts with clear deadlines.\n",
      "   b. Prioritize tasks based on urgency and importance.\n",
      "   c. Use time-tracking or project management tools to stay organized.\n",
      "   d. Communicate with team members about progress and seek help if needed.\n",
      "   e. Establish a routine/schedule to ensure balanced workload.\n",
      "   f. Take breaks to avoid burnout and maintain productivity.\n",
      "\n",
      "2. Written communication skills:\n",
      "\n",
      "   a. Set a specific time each day to respond to customer requests promptly.\n",
      "   b. Use templates and clear language to ensure written documentation is concise and easy to understand.\n",
      "   c. Seek feedback from team members on written communication and implement changes accordingly.\n",
      "   d. Encourage open communication to clarify any misunderstandings.\n",
      "\n",
      "3. Task delegation:\n",
      "\n",
      "   a. Identify team members' strengths and delegate tasks accordingly.\n",
      "   b. Delegate tasks to reduce workload and empower team members.\n",
      "   c. Prov"
     ]
    }
   ],
   "source": [
    "print((chain_1|chain_2|chain_3).invoke(employee_review).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c070e46d-fcae-4a7f-8901-b192f1fc3017",
   "metadata": {},
   "source": [
    "### LLMRouterChain\n",
    "\n",
    "`LLMRouterChain` is used when you want to route a prompt to different chains or prompts depending on the input. It allows conditional execution paths, where an LLM can decide which destination (e.g., math, history, writing) to route a given input to based on predefined criteria or patterns.\n",
    "\n",
    "Use case: topic routing, multi-skill assistants, task-specific logic dispatching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40e8a6c3-d539-4ebe-afa5-328f10b9553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "beginner_template = '''You are an elementary school teacher who is really\n",
    "focused on students in the age group of 6 to 10 and explain complex topics in easy to understand terms for the given age group. \n",
    "You assume no prior knowledge. Here is the question\\n{input}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b02fe171-626f-4afe-bbea-70b68a9f312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_template = '''You are a world expert physics professor who explains physics topics\n",
    "to advanced audience members. You can assume anyone you answer has a \n",
    "PhD level understanding of Physics. Here is the question\\n{input}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a5644a9-6c82-4c05-95aa-0c65af01c2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {'name':'advanced physics','description': 'Answers advanced physics questions',\n",
    "     'prompt_template':expert_template},\n",
    "    {'name':'beginner physics','description': 'Answers basic beginner physics questions',\n",
    "     'prompt_template':beginner_template},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "595541d7-f244-4dcd-8f73-8c16cf602eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/pixi/.pixi/envs/default/lib/python3.14/site-packages/pydantic/main.py:250: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)"
     ]
    }
   ],
   "source": [
    "chain = MultiPromptChain.from_prompts(chat_llm, prompt_infos, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43191812-b695-418b-93de-64c27f44dcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0mbeginner physics: {'input': 'Why does a basketball bounce?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "A basketball bounces because of a concept called elastic potential energy. When you throw or hit a basketball, it gains kinetic energy, which is the energy of motion. As the basketball hits the ground, all that kinetic energy is transformed into elastic potential energy. Elastic potential energy is the energy stored in an object that is compressed, stretched, or deformed.\n",
      "\n",
      "When the basketball hits the ground, it gets compressed or squished a little bit. Imagine pushing down on a toy that bounces back when you let go. That's because the energy you used to push it down is stored as elastic potential energy, and when you let go, the energy is released and the toy bounces back up. The same thing happens with a basketball!\n",
      "\n",
      "Now, when the basketball is on the ground and has this stored energy, it wants to go back to its original shape. This is called \"springing back\" or \"rebounding.\" So, the basketball uses this elastic potential energy to push itself back up, and because it's moving again, it now has kinetic energy. And that's why the basketball bounces!"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"Why does a basket ball bounce?\")['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62426eba-3b5a-44da-ae94-589dd9827f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0madvanced physics: {'input': 'How do Feynman Diagrams work in the context of quantum field theory?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Feynman diagrams are a graphical representation of the mathematical expressions of quantum field theory (QFT). They provide a visual and intuitive way to represent and analyze processes in particle physics and quantum electrodynamics (QED). These diagrams were invented by Nobel laureate Richard Feynman and are named after him.\n",
      "\n",
      "In QFT, particles are treated as excitations of fields, and their interactions are described by the coupling of these fields. Feynman diagrams represent the amplitudes of different processes as a sum over all possible ways the particles can interact. These diagrams consist of lines, representing particles, and vertices, representing interactions.\n",
      "\n",
      "The basic elements of a Feynman diagram include:\n",
      "\n",
      "1. Vertices (interaction points): Each vertex represents an interaction between particles, governed by the laws of QFT and the specific particles involved. The type of interaction is indicated by the type of vertex, such as a four-point vertex for the electromagnetic interaction or a Yukawa-type vertex for the strong force.\n",
      "\n",
      "2. Propagators (lines): These represent the motion of particles between vertices. The lines have arrows indicating the direction of particle flow (momentum), and the width of"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"How do Feynman Diagrams work?\")['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cdfb8c0-2a59-43c0-8ef7-f15d938b993d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0mbeginner physics: {'input': 'How high can an astronaut jump on the moon?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "On the Moon, an astronaut can jump much higher than on Earth! That's because there's less gravity on the Moon. On Earth, gravity pulls us down with a force of about 9.8 meters per second squared (or m/s^2). But on the Moon, the gravity is only about 1/6 of Earth's gravity, which means it pulls objects down at about 1.6 m/s^2.\n",
      "\n",
      "This makes it easier for astronauts to jump high! Imagine you're jumping on a trampoline. On Earth, each jump reaches 10 times higher than your normal height. On the Moon, each jump would reach about 6 times higher! So, if you're 5 feet tall on Earth, you would jump about 50 feet high on the Moon. Isn't that amazing?"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"How high can an astronaut jump on the moon?\")['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c50df-480c-40ca-86b2-e0658ea23c2d",
   "metadata": {},
   "source": [
    "### TransformChain\n",
    "\n",
    "`TransformChain` allows you to insert arbitrary Python logic into a LangChain pipeline. It lets you define a transformation function that takes in inputs and returns a modified dictionary of outputs. This is useful for pre- or post-processing data before or after it passes through a model or another chain.\n",
    "\n",
    "Use case: text normalization, formatting, filtering, or enrichment between model steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc9def90-4a6c-4d2e-9089-b8ae58d60894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple transformation function\n",
    "def uppercase_fn(inputs: dict) -> dict:\n",
    "    return {\"output\": inputs[\"text\"].upper()}\n",
    "\n",
    "transform_chain = TransformChain(input_variables=[\"text\"], output_variables=[\"output\"], transform=uppercase_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cea099d7-a867-410a-880c-1636c1b2feb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformChain output: {'text': 'this should be uppercase', 'output': 'THIS SHOULD BE UPPERCASE'}"
     ]
    }
   ],
   "source": [
    "# Run it\n",
    "output = transform_chain.invoke({\"text\": \"this should be uppercase\"})\n",
    "print(\"TransformChain output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16ecd8-5858-44c0-895e-fb2afd5e4c21",
   "metadata": {},
   "source": [
    "### MathChain\n",
    "LangChain's MathChain is a specialized chain used to evaluate or solve math-related prompts, especially those involving multi-step reasoning or intermediate calculations. It's part of LangChain’s approach to tool-augmented reasoning, where LLMs use helper functions (like a calculator) to improve accuracy.\n",
    "\n",
    "It does so by:\n",
    "\n",
    " - Having the LLM generate a math expression or plan\n",
    "\n",
    " - Using a Python REPL tool (or custom calculator tool) to actually compute the result\n",
    "\n",
    " - Returning the final result in a structured way\n",
    "\n",
    "It's especially useful for:\n",
    "\n",
    " - Word problems\n",
    "\n",
    " - Problems involving arithmetic, algebra, or logic\n",
    "\n",
    " - Cases where hallucination of numbers is problematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba5683f4-8000-45ca-8bf0-eaff33b801c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5faf8ffb-6164-42cd-8f13-11cd4abb4e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'If a train travels 60 km in 1.5 hours, what is its average speed?', 'answer': 'Answer: 0.011111111111111112'}"
     ]
    }
   ],
   "source": [
    "# Initialize the math chain\n",
    "math_chain = LLMMathChain.from_llm(llm=llm)\n",
    "\n",
    "# Run a word problem\n",
    "result = math_chain.invoke(\"If a train travels 60 km in 1.5 hours, what is its average speed?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6604d53-3426-4b38-a1b7-898ea5ab27ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'A football is kicked from the ground and reaches its maximum height of 5m in 10m horizontal distance from where it was kicked. How far from the kicking point will it land, assuming there is no air resistance and it flies in a perfectly parabolic arc?', 'answer': 'Answer:  8.0'}"
     ]
    }
   ],
   "source": [
    "# Run a word problem that breaks it\n",
    "result = math_chain.invoke(\"A football is kicked from the ground and reaches its maximum height of 5m in 10m horizontal distance from where it was kicked. How far from the kicking point will it land, assuming there is no air resistance and it flies in a perfectly parabolic arc?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d7e2633-00e3-44e5-a8d2-4ba49a5693e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "LLMMathChain._evaluate(\"\nnp.roots([1, 1, -2])\n\") raised error: Expression np.roots([1, 1, -2]) has forbidden control characters.. Please try again with a valid numerical expression",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pixi/.pixi/envs/default/lib/python3.14/site-packages/langchain_classic/chains/llm_math/base.py:206\u001b[39m, in \u001b[36mLLMMathChain._evaluate_expression\u001b[39m\u001b[34m(self, expression)\u001b[39m\n\u001b[32m    204\u001b[39m     local_dict = {\u001b[33m\"\u001b[39m\u001b[33mpi\u001b[39m\u001b[33m\"\u001b[39m: math.pi, \u001b[33m\"\u001b[39m\u001b[33me\u001b[39m\u001b[33m\"\u001b[39m: math.e}\n\u001b[32m    205\u001b[39m     output = \u001b[38;5;28mstr\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m         \u001b[43mnumexpr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexpression\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m            \u001b[49m\u001b[43mglobal_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# restrict access to globals\u001b[39;49;00m\n\u001b[32m    209\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# add common mathematical functions\u001b[39;49;00m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    211\u001b[39m     )\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pixi/.pixi/envs/default/lib/python3.14/site-packages/numexpr/necompiler.py:991\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(ex, local_dict, global_dict, out, order, casting, sanitize, _frame_depth, **kwargs)\u001b[39m\n\u001b[32m    990\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pixi/.pixi/envs/default/lib/python3.14/site-packages/numexpr/necompiler.py:894\u001b[39m, in \u001b[36mvalidate\u001b[39m\u001b[34m(ex, local_dict, global_dict, out, order, casting, _frame_depth, sanitize, **kwargs)\u001b[39m\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m expr_key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _names_cache.c:\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m     _names_cache.c[expr_key] = \u001b[43mgetExprNames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitize\u001b[49m\u001b[43m=\u001b[49m\u001b[43msanitize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    895\u001b[39m names, ex_uses_vml = _names_cache.c[expr_key]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pixi/.pixi/envs/default/lib/python3.14/site-packages/numexpr/necompiler.py:737\u001b[39m, in \u001b[36mgetExprNames\u001b[39m\u001b[34m(text, context, sanitize)\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgetExprNames\u001b[39m(text, context, sanitize: \u001b[38;5;28mbool\u001b[39m=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m737\u001b[39m     ex = \u001b[43mstringToExpression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    738\u001b[39m     ast = expressionToAST(ex)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pixi/.pixi/envs/default/lib/python3.14/site-packages/numexpr/necompiler.py:297\u001b[39m, in \u001b[36mstringToExpression\u001b[39m\u001b[34m(s, types, context, sanitize)\u001b[39m\n\u001b[32m    296\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _blacklist_re.search(skip_quotes) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mExpression \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has forbidden control characters.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    299\u001b[39m old_ctx = expressions._context.get_current_context()\n",
      "\u001b[31mValueError\u001b[39m: Expression np.roots([1, 1, -2]) has forbidden control characters.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Running an algebra problem does not work\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m result = \u001b[43mmath_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCan you solve the following equation for x? x^2 + x - 2 = 0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pixi/.pixi/envs/default/lib/python3.14/site-packages/langchain_classic/chains/base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    165\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    166\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    169\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    170\u001b[39m     )\n\u001b[32m    172\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    173\u001b[39m         inputs,\n\u001b[32m    174\u001b[39m         outputs,\n\u001b[32m    175\u001b[39m         return_only_outputs,\n\u001b[32m    176\u001b[39m     )\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pixi/.pixi/envs/default/lib/python3.14/site-packages/langchain_classic/chains/llm_math/base.py:280\u001b[39m, in \u001b[36mLLMMathChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    274\u001b[39m _run_manager.on_text(inputs[\u001b[38;5;28mself\u001b[39m.input_key])\n\u001b[32m    275\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m.llm_chain.predict(\n\u001b[32m    276\u001b[39m     question=inputs[\u001b[38;5;28mself\u001b[39m.input_key],\n\u001b[32m    277\u001b[39m     stop=[\u001b[33m\"\u001b[39m\u001b[33m```output\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    278\u001b[39m     callbacks=_run_manager.get_child(),\n\u001b[32m    279\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_llm_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_run_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pixi/.pixi/envs/default/lib/python3.14/site-packages/langchain_classic/chains/llm_math/base.py:232\u001b[39m, in \u001b[36mLLMMathChain._process_llm_result\u001b[39m\u001b[34m(self, llm_output, run_manager)\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_match:\n\u001b[32m    231\u001b[39m     expression = text_match.group(\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate_expression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpression\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m     run_manager.on_text(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAnswer: \u001b[39m\u001b[33m\"\u001b[39m, verbose=\u001b[38;5;28mself\u001b[39m.verbose)\n\u001b[32m    234\u001b[39m     run_manager.on_text(output, color=\u001b[33m\"\u001b[39m\u001b[33myellow\u001b[39m\u001b[33m\"\u001b[39m, verbose=\u001b[38;5;28mself\u001b[39m.verbose)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pixi/.pixi/envs/default/lib/python3.14/site-packages/langchain_classic/chains/llm_math/base.py:217\u001b[39m, in \u001b[36mLLMMathChain._evaluate_expression\u001b[39m\u001b[34m(self, expression)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    213\u001b[39m     msg = (\n\u001b[32m    214\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mLLMMathChain._evaluate(\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpression\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m) raised error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    215\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m Please try again with a valid numerical expression\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    216\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[38;5;66;03m# Remove any leading and trailing brackets from the output\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m re.sub(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m^\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m[|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m]$\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, output)\n",
      "\u001b[31mValueError\u001b[39m: LLMMathChain._evaluate(\"\nnp.roots([1, 1, -2])\n\") raised error: Expression np.roots([1, 1, -2]) has forbidden control characters.. Please try again with a valid numerical expression"
     ]
    }
   ],
   "source": [
    "# Running an algebra problem does not work\n",
    "result = math_chain.invoke(\"Can you solve the following equation for x? x^2 + x - 2 = 0\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eb306f-c1a1-409d-9c55-6871b9e95a86",
   "metadata": {},
   "source": [
    "### Applying Chains\n",
    "**Task decomposition** (or \"dividing labor\") is a key concept in prompt engineering and chain design.\n",
    "\n",
    "It involves the technique of:\n",
    "\n",
    " - Breaking down a complex task into smaller, manageable sub-tasks\n",
    "\n",
    " - Solving them step-by-step, and optionally recombining the results\n",
    "\n",
    "This leads to:\n",
    "\n",
    " - Better accuracy\n",
    "\n",
    " - Clearer LLM reasoning\n",
    "\n",
    " - Easier chaining of logic\n",
    "\n",
    "---\n",
    "\n",
    "Take for example this task: “Write a summary of the main arguments in this article, and list 3 questions the reader should consider.”\n",
    "\n",
    "We break it down into two steps:\n",
    "1. Summarize the text\n",
    "\n",
    "2. Generate reflective questions based on the summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ac35797-9a8c-4e9d-9e99-4e7699b2e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization Prompt\n",
    "summarize_prompt = PromptTemplate.from_template(\n",
    "    \"Summarize the main arguments of the following article:\\n\\n{article}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c26ad14-0dc8-4d5f-b992-d7aca464530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reflection Prompt\n",
    "question_prompt = PromptTemplate.from_template(\n",
    "    \"Based on the following summary, list 3 important questions the reader should consider:\\n\\n{summary}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "043bcc5f-ef16-4b36-9a45-e0c60ca15866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose chains using the pipe syntax\n",
    "summarize_chain = summarize_prompt | llm\n",
    "question_chain = question_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04e23310-e936-44a9-ac0a-6eaa928eb67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "article_text = (\n",
    "    \"The demand for AI skills has exploded in recent years, and it’s likely to continue as more and more businesses embrace artificial intelligence solutions.\"\n",
    "    \"The number of employers looking for AI-literate employees quadrupled between 2010 and 2019, and AI skills are becoming essential across a wide range of industries, making them a valuable asset for advancing your career and staying competitive in a rapidly evolving job market.\"\n",
    "    \"AI-related jobs typically pay 11% more than non-AI roles within the same company.\"\n",
    "    \"Skills with AI are particularly useful if you plan to work in the information, professional services, administrative, or finance sectors.\"\n",
    "    \"AI adoption offers several potential benefits. It helps automate repetitive processes like data entry to improve operational efficiency.\"\n",
    "    \"AI can also process and analyze large data sets rapidly, enabling it to identify patterns and make reasoned predictions to aid robust decision-making.\"\n",
    "    \"For some businesses, using tools such as call bots and chatbots helps streamline customer interactions to boost engagement and satisfaction.\"\n",
    "    \"Numerous businesses have used AI to improve customer experiences and drive growth. For example, J.P. Morgan and Chase developed the award-winning OmniAI platform to deliver accurate financial insights.\"\n",
    "    \"The model can perform deep, comprehensive analyses of vast data sets, reducing operational costs and enabling faster solution development.\"\n",
    "    \"AI has the potential to automate non-routine tasks and solve some of the world’s most complex problems.\"\n",
    "    \"For example, AI technologies can model climate change predictions, improve energy grid efficiency, and even help you reduce your household energy consumption through smart home heating systems.\"\n",
    "    \"Other applications include analyzing data during clinical trials and optimizing journeys to reduce the load on transport infrastructure.\"\n",
    "    \"However, calculating the impact of AI on global challenges is complex, and even seemingly perfect solutions can have unintended outcomes.\"\n",
    "    \"For instance, improving your home’s efficiency may encourage you to spend more time in your perfectly heated house, increasing your use of energy-hungry appliances.\"\n",
    "    \"Accounting for unforeseen effects is just one potential pitfall of relying on AI.\"\n",
    "    \"Poor data protection practices increase the risk of privacy violations, while training models on biased data could lead to discrimination.\"\n",
    "    \"This is why ethical practices are essential for responsible AI development.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad6b81b5-1976-4bd3-9538-a191a7da78f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step execution\n",
    "summary = summarize_chain.invoke({\"article\": article_text})\n",
    "questions = question_chain.invoke({\"summary\": summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68abc74d-9757-4675-b0d3-9d5db6a3c4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      " Some AI developers advocate for an AI Bill of Rights to ensure responsible AI development. The AI Bill of Rights aims to protect individuals’ right to privacy, right to transparency, and right to fairness.The AI Bill of Rights should also include a right to explanation, ensuring that AI systems are always transparent and explainable, as well as a right to redress, ensuring that responsible data-sharing practices are in place.However, implementing these rights will require cooperation among governments, businesses, and individuals.\n",
      "\n",
      "The article highlights the growing demand for AI skills in the job market and the potential benefits of AI adoption for businesses. AI-related jobs typically pay 11% more than non-AI roles within the same company. AI skills are becoming essential across various industries, making them a valuable asset for advancing your career and staying competitive in a rapidly changing job market. AI technologies can automate repetitive processes, improve operational efficiency, process and analyze large data sets rapidly, and identify patterns and make reasoned predictions to aid decision-making. AI has the potential to automate non-routine tasks and solve some of the world’s most complex problems, such as modeling climate change predictions, optimizing journeys, and reducing household energy consumption. However,\n",
      "\n",
      "Reflective Questions:\n",
      "  AI also raises some concerns, such as job displacement, data privacy, and security.\n",
      "\n",
      "Important questions to consider include:\n",
      "\n",
      "1. What are the benefits of an AI Bill of Rights?\n",
      "\n",
      "2. How can governments, businesses, and individuals cooperate to implement the rights outlined in the AI Bill of Rights?\n",
      "\n",
      "3. How can AI technologies improve operational efficiency, data analysis, and decision-making?\n",
      "\n",
      "4. What are the potential concerns raised by AI technologies, such as job displacement, data privacy, and security?\n",
      "\n",
      "5. How can AI skills be utilized to advance your career and remain competitive in a rapidly changing job market?\n",
      "\n",
      "6. How can AI technologies aid in solving some of the world’s most complex problems, such as modeling climate change predictions, optimizing journeys, and reducing household energy consumption?\n",
      "\n",
      "7. What steps can be taken to mitigate the potential negative impacts of AI technologies, such as job displacement, data privacy, and security concerns?\n",
      "\n",
      "8. How can businesses balance the benefits of AI adoption with the potential negative impacts on workers and society?\n",
      "\n",
      "9. What are the essential AI skills that are in high demand in the job market?\n",
      "\n",
      "1"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "print(\"Summary:\\n\", summary)\n",
    "print(\"\\nReflective Questions:\\n\", questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16d1d7d5-cc40-4285-b03f-d8cd55221a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c25036f9",
   "metadata": {},
   "source": [
    "## ⚠️ Important: GPU Memory Management\n",
    "\n",
    "**Running cleanup cells alone does NOT fully release GPU memory!**\n",
    "\n",
    "The cleanup cell removes Python references and clears PyTorch cache, but the kernel process\n",
    "still holds GPU allocations until it terminates.\n",
    "\n",
    "**To fully release GPU memory:**\n",
    "1. Run the cleanup cell below\n",
    "2. Then: **Kernel → Shutdown** (or restart) this notebook's kernel\n",
    "\n",
    "**Before running the next notebook:**\n",
    "- Check GPU status in the startup cell\n",
    "- If GPU is still >80% used, shutdown unused notebook kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5c2bc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=== Cleaning Up Resources ===\n",
       "\n",
       "1. Releasing HuggingFace resources...\n",
       "   ✓ Deleted: model, tokenizer\n",
       "\n",
       "2. Clearing GPU cache...\n",
       "3. ✓ Garbage collection complete\n",
       "\n",
       "4. Final GPU Memory Status (System-Wide):\n",
       "   GPU 0: 8.88 GB used, 7.11 GB free\n",
       "\n",
       "========================================\n",
       "✓ Cleanup complete!\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Resource Cleanup ===\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "print(\"=== Cleaning Up Resources ===\\n\")\n",
    "\n",
    "# 1. Delete HuggingFace objects\n",
    "print(\"1. Releasing HuggingFace resources...\")\n",
    "hf_objects = ['model', 'tokenizer', 'text_pipeline', 'llm', 'chat_llm', 'pipe']\n",
    "deleted = []\n",
    "for obj_name in hf_objects:\n",
    "    if obj_name in globals():\n",
    "        del globals()[obj_name]\n",
    "        deleted.append(obj_name)\n",
    "if deleted:\n",
    "    print(f\"   ✓ Deleted: {', '.join(deleted)}\")\n",
    "else:\n",
    "    print(\"   No HuggingFace objects to delete\")\n",
    "\n",
    "# 2. Clear GPU cache\n",
    "print(\"\\n2. Clearing GPU cache...\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# 3. Python garbage collection\n",
    "gc.collect()\n",
    "print(\"3. ✓ Garbage collection complete\")\n",
    "\n",
    "# 4. Show SYSTEM-WIDE GPU memory status (using pynvml)\n",
    "print(\"\\n4. Final GPU Memory Status (System-Wide):\")\n",
    "try:\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()\n",
    "    for i in range(pynvml.nvmlDeviceGetCount()):\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "        info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        used_gb = info.used / 1024**3\n",
    "        free_gb = info.free / 1024**3\n",
    "        print(f\"   GPU {i}: {used_gb:.2f} GB used, {free_gb:.2f} GB free\")\n",
    "    pynvml.nvmlShutdown()\n",
    "except:\n",
    "    # Fallback to per-process only\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        print(f\"   GPU {i}: {allocated:.2f} GB (this process only)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"✓ Cleanup complete!\")\n",
    "\n",
    "# === OPTIONAL: Shutdown Kernel to Fully Release GPU Memory ===\n",
    "# Uncomment the next line to shutdown this kernel after cleanup\n",
    "# (Required to fully release GPU memory for other notebooks)\n",
    "\n",
    "# from IPython import get_ipython; get_ipython().kernel.do_shutdown(restart=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
