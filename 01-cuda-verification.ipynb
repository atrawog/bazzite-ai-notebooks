{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# CUDA Verification\n",
    "\n",
    "This notebook verifies GPU access and CUDA functionality in your container environment.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Check if CUDA is available\n",
    "- View GPU properties\n",
    "- Test basic tensor operations on GPU\n",
    "- Benchmark CPU vs GPU performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.6\n",
      "PyTorch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import platform\n",
    "\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2",
   "metadata": {},
   "source": [
    "## 2. Check CUDA Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cuda-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Version: 12.4\n",
      "cuDNN Version: 90100\n",
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA Available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"cuDNN Version: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  GPU not detected. Check that:\")\n",
    "    print(\"   1. Container was started with --device=nvidia.com/gpu=all\")\n",
    "    print(\"   2. Host system has ujust setup-gpu-containers configured\")\n",
    "    print(\"   3. NVIDIA drivers are properly installed on host\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3",
   "metadata": {},
   "source": [
    "## 3. GPU Device Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "gpu-props",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU 0: NVIDIA GeForce RTX 4080 SUPER\n",
      "  Compute Capability: 8.9\n",
      "  Total Memory: 15.57 GB\n",
      "  Multiprocessors: 80\n",
      "  CUDA Cores (estimated): 10240\n"
     ]
    }
   ],
   "source": [
    "if cuda_available:\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Compute Capability: {props.major}.{props.minor}\")\n",
    "        print(f\"  Total Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"  Multiprocessors: {props.multi_processor_count}\")\n",
    "        print(f\"  CUDA Cores (estimated): {props.multi_processor_count * 128}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4",
   "metadata": {},
   "source": [
    "## 4. Test Tensor Operations on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tensor-ops",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Successfully created and multiplied tensors on GPU\n",
      "\n",
      "Result tensor shape: torch.Size([3, 3])\n",
      "Result tensor device: cuda:0\n",
      "\n",
      "Sample output:\n",
      "tensor([[ 1.1288,  1.6837,  2.8337],\n",
      "        [ 1.1353,  2.3751,  1.8922],\n",
      "        [ 0.0608,  1.1372, -0.1349]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if cuda_available:\n",
    "    # Create tensors on GPU\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    \n",
    "    # Create random tensors\n",
    "    a = torch.randn(3, 3, device=device)\n",
    "    b = torch.randn(3, 3, device=device)\n",
    "    \n",
    "    # Perform matrix multiplication\n",
    "    c = torch.matmul(a, b)\n",
    "    \n",
    "    print(\"‚úì Successfully created and multiplied tensors on GPU\")\n",
    "    print(f\"\\nResult tensor shape: {c.shape}\")\n",
    "    print(f\"Result tensor device: {c.device}\")\n",
    "    print(f\"\\nSample output:\\n{c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5",
   "metadata": {},
   "source": [
    "## 5. Performance Benchmark: CPU vs GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "benchmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Time (10 iterations): 2.7173 seconds\n",
      "GPU Time (10 iterations): 0.0700 seconds\n",
      "\n",
      "üöÄ GPU Speedup: 38.82x faster\n"
     ]
    }
   ],
   "source": [
    "size = 5000\n",
    "iterations = 10\n",
    "\n",
    "# CPU benchmark\n",
    "a_cpu = torch.randn(size, size)\n",
    "b_cpu = torch.randn(size, size)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(iterations):\n",
    "    c_cpu = torch.matmul(a_cpu, b_cpu)\n",
    "cpu_time = time.time() - start\n",
    "\n",
    "print(f\"CPU Time ({iterations} iterations): {cpu_time:.4f} seconds\")\n",
    "\n",
    "# GPU benchmark (if available)\n",
    "if cuda_available:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    a_gpu = torch.randn(size, size, device=device)\n",
    "    b_gpu = torch.randn(size, size, device=device)\n",
    "    \n",
    "    # Warm up GPU\n",
    "    for _ in range(5):\n",
    "        _ = torch.matmul(a_gpu, b_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Actual benchmark\n",
    "    start = time.time()\n",
    "    for _ in range(iterations):\n",
    "        c_gpu = torch.matmul(a_gpu, b_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    gpu_time = time.time() - start\n",
    "    \n",
    "    print(f\"GPU Time ({iterations} iterations): {gpu_time:.4f} seconds\")\n",
    "    print(f\"\\nüöÄ GPU Speedup: {cpu_time/gpu_time:.2f}x faster\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  GPU not available for benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "If all cells executed successfully:\n",
    "- ‚úÖ CUDA is properly configured\n",
    "- ‚úÖ GPU is accessible from PyTorch\n",
    "- ‚úÖ GPU operations are working correctly\n",
    "- ‚úÖ GPU provides significant performance advantage\n",
    "\n",
    "**Next steps:**\n",
    "- Try the PyTorch Quickstart notebook (02-pytorch-quickstart.ipynb)\n",
    "- Explore the Data Science Template (03-data-science-template.ipynb)\n",
    "- Start your own ML/AI projects!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
